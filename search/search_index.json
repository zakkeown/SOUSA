{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"SOUSA: Synthetic Open Unified Snare Assessment","text":"<p>A synthetic dataset generator for all 40 PAS drum rudiments, designed to train machine learning models for drumming performance assessment.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>100K+ Samples - Comprehensive coverage of all PAS rudiments</li> <li>Multi-Soundfont Audio - Practice pad, marching snare, drum kits</li> <li>Audio Augmentation - Room acoustics, mic simulation, compression</li> <li>Hierarchical Labels - Stroke, measure, and exercise-level scores</li> <li>Profile-Based Splits - Proper train/val/test generalization</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>from datasets import load_dataset\n\ndataset = load_dataset(\"zkeown/sousa\")\nsample = dataset[\"train\"][0]\nprint(f\"Rudiment: {sample['rudiment_slug']}\")\nprint(f\"Overall Score: {sample['overall_score']:.1f}\")\n</code></pre> <p>Get Started View on HuggingFace</p>"},{"location":"#what-is-sousa","title":"What is SOUSA?","text":"<p>SOUSA generates synthetic drum rudiment performances with realistic timing and velocity variations modeled from player skill profiles. Each sample includes:</p> <ul> <li>MIDI performance data with per-stroke timing and velocity</li> <li>Synthesized audio using multiple soundfonts (practice pad, marching snare, drum kits)</li> <li>Hierarchical labels at stroke, measure, and exercise levels</li> <li>Player profile metadata including skill tier and performance characteristics</li> </ul>"},{"location":"#dataset-overview","title":"Dataset Overview","text":"Component Description Rudiments All 40 PAS International Drum Rudiments Profiles 100 player profiles across 4 skill tiers Audio 44.1kHz FLAC, multiple soundfonts and augmentations Labels 0-100 scores for timing, velocity, balance, and more Splits Profile-based train/val/test (70/15/15)"},{"location":"#use-cases","title":"Use Cases","text":"<p>SOUSA is designed for:</p> <ul> <li>Performance Assessment - Train models to score drumming quality</li> <li>Skill Classification - Classify performances by skill level</li> <li>Rudiment Recognition - Identify which rudiment is being played</li> <li>Audio Understanding - General research on percussive audio</li> </ul>"},{"location":"#documentation","title":"Documentation","text":"<ul> <li> <p> Installation</p> <p>Install SOUSA and its dependencies</p> <p> Installation Guide</p> </li> <li> <p> Quick Start</p> <p>Generate your first dataset in 5 minutes</p> <p> Quick Start</p> </li> <li> <p> Loading Data</p> <p>Access the dataset from HuggingFace or locally</p> <p> Loading Guide</p> </li> <li> <p> User Guide</p> <p>Detailed examples for ML tasks</p> <p> User Guide</p> </li> </ul>"},{"location":"#generation-presets","title":"Generation Presets","text":"Preset Profiles Tempos Augmentations Samples Storage small 10 3 1 ~1,200 ~1 GB medium 50 3 2 ~12,000 ~10 GB full 100 5 5 ~100,000 ~97 GB"},{"location":"#rudiments-covered","title":"Rudiments Covered","text":"<p>All 40 PAS International Drum Rudiments:</p> <ul> <li>Roll Rudiments (15): Single/Double/Triple Stroke Rolls, 5-17 Stroke Rolls</li> <li>Diddle Rudiments (5): Paradiddles and variants</li> <li>Flam Rudiments (12): Flam, Flam Accent, Flam Tap, Flamacue, etc.</li> <li>Drag Rudiments (8): Drags, Drag Taps, Ratamacue variants</li> </ul>"},{"location":"#license","title":"License","text":"<p>SOUSA is released under the MIT License.</p>"},{"location":"LIMITATIONS/","title":"Dataset Limitations","text":"<p>This document explicitly describes the known limitations and potential biases of the SOUSA dataset. Understanding these limitations is critical for appropriate use in ML research.</p>"},{"location":"LIMITATIONS/#synthetic-data-characteristics","title":"Synthetic Data Characteristics","text":""},{"location":"LIMITATIONS/#self-referential-labels","title":"Self-Referential Labels","text":"<p>SOUSA labels are derived from the same parameters used to generate the data. This creates a circular relationship:</p> <pre><code>PlayerProfile parameters -&gt; Generate timing/velocity errors -&gt; Compute scores from errors\n</code></pre> <p>Implications</p> <ul> <li>Models may learn the generation function rather than \"drumming quality\"</li> <li>High accuracy on SOUSA may not transfer to real-world assessment</li> <li>Labels are mathematically consistent but not validated against human perception</li> </ul> <p>Recommendation</p> <p>Treat SOUSA as a pretraining or development dataset. Validate models on real human performances before deployment.</p>"},{"location":"LIMITATIONS/#well-behaved-error-distribution","title":"Well-Behaved Error Distribution","text":"<p>Timing and velocity errors are sampled from Gaussian distributions with skill-tier-dependent parameters:</p> Dimension Beginner Professional Timing std (ms) ~50 ~7 Velocity std ~0.12 ~0.07 <p>What this captures:</p> <ul> <li>Gradual skill progression</li> <li>Correlated improvement across dimensions</li> <li>Realistic variance within skill tiers</li> </ul> <p>What this misses:</p> <ul> <li>Catastrophic failures (wrong sticking patterns, skipped notes)</li> <li>Tempo collapse under difficulty</li> <li>Fatigue effects over long exercises</li> <li>Cognitive errors (playing wrong rudiment)</li> <li>Recovery patterns after mistakes</li> </ul> <p>Implication</p> <p>Models trained on SOUSA may not handle out-of-distribution failures that occur in real practice sessions.</p>"},{"location":"LIMITATIONS/#instrument-limitations","title":"Instrument Limitations","text":""},{"location":"LIMITATIONS/#single-instrument","title":"Single Instrument","text":"<p>SOUSA generates snare drum only (MIDI note 38, General MIDI acoustic snare).</p> <p>Not included:</p> <ul> <li>Kick drum patterns</li> <li>Hi-hat/cymbal interplay</li> <li>Tom fills</li> <li>Full drum kit coordination</li> <li>Marching tenor (quads) or bass drum parts</li> </ul> <p>Implication</p> <p>Models are specialized for snare-only assessment and will not generalize to full kit drumming.</p>"},{"location":"LIMITATIONS/#soundfont-dependency","title":"Soundfont Dependency","text":"<p>Audio is synthesized using SF2 soundfonts, not recorded from physical instruments.</p> Soundfont Character <code>generalu</code> General MIDI standard <code>marching</code> Marching snare character <code>mtpowerd</code> Modern processed sound <code>douglasn</code> Natural acoustic character <code>fluidr3</code> FluidR3 GM standard <p>Implications:</p> <ul> <li>Timbral variety is limited to these 5 soundfonts</li> <li>Synthesized transients may differ from real drum recordings</li> <li>No stick type variation, head tuning, or snare wire adjustments</li> <li>Models may overfit to synthesis artifacts</li> </ul>"},{"location":"LIMITATIONS/#label-quality","title":"Label Quality","text":""},{"location":"LIMITATIONS/#perceptual-score-scaling-v02","title":"Perceptual Score Scaling (v0.2+)","text":"<p>Scores now use sigmoid scaling that better matches human perception:</p> <pre><code># Old linear scaling (v0.1):\ntiming_accuracy = 100 - (mean_abs_error_ms * 2)  # 0ms = 100, 50ms = 0\n\n# New sigmoid scaling (v0.2+):\ntiming_accuracy = 100 * sigmoid((25 - mean_abs_error) / 10)\n# &lt;10ms -&gt; 92-100 (imperceptible)\n# 25ms -&gt; 50 (noticeable)\n# &gt;50ms -&gt; 0-8 (clearly audible)\n</code></pre> <p>Version Compatibility</p> <p>If using pre-v0.2 datasets, scores will differ slightly. Regenerate for consistency.</p>"},{"location":"LIMITATIONS/#skill-tier-overlap-classification-ceiling","title":"Skill Tier Overlap (Classification Ceiling)","text":"<p>Due to realistic skill distributions, adjacent tiers have significant score overlap:</p> Adjacent Tiers Score Overlap beginner &lt;-&gt; intermediate ~67% intermediate &lt;-&gt; advanced ~83% advanced &lt;-&gt; professional ~83% <p>Implications:</p> <ul> <li>4-class skill tier classification has an inherent accuracy ceiling due to label noise</li> <li>A sample with score=50 could legitimately be beginner, intermediate, or advanced</li> <li>Models will plateau at ~70-80% accuracy regardless of capacity</li> </ul> <p>Mitigations provided:</p> <ol> <li> <p><code>tier_confidence</code> (0-1): Indicates how central a sample is to its tier's distribution. Filter low-confidence samples for cleaner training.</p> </li> <li> <p><code>skill_tier_binary</code> (novice/skilled): 2-class alternative with less overlap:</p> <ul> <li>novice = beginner + intermediate</li> <li>skilled = advanced + professional</li> </ul> </li> </ol> <p>Recommendation</p> <p>For classification, use <code>skill_tier_binary</code> or filter by <code>tier_confidence &gt; 0.5</code>. For assessment, use regression on <code>overall_score</code>.</p>"},{"location":"LIMITATIONS/#score-correlations-redundancy","title":"Score Correlations (Redundancy)","text":"<p>Several scores are highly correlated (r &gt; 0.85):</p> <pre><code>timing_accuracy &lt;-&gt; timing_consistency: r = 0.89\noverall_score &lt;-&gt; timing_consistency: r = 0.89\noverall_score &lt;-&gt; timing_accuracy: r = 0.88\n</code></pre> <p>Implications:</p> <ul> <li>Using all scores as separate targets provides no additional learning signal</li> <li>Multi-task models should use orthogonal targets</li> </ul> <p>Recommended minimal score set:</p> <ol> <li><code>overall_score</code> - General quality (captures correlated cluster)</li> <li><code>tempo_stability</code> - Independent signal about rushing/dragging</li> </ol>"},{"location":"LIMITATIONS/#hand-balance-ceiling-effect","title":"Hand Balance Ceiling Effect","text":"<p>The <code>hand_balance</code> score has a ceiling effect (mean=88, most samples near 100):</p> Metric Value Mean 87.7 Std 11.2 Range [35.3, 100.0] <p>Cause: v0.1 measured only velocity ratio, which is nearly always high.</p> <p>v0.2 fix: <code>hand_balance</code> now combines velocity (50%) + timing (50%) for better discrimination.</p> <p>Recommendation</p> <p>For pre-v0.2 datasets, exclude <code>hand_balance</code> from ML targets.</p>"},{"location":"LIMITATIONS/#no-human-validation","title":"No Human Validation","text":"<p>Scores are computed from mathematical formulas, not human ratings.</p> <p>Not validated:</p> <ul> <li>Perceptual relevance of scoring thresholds</li> <li>Whether a score of 75 \"sounds\" better than 65 to human listeners</li> <li>Inter-rater reliability with music instructors</li> <li>Cultural or stylistic scoring preferences</li> </ul> <p>Recommendation</p> <p>Do not interpret score outputs as equivalent to instructor feedback without external validation.</p>"},{"location":"LIMITATIONS/#rudiment-specific-score-availability","title":"Rudiment-Specific Score Availability","text":"<p>Some scores are only computed for applicable rudiments:</p> Score Available When <code>flam_quality</code> Rudiment contains flams <code>diddle_quality</code> Rudiment contains diddles <code>roll_sustain</code> Rudiment is a roll type <p>Implication</p> <p>Null values exist in the dataset. Handle appropriately in training (masking, separate heads, etc.).</p>"},{"location":"LIMITATIONS/#domain-gap-considerations","title":"Domain Gap Considerations","text":""},{"location":"LIMITATIONS/#synthetic-vs-real-recordings","title":"Synthetic vs. Real Recordings","text":"<p>Real drum recordings include factors not present in SOUSA:</p> Factor SOUSA Real World Bleed from other instruments No Yes Room acoustics Simulated IR Physical space Microphone characteristics Modeled Actual hardware Performance context Isolated Musical setting Player psychology None Nerves, fatigue, musicality <p>Implication: Models trained on SOUSA may struggle with:</p> <ul> <li>Multi-instrument mixes</li> <li>Unusual room characteristics</li> <li>Low-quality or phone recordings not matching augmentation presets</li> <li>Real performance variability</li> </ul>"},{"location":"LIMITATIONS/#augmentation-coverage","title":"Augmentation Coverage","text":"<p>SOUSA includes 10 augmentation presets ranging from clean studio to lo-fi:</p> <pre><code>clean_studio -&gt; practice_room -&gt; concert_hall -&gt; gym -&gt; lo_fi\n</code></pre> <p>Not covered:</p> <ul> <li>Extreme clipping or distortion</li> <li>Heavy compression artifacts (broadcast limiting)</li> <li>Pitch shifting or time stretching</li> <li>Real-world microphone failures</li> <li>Environmental noise beyond included profiles</li> </ul>"},{"location":"LIMITATIONS/#statistical-considerations","title":"Statistical Considerations","text":""},{"location":"LIMITATIONS/#skill-tier-distribution-class-imbalance","title":"Skill Tier Distribution &amp; Class Imbalance","text":"<p>Default generation uses:</p> Tier Proportion Imbalance Professional 8% 4.26x underrepresented Beginner 26% - Advanced 32% - Intermediate 34% majority class <p>Class imbalance ratio: 4.26:1 (intermediate vs professional)</p> <p>Implications:</p> <ul> <li>Models trained without class weights will be biased toward intermediate predictions</li> <li>Accuracy alone is misleading; use balanced accuracy</li> <li>Professional tier predictions will have lower recall</li> </ul> <p>Mitigations provided:</p> <ol> <li>Class weights are enabled by default in <code>skill_classification.py</code></li> <li>Balanced accuracy is used for early stopping and model selection</li> <li>Per-class metrics are reported in evaluation</li> </ol> <p>Recommendation</p> <p>Always use class weights for skill tier classification. Report balanced accuracy, not just accuracy.</p>"},{"location":"LIMITATIONS/#profile-based-splits","title":"Profile-Based Splits","text":"<p>Train/val/test splits are by player profile, not by sample:</p> <ul> <li>All samples from a profile stay in the same split</li> <li>Prevents data leakage from player-specific patterns</li> <li>May make generalization harder (testing on \"new players\")</li> </ul>"},{"location":"LIMITATIONS/#recommended-use-cases","title":"Recommended Use Cases","text":""},{"location":"LIMITATIONS/#appropriate-uses","title":"Appropriate Uses","text":"Use Case Why It Works Pretraining Initialize models before fine-tuning on real data Architecture development Compare model architectures on controlled data Ablation studies Isolate effects of audio augmentation, skill tiers, etc. Educational tools Build practice aids where exact calibration is less critical Baseline establishment Provide reproducible benchmarks for the research community"},{"location":"LIMITATIONS/#use-with-caution","title":"Use With Caution","text":"Use Case Concern Production assessment Validate on real performances before deployment Certification/grading Scores are not calibrated to educational standards Cross-instrument transfer Models will not generalize to non-snare instruments"},{"location":"LIMITATIONS/#not-recommended","title":"Not Recommended","text":"<p>Avoid These Uses</p> <ul> <li>Direct deployment without real-world validation</li> <li>Claims of human-equivalent assessment without instructor calibration studies</li> <li>Full drum kit assessment (snare only)</li> </ul>"},{"location":"LIMITATIONS/#future-work","title":"Future Work","text":"<p>Potential improvements to address these limitations:</p> <ol> <li>Human validation study: Collect instructor ratings on a subset for calibration</li> <li>Failure mode injection: Add catastrophic errors (wrong patterns, tempo collapse)</li> <li>Real recording subset: Include a small set of actual recordings for domain gap measurement</li> <li>Extended instrument support: Marching percussion, full kit patterns</li> <li>Longitudinal profiles: Model player improvement over time</li> </ol>"},{"location":"LIMITATIONS/#citation","title":"Citation","text":"<p>If you use SOUSA in research, please cite:</p> <pre><code>@dataset{sousa2024,\n  title={SOUSA: Synthetic Open Unified Snare Assessment Dataset},\n  author={...},\n  year={2024},\n  url={https://huggingface.co/datasets/zkeown/sousa}\n}\n</code></pre> <p>Last updated: January 2026 (v0.2 - added tier_confidence, perceptual scoring, class weights)</p>"},{"location":"api/","title":"API Reference","text":"<p>The SOUSA (Synthetic Open Unified Snare Assessment) package is organized into several modules that handle different aspects of synthetic drum rudiment dataset generation.</p>"},{"location":"api/#module-structure","title":"Module Structure","text":"<p>The <code>dataset_gen</code> package follows this hierarchical structure:</p> <pre><code>dataset_gen/\n\u251c\u2500\u2500 rudiments/        # Rudiment definitions and loading\n\u251c\u2500\u2500 profiles/         # Player profile generation\n\u251c\u2500\u2500 midi_gen/         # MIDI performance generation\n\u251c\u2500\u2500 audio_synth/      # Audio synthesis via FluidSynth\n\u251c\u2500\u2500 audio_aug/        # Audio augmentation chain\n\u251c\u2500\u2500 labels/           # Hierarchical label computation\n\u251c\u2500\u2500 pipeline/         # Dataset generation orchestration\n\u2514\u2500\u2500 validation/       # Dataset validation and analysis\n</code></pre>"},{"location":"api/#module-overview","title":"Module Overview","text":""},{"location":"api/#rudiments","title":"Rudiments","text":"<p>Handles loading and parsing of the 40 PAS (Percussive Arts Society) drum rudiment definitions from YAML files. Provides schema classes for representing strokes, sticking patterns, and complete rudiment specifications.</p>"},{"location":"api/#profiles","title":"Profiles","text":"<p>Generates player profiles with skill-tier-based archetypes. Each profile represents a virtual drummer with correlated execution dimensions covering timing accuracy, dynamics control, and hand balance.</p>"},{"location":"api/#midi-generation","title":"MIDI Generation","text":"<p>Generates MIDI performances from rudiment definitions and player profiles. Applies realistic timing deviations, velocity variations, and articulation-specific processing for flams, diddles, and rolls.</p>"},{"location":"api/#audio-synthesis","title":"Audio Synthesis","text":"<p>FluidSynth wrapper for rendering MIDI performances to audio. Supports multiple soundfonts for different drum sounds (practice pad, marching snare, drum kits).</p>"},{"location":"api/#audio-augmentation","title":"Audio Augmentation","text":"<p>Comprehensive audio augmentation pipeline including room simulation (convolution reverb), microphone modeling, recording chain simulation (preamp, compression, EQ), and degradation effects (noise, bit reduction).</p>"},{"location":"api/#labels","title":"Labels","text":"<p>Computes hierarchical labels at three levels:</p> <ul> <li>Stroke-level: Individual timing and velocity measurements</li> <li>Measure-level: Aggregate statistics per measure</li> <li>Exercise-level: Overall performance scores (0-100 scale)</li> </ul>"},{"location":"api/#pipeline","title":"Pipeline","text":"<p>Orchestrates the complete dataset generation pipeline from profiles to stored samples. Handles parallel processing, checkpointing, and Parquet/audio file storage.</p>"},{"location":"api/#validation","title":"Validation","text":"<p>Dataset validation and statistical analysis tools. Verifies label correctness, data integrity, and skill tier ordering. Generates comprehensive validation reports.</p>"},{"location":"api/#quick-start","title":"Quick Start","text":"<pre><code>from dataset_gen.pipeline.generate import generate_dataset\n\n# Generate a small dataset\nsplits = generate_dataset(\n    output_dir=\"output/dataset\",\n    num_profiles=10,\n    soundfont_path=\"data/soundfonts/snare.sf2\",\n    seed=42,\n)\n</code></pre>"},{"location":"api/#key-types","title":"Key Types","text":"<p>The most commonly used types across the package:</p> <ul> <li><code>Rudiment</code> - Complete rudiment definition from YAML</li> <li><code>StickingPattern</code> - Stroke sequence for a rudiment</li> <li><code>PlayerProfile</code> - Virtual drummer characteristics</li> <li><code>SkillTier</code> - Skill level enum (beginner/intermediate/advanced/professional)</li> <li><code>Sample</code> - Complete sample with all hierarchical labels</li> <li><code>GeneratedPerformance</code> - MIDI performance with stroke events</li> </ul>"},{"location":"api/audio-aug/","title":"Audio Augmentation Module","text":"<p>The audio augmentation module provides a comprehensive pipeline for adding realistic recording characteristics to synthesized audio. This increases dataset diversity and helps models generalize to real-world recording conditions.</p> <p>The augmentation chain includes:</p> <ul> <li>Room simulation: Convolution reverb with impulse responses</li> <li>Microphone modeling: Frequency response curves and positioning effects</li> <li>Recording chain: Preamp saturation, compression, and EQ</li> <li>Degradation: Noise injection, bit depth reduction, sample rate artifacts</li> </ul>"},{"location":"api/audio-aug/#recording-chain","title":"Recording Chain","text":"<p>Simulates analog recording chain including preamp coloration, dynamics compression, and master EQ.</p> <p>Recording chain simulation (preamp, compression, EQ).</p> <p>This module simulates the analog recording chain including preamp coloration, compression, and master EQ.</p>"},{"location":"api/audio-aug/#dataset_gen.audio_aug.chain.PreampType","title":"PreampType","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Types of preamp character.</p>"},{"location":"api/audio-aug/#dataset_gen.audio_aug.chain.PreampConfig","title":"PreampConfig  <code>dataclass</code>","text":"<pre><code>PreampConfig(preamp_type: PreampType = CLEAN, gain_db: float = 0.0, drive: float = 0.0, output_gain_db: float = 0.0)\n</code></pre> <p>Configuration for preamp simulation.</p>"},{"location":"api/audio-aug/#dataset_gen.audio_aug.chain.CompressorConfig","title":"CompressorConfig  <code>dataclass</code>","text":"<pre><code>CompressorConfig(enabled: bool = True, threshold_db: float = -12.0, ratio: float = 4.0, attack_ms: float = 10.0, release_ms: float = 100.0, knee_db: float = 6.0, makeup_gain_db: float = 0.0)\n</code></pre> <p>Configuration for dynamics compression.</p>"},{"location":"api/audio-aug/#dataset_gen.audio_aug.chain.EQConfig","title":"EQConfig  <code>dataclass</code>","text":"<pre><code>EQConfig(enabled: bool = True, highpass_freq: float = 40.0, highpass_enabled: bool = True, lowpass_freq: float = 18000.0, lowpass_enabled: bool = True, low_shelf: tuple[float, float] = (100.0, 0.0), high_shelf: tuple[float, float] = (8000.0, 0.0))\n</code></pre> <p>Configuration for master EQ.</p>"},{"location":"api/audio-aug/#dataset_gen.audio_aug.chain.ChainConfig","title":"ChainConfig  <code>dataclass</code>","text":"<pre><code>ChainConfig(preamp: PreampConfig | None = None, compressor: CompressorConfig | None = None, eq: EQConfig | None = None)\n</code></pre> <p>Full recording chain configuration.</p>"},{"location":"api/audio-aug/#dataset_gen.audio_aug.chain.RecordingChain","title":"RecordingChain","text":"<pre><code>RecordingChain(config: ChainConfig | None = None, sample_rate: int = 44100)\n</code></pre> <p>Simulate a recording chain with preamp, compression, and EQ.</p> <p>Processing order: Input -&gt; Preamp -&gt; Compressor -&gt; EQ -&gt; Output</p> <p>Initialize recording chain.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>ChainConfig | None</code> <p>Chain configuration</p> <code>None</code> <code>sample_rate</code> <code>int</code> <p>Audio sample rate</p> <code>44100</code> Source code in <code>dataset_gen/audio_aug/chain.py</code> <pre><code>def __init__(\n    self,\n    config: ChainConfig | None = None,\n    sample_rate: int = 44100,\n):\n    \"\"\"\n    Initialize recording chain.\n\n    Args:\n        config: Chain configuration\n        sample_rate: Audio sample rate\n    \"\"\"\n    self.config = config or ChainConfig()\n    self.sample_rate = sample_rate\n</code></pre>"},{"location":"api/audio-aug/#dataset_gen.audio_aug.chain.RecordingChain.process","title":"process","text":"<pre><code>process(audio: ndarray, config: ChainConfig | None = None) -&gt; ndarray\n</code></pre> <p>Process audio through the recording chain.</p> <p>Parameters:</p> Name Type Description Default <code>audio</code> <code>ndarray</code> <p>Input audio (shape: [samples] or [samples, channels])</p> required <code>config</code> <code>ChainConfig | None</code> <p>Optional override configuration</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Processed audio</p> Source code in <code>dataset_gen/audio_aug/chain.py</code> <pre><code>def process(\n    self,\n    audio: np.ndarray,\n    config: ChainConfig | None = None,\n) -&gt; np.ndarray:\n    \"\"\"\n    Process audio through the recording chain.\n\n    Args:\n        audio: Input audio (shape: [samples] or [samples, channels])\n        config: Optional override configuration\n\n    Returns:\n        Processed audio\n    \"\"\"\n    cfg = config or self.config\n\n    # Handle mono/stereo\n    if audio.ndim == 1:\n        audio = audio.reshape(-1, 1)\n        was_mono = True\n    else:\n        was_mono = False\n\n    output = audio.copy()\n\n    # Apply preamp\n    if cfg.preamp is not None:\n        output = self._apply_preamp(output, cfg.preamp)\n\n    # Apply compression\n    if cfg.compressor is not None and cfg.compressor.enabled:\n        output = self._apply_compression(output, cfg.compressor)\n\n    # Apply EQ\n    if cfg.eq is not None and cfg.eq.enabled:\n        output = self._apply_eq(output, cfg.eq)\n\n    # Prevent clipping\n    max_val = np.max(np.abs(output))\n    if max_val &gt; 1.0:\n        output = output / max_val * 0.99\n\n    if was_mono:\n        output = output.flatten()\n\n    return output\n</code></pre>"},{"location":"api/audio-aug/#dataset_gen.audio_aug.chain.apply_chain","title":"apply_chain","text":"<pre><code>apply_chain(audio: ndarray, preamp_type: PreampType = CLEAN, drive: float = 0.0, compression_ratio: float = 4.0, compression_threshold_db: float = -12.0, sample_rate: int = 44100) -&gt; ndarray\n</code></pre> <p>Convenience function to apply recording chain.</p> <p>Parameters:</p> Name Type Description Default <code>audio</code> <code>ndarray</code> <p>Input audio array</p> required <code>preamp_type</code> <code>PreampType</code> <p>Type of preamp character</p> <code>CLEAN</code> <code>drive</code> <code>float</code> <p>Preamp drive/saturation (0-1)</p> <code>0.0</code> <code>compression_ratio</code> <code>float</code> <p>Compressor ratio</p> <code>4.0</code> <code>compression_threshold_db</code> <code>float</code> <p>Compressor threshold</p> <code>-12.0</code> <code>sample_rate</code> <code>int</code> <p>Audio sample rate</p> <code>44100</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Processed audio</p> Source code in <code>dataset_gen/audio_aug/chain.py</code> <pre><code>def apply_chain(\n    audio: np.ndarray,\n    preamp_type: PreampType = PreampType.CLEAN,\n    drive: float = 0.0,\n    compression_ratio: float = 4.0,\n    compression_threshold_db: float = -12.0,\n    sample_rate: int = 44100,\n) -&gt; np.ndarray:\n    \"\"\"\n    Convenience function to apply recording chain.\n\n    Args:\n        audio: Input audio array\n        preamp_type: Type of preamp character\n        drive: Preamp drive/saturation (0-1)\n        compression_ratio: Compressor ratio\n        compression_threshold_db: Compressor threshold\n        sample_rate: Audio sample rate\n\n    Returns:\n        Processed audio\n    \"\"\"\n    config = ChainConfig(\n        preamp=PreampConfig(\n            preamp_type=preamp_type,\n            drive=drive,\n        ),\n        compressor=CompressorConfig(\n            ratio=compression_ratio,\n            threshold_db=compression_threshold_db,\n        ),\n    )\n\n    chain = RecordingChain(config, sample_rate)\n    return chain.process(audio)\n</code></pre>"},{"location":"api/audio-aug/#room-simulation","title":"Room Simulation","text":"<p>Applies realistic room acoustics using convolution reverb with impulse responses or synthetic reverb generation.</p> <p>Room simulation using convolution reverb.</p> <p>This module applies realistic room acoustics to audio using impulse responses (IRs) from various room types.</p>"},{"location":"api/audio-aug/#dataset_gen.audio_aug.room.RoomType","title":"RoomType","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Types of rooms/spaces for acoustic simulation.</p>"},{"location":"api/audio-aug/#dataset_gen.audio_aug.room.RoomConfig","title":"RoomConfig  <code>dataclass</code>","text":"<pre><code>RoomConfig(room_type: RoomType = STUDIO, wet_dry_mix: float = 0.3, ir_path: Path | None = None, decay_time_sec: float = 0.5, early_reflection_delay_ms: float = 20.0, pre_delay_ms: float = 10.0, ir_trim_sec: float | None = None, ir_gain: float = 1.0)\n</code></pre> <p>Configuration for room simulation.</p>"},{"location":"api/audio-aug/#dataset_gen.audio_aug.room.RoomSimulator","title":"RoomSimulator","text":"<pre><code>RoomSimulator(config: RoomConfig | None = None, ir_directory: Path | str | None = None, sample_rate: int = 44100)\n</code></pre> <p>Apply room acoustics simulation to audio.</p> <p>Uses convolution reverb with impulse responses when available, or generates synthetic reverb based on room characteristics.</p> <p>Initialize room simulator.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>RoomConfig | None</code> <p>Room configuration</p> <code>None</code> <code>ir_directory</code> <code>Path | str | None</code> <p>Directory containing impulse response files</p> <code>None</code> <code>sample_rate</code> <code>int</code> <p>Audio sample rate</p> <code>44100</code> Source code in <code>dataset_gen/audio_aug/room.py</code> <pre><code>def __init__(\n    self,\n    config: RoomConfig | None = None,\n    ir_directory: Path | str | None = None,\n    sample_rate: int = 44100,\n):\n    \"\"\"\n    Initialize room simulator.\n\n    Args:\n        config: Room configuration\n        ir_directory: Directory containing impulse response files\n        sample_rate: Audio sample rate\n    \"\"\"\n    self.config = config or RoomConfig()\n    self.ir_directory = Path(ir_directory) if ir_directory else None\n    self.sample_rate = sample_rate\n\n    self._ir_cache: dict[str, np.ndarray] = {}\n</code></pre>"},{"location":"api/audio-aug/#dataset_gen.audio_aug.room.RoomSimulator.process","title":"process","text":"<pre><code>process(audio: ndarray, config: RoomConfig | None = None) -&gt; ndarray\n</code></pre> <p>Apply room simulation to audio.</p> <p>Parameters:</p> Name Type Description Default <code>audio</code> <code>ndarray</code> <p>Input audio (shape: [samples] or [samples, channels])</p> required <code>config</code> <code>RoomConfig | None</code> <p>Optional override configuration</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Processed audio with room acoustics applied</p> Source code in <code>dataset_gen/audio_aug/room.py</code> <pre><code>def process(\n    self,\n    audio: np.ndarray,\n    config: RoomConfig | None = None,\n) -&gt; np.ndarray:\n    \"\"\"\n    Apply room simulation to audio.\n\n    Args:\n        audio: Input audio (shape: [samples] or [samples, channels])\n        config: Optional override configuration\n\n    Returns:\n        Processed audio with room acoustics applied\n    \"\"\"\n    cfg = config or self.config\n\n    # Handle mono/stereo\n    if audio.ndim == 1:\n        audio = audio.reshape(-1, 1)\n        was_mono = True\n    else:\n        was_mono = False\n\n    # Get or generate impulse response\n    ir = self._get_impulse_response(cfg)\n\n    # Apply convolution reverb\n    wet = self._convolve(audio, ir)\n\n    # Mix dry and wet signals\n    output = self._mix(audio, wet, cfg.wet_dry_mix)\n\n    if was_mono:\n        output = output.flatten()\n\n    return output\n</code></pre>"},{"location":"api/audio-aug/#dataset_gen.audio_aug.room.RoomSimulator.get_default_wet_dry","title":"get_default_wet_dry","text":"<pre><code>get_default_wet_dry(room_type: RoomType) -&gt; float\n</code></pre> <p>Get default wet/dry mix for a room type.</p> Source code in <code>dataset_gen/audio_aug/room.py</code> <pre><code>def get_default_wet_dry(self, room_type: RoomType) -&gt; float:\n    \"\"\"Get default wet/dry mix for a room type.\"\"\"\n    return ROOM_CHARACTERISTICS.get(\n        room_type,\n        ROOM_CHARACTERISTICS[RoomType.STUDIO],\n    )[\"wet_dry_default\"]\n</code></pre>"},{"location":"api/audio-aug/#dataset_gen.audio_aug.room.apply_room","title":"apply_room","text":"<pre><code>apply_room(audio: ndarray, room_type: RoomType = STUDIO, wet_dry_mix: float | None = None, sample_rate: int = 44100, ir_directory: Path | str | None = None) -&gt; ndarray\n</code></pre> <p>Convenience function to apply room simulation.</p> <p>Parameters:</p> Name Type Description Default <code>audio</code> <code>ndarray</code> <p>Input audio array</p> required <code>room_type</code> <code>RoomType</code> <p>Type of room to simulate</p> <code>STUDIO</code> <code>wet_dry_mix</code> <code>float | None</code> <p>Wet/dry mix (None = use default for room type)</p> <code>None</code> <code>sample_rate</code> <code>int</code> <p>Audio sample rate</p> <code>44100</code> <code>ir_directory</code> <code>Path | str | None</code> <p>Directory containing IR files</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Audio with room acoustics applied</p> Source code in <code>dataset_gen/audio_aug/room.py</code> <pre><code>def apply_room(\n    audio: np.ndarray,\n    room_type: RoomType = RoomType.STUDIO,\n    wet_dry_mix: float | None = None,\n    sample_rate: int = 44100,\n    ir_directory: Path | str | None = None,\n) -&gt; np.ndarray:\n    \"\"\"\n    Convenience function to apply room simulation.\n\n    Args:\n        audio: Input audio array\n        room_type: Type of room to simulate\n        wet_dry_mix: Wet/dry mix (None = use default for room type)\n        sample_rate: Audio sample rate\n        ir_directory: Directory containing IR files\n\n    Returns:\n        Audio with room acoustics applied\n    \"\"\"\n    simulator = RoomSimulator(ir_directory=ir_directory, sample_rate=sample_rate)\n\n    if wet_dry_mix is None:\n        wet_dry_mix = simulator.get_default_wet_dry(room_type)\n\n    config = RoomConfig(\n        room_type=room_type,\n        wet_dry_mix=wet_dry_mix,\n    )\n\n    return simulator.process(audio, config)\n</code></pre>"},{"location":"api/audio-aug/#microphone-modeling","title":"Microphone Modeling","text":"<p>Simulates different microphone types, positions, and distances with characteristic frequency responses.</p> <p>Microphone simulation for realistic recording characteristics.</p> <p>This module simulates different microphone types, distances, and positions to add realistic recording characteristics.</p>"},{"location":"api/audio-aug/#dataset_gen.audio_aug.mic.MicType","title":"MicType","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Types of microphones with different frequency characteristics.</p>"},{"location":"api/audio-aug/#dataset_gen.audio_aug.mic.MicPosition","title":"MicPosition","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Microphone position relative to sound source.</p>"},{"location":"api/audio-aug/#dataset_gen.audio_aug.mic.MicConfig","title":"MicConfig  <code>dataclass</code>","text":"<pre><code>MicConfig(mic_type: MicType = CONDENSER, position: MicPosition = CENTER, distance_meters: float = 0.5, proximity_effect: bool = True, distance_rolloff: bool = True, self_noise_db: float = -80.0, output_gain: float = 1.0)\n</code></pre> <p>Configuration for microphone simulation.</p>"},{"location":"api/audio-aug/#dataset_gen.audio_aug.mic.MicSimulator","title":"MicSimulator","text":"<pre><code>MicSimulator(config: MicConfig | None = None, sample_rate: int = 44100)\n</code></pre> <p>Simulate microphone characteristics on audio.</p> <p>Applies frequency response curves, proximity effect, distance-based filtering, and mic self-noise.</p> <p>Initialize microphone simulator.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>MicConfig | None</code> <p>Microphone configuration</p> <code>None</code> <code>sample_rate</code> <code>int</code> <p>Audio sample rate</p> <code>44100</code> Source code in <code>dataset_gen/audio_aug/mic.py</code> <pre><code>def __init__(\n    self,\n    config: MicConfig | None = None,\n    sample_rate: int = 44100,\n):\n    \"\"\"\n    Initialize microphone simulator.\n\n    Args:\n        config: Microphone configuration\n        sample_rate: Audio sample rate\n    \"\"\"\n    self.config = config or MicConfig()\n    self.sample_rate = sample_rate\n\n    # Pre-compute filter for default config\n    self._filter_cache: dict[str, tuple] = {}\n</code></pre>"},{"location":"api/audio-aug/#dataset_gen.audio_aug.mic.MicSimulator.process","title":"process","text":"<pre><code>process(audio: ndarray, config: MicConfig | None = None) -&gt; ndarray\n</code></pre> <p>Apply microphone simulation to audio.</p> <p>Parameters:</p> Name Type Description Default <code>audio</code> <code>ndarray</code> <p>Input audio (shape: [samples] or [samples, channels])</p> required <code>config</code> <code>MicConfig | None</code> <p>Optional override configuration</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Audio with microphone characteristics applied</p> Source code in <code>dataset_gen/audio_aug/mic.py</code> <pre><code>def process(\n    self,\n    audio: np.ndarray,\n    config: MicConfig | None = None,\n) -&gt; np.ndarray:\n    \"\"\"\n    Apply microphone simulation to audio.\n\n    Args:\n        audio: Input audio (shape: [samples] or [samples, channels])\n        config: Optional override configuration\n\n    Returns:\n        Audio with microphone characteristics applied\n    \"\"\"\n    cfg = config or self.config\n\n    # Handle mono/stereo\n    if audio.ndim == 1:\n        audio = audio.reshape(-1, 1)\n        was_mono = True\n    else:\n        was_mono = False\n\n    output = audio.copy()\n\n    # Apply mic type frequency response\n    output = self._apply_frequency_response(output, cfg.mic_type)\n\n    # Apply position-based filtering\n    output = self._apply_position_effect(output, cfg.position)\n\n    # Apply proximity effect for close mics\n    if cfg.proximity_effect and cfg.distance_meters &lt; 0.3:\n        output = self._apply_proximity_effect(output, cfg.distance_meters)\n\n    # Apply distance rolloff\n    if cfg.distance_rolloff:\n        output = self._apply_distance_rolloff(output, cfg.distance_meters)\n\n    # Add self-noise\n    if cfg.self_noise_db &gt; -100:\n        output = self._add_self_noise(output, cfg.self_noise_db)\n\n    # Apply output gain\n    output = output * cfg.output_gain\n\n    # Prevent clipping\n    max_val = np.max(np.abs(output))\n    if max_val &gt; 1.0:\n        output = output / max_val\n\n    if was_mono:\n        output = output.flatten()\n\n    return output\n</code></pre>"},{"location":"api/audio-aug/#dataset_gen.audio_aug.mic.apply_mic","title":"apply_mic","text":"<pre><code>apply_mic(audio: ndarray, mic_type: MicType = CONDENSER, position: MicPosition = CENTER, distance: float = 0.5, sample_rate: int = 44100) -&gt; ndarray\n</code></pre> <p>Convenience function to apply mic simulation.</p> <p>Parameters:</p> Name Type Description Default <code>audio</code> <code>ndarray</code> <p>Input audio array</p> required <code>mic_type</code> <code>MicType</code> <p>Type of microphone</p> <code>CONDENSER</code> <code>position</code> <code>MicPosition</code> <p>Mic position relative to source</p> <code>CENTER</code> <code>distance</code> <code>float</code> <p>Distance in meters</p> <code>0.5</code> <code>sample_rate</code> <code>int</code> <p>Audio sample rate</p> <code>44100</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Audio with mic characteristics applied</p> Source code in <code>dataset_gen/audio_aug/mic.py</code> <pre><code>def apply_mic(\n    audio: np.ndarray,\n    mic_type: MicType = MicType.CONDENSER,\n    position: MicPosition = MicPosition.CENTER,\n    distance: float = 0.5,\n    sample_rate: int = 44100,\n) -&gt; np.ndarray:\n    \"\"\"\n    Convenience function to apply mic simulation.\n\n    Args:\n        audio: Input audio array\n        mic_type: Type of microphone\n        position: Mic position relative to source\n        distance: Distance in meters\n        sample_rate: Audio sample rate\n\n    Returns:\n        Audio with mic characteristics applied\n    \"\"\"\n    config = MicConfig(\n        mic_type=mic_type,\n        position=position,\n        distance_meters=distance,\n    )\n\n    simulator = MicSimulator(config, sample_rate)\n    return simulator.process(audio)\n</code></pre>"},{"location":"api/audio-aug/#audio-degradation","title":"Audio Degradation","text":"<p>Adds various degradations including noise, bit depth reduction, sample rate artifacts, and tape-style effects.</p> <p>Audio degradation for simulating various recording conditions.</p> <p>This module adds realistic degradations including noise, bit depth reduction, sample rate artifacts, and more.</p>"},{"location":"api/audio-aug/#dataset_gen.audio_aug.degradation.NoiseType","title":"NoiseType","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Types of background noise.</p>"},{"location":"api/audio-aug/#dataset_gen.audio_aug.degradation.NoiseConfig","title":"NoiseConfig  <code>dataclass</code>","text":"<pre><code>NoiseConfig(enabled: bool = True, noise_type: NoiseType = PINK, level_db: float = -40.0, noise_file: Path | None = None)\n</code></pre> <p>Configuration for noise injection.</p>"},{"location":"api/audio-aug/#dataset_gen.audio_aug.degradation.BitDepthConfig","title":"BitDepthConfig  <code>dataclass</code>","text":"<pre><code>BitDepthConfig(enabled: bool = False, bit_depth: int = 16, dither: bool = True)\n</code></pre> <p>Configuration for bit depth reduction.</p>"},{"location":"api/audio-aug/#dataset_gen.audio_aug.degradation.SampleRateConfig","title":"SampleRateConfig  <code>dataclass</code>","text":"<pre><code>SampleRateConfig(enabled: bool = False, target_rate: int = 22050, anti_alias: bool = True)\n</code></pre> <p>Configuration for sample rate degradation.</p>"},{"location":"api/audio-aug/#dataset_gen.audio_aug.degradation.DegradationConfig","title":"DegradationConfig  <code>dataclass</code>","text":"<pre><code>DegradationConfig(noise: NoiseConfig | None = None, bit_depth: BitDepthConfig | None = None, sample_rate: SampleRateConfig | None = None, dc_offset: float = 0.0, phase_shift: float = 0.0, wow_flutter: float = 0.0, dropout_probability: float = 0.0)\n</code></pre> <p>Full degradation configuration.</p>"},{"location":"api/audio-aug/#dataset_gen.audio_aug.degradation.AudioDegrader","title":"AudioDegrader","text":"<pre><code>AudioDegrader(config: DegradationConfig | None = None, sample_rate: int = 44100, noise_directory: Path | str | None = None)\n</code></pre> <p>Apply various degradations to audio.</p> <p>Simulates imperfect recording conditions, equipment limitations, and transmission artifacts.</p> <p>Initialize audio degrader.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>DegradationConfig | None</code> <p>Degradation configuration</p> <code>None</code> <code>sample_rate</code> <code>int</code> <p>Audio sample rate</p> <code>44100</code> <code>noise_directory</code> <code>Path | str | None</code> <p>Directory containing noise profile files</p> <code>None</code> Source code in <code>dataset_gen/audio_aug/degradation.py</code> <pre><code>def __init__(\n    self,\n    config: DegradationConfig | None = None,\n    sample_rate: int = 44100,\n    noise_directory: Path | str | None = None,\n):\n    \"\"\"\n    Initialize audio degrader.\n\n    Args:\n        config: Degradation configuration\n        sample_rate: Audio sample rate\n        noise_directory: Directory containing noise profile files\n    \"\"\"\n    self.config = config or DegradationConfig()\n    self.sample_rate = sample_rate\n    self.noise_directory = Path(noise_directory) if noise_directory else None\n\n    self._noise_cache: dict[str, np.ndarray] = {}\n</code></pre>"},{"location":"api/audio-aug/#dataset_gen.audio_aug.degradation.AudioDegrader.process","title":"process","text":"<pre><code>process(audio: ndarray, config: DegradationConfig | None = None, seed: int | None = None) -&gt; ndarray\n</code></pre> <p>Apply degradations to audio.</p> <p>Parameters:</p> Name Type Description Default <code>audio</code> <code>ndarray</code> <p>Input audio (shape: [samples] or [samples, channels])</p> required <code>config</code> <code>DegradationConfig | None</code> <p>Optional override configuration</p> <code>None</code> <code>seed</code> <code>int | None</code> <p>Random seed for reproducible degradations</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Degraded audio</p> Source code in <code>dataset_gen/audio_aug/degradation.py</code> <pre><code>def process(\n    self,\n    audio: np.ndarray,\n    config: DegradationConfig | None = None,\n    seed: int | None = None,\n) -&gt; np.ndarray:\n    \"\"\"\n    Apply degradations to audio.\n\n    Args:\n        audio: Input audio (shape: [samples] or [samples, channels])\n        config: Optional override configuration\n        seed: Random seed for reproducible degradations\n\n    Returns:\n        Degraded audio\n    \"\"\"\n    cfg = config or self.config\n    rng = np.random.default_rng(seed)\n\n    # Handle mono/stereo\n    if audio.ndim == 1:\n        audio = audio.reshape(-1, 1)\n        was_mono = True\n    else:\n        was_mono = False\n\n    output = audio.copy()\n\n    # Apply wow and flutter (before other processing)\n    if cfg.wow_flutter &gt; 0:\n        output = self._apply_wow_flutter(output, cfg.wow_flutter, rng)\n\n    # Apply dropouts\n    if cfg.dropout_probability &gt; 0:\n        output = self._apply_dropouts(output, cfg.dropout_probability, rng)\n\n    # Apply sample rate degradation\n    if cfg.sample_rate.enabled:\n        output = self._apply_sample_rate_degradation(output, cfg.sample_rate)\n\n    # Apply bit depth reduction\n    if cfg.bit_depth.enabled:\n        output = self._apply_bit_depth_reduction(output, cfg.bit_depth, rng)\n\n    # Add noise\n    if cfg.noise.enabled:\n        output = self._add_noise(output, cfg.noise, rng)\n\n    # Apply DC offset\n    if cfg.dc_offset != 0:\n        output = output + cfg.dc_offset\n\n    # Apply phase shift between channels\n    if cfg.phase_shift &gt; 0 and output.shape[1] &gt; 1:\n        output = self._apply_phase_shift(output, cfg.phase_shift)\n\n    # Clip to valid range\n    output = np.clip(output, -1.0, 1.0)\n\n    if was_mono:\n        output = output.flatten()\n\n    return output\n</code></pre>"},{"location":"api/audio-aug/#dataset_gen.audio_aug.degradation.add_noise","title":"add_noise","text":"<pre><code>add_noise(audio: ndarray, noise_type: NoiseType = PINK, level_db: float = -40.0, sample_rate: int = 44100, seed: int | None = None) -&gt; ndarray\n</code></pre> <p>Convenience function to add noise to audio.</p> <p>Parameters:</p> Name Type Description Default <code>audio</code> <code>ndarray</code> <p>Input audio array</p> required <code>noise_type</code> <code>NoiseType</code> <p>Type of noise to add</p> <code>PINK</code> <code>level_db</code> <code>float</code> <p>Noise level in dB relative to signal</p> <code>-40.0</code> <code>sample_rate</code> <code>int</code> <p>Audio sample rate</p> <code>44100</code> <code>seed</code> <code>int | None</code> <p>Random seed</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Audio with noise added</p> Source code in <code>dataset_gen/audio_aug/degradation.py</code> <pre><code>def add_noise(\n    audio: np.ndarray,\n    noise_type: NoiseType = NoiseType.PINK,\n    level_db: float = -40.0,\n    sample_rate: int = 44100,\n    seed: int | None = None,\n) -&gt; np.ndarray:\n    \"\"\"\n    Convenience function to add noise to audio.\n\n    Args:\n        audio: Input audio array\n        noise_type: Type of noise to add\n        level_db: Noise level in dB relative to signal\n        sample_rate: Audio sample rate\n        seed: Random seed\n\n    Returns:\n        Audio with noise added\n    \"\"\"\n    config = DegradationConfig(\n        noise=NoiseConfig(\n            enabled=True,\n            noise_type=noise_type,\n            level_db=level_db,\n        ),\n    )\n\n    degrader = AudioDegrader(config, sample_rate)\n    return degrader.process(audio, seed=seed)\n</code></pre>"},{"location":"api/audio-synth/","title":"Audio Synthesis Module","text":"<p>The audio synthesis module provides a FluidSynth wrapper for rendering MIDI performances to audio using SF2 soundfonts. It supports multiple soundfonts for different drum sounds (practice pad, marching snare, drum kits) and configurable synthesis parameters.</p> <p>Key features:</p> <ul> <li>Real-time MIDI event processing</li> <li>Support for multiple loaded soundfonts</li> <li>Configurable reverb and chorus effects</li> <li>Output to numpy arrays or audio files (WAV, FLAC)</li> </ul> <p>Note: This module requires FluidSynth to be installed on the system:</p> <pre><code># macOS\nbrew install fluid-synth &amp;&amp; pip install pyfluidsynth\n\n# Linux\napt install fluidsynth &amp;&amp; pip install pyfluidsynth\n</code></pre> <p>FluidSynth-based audio synthesizer for MIDI rendering.</p> <p>This module provides a wrapper around FluidSynth for converting MIDI performances to audio using various soundfonts.</p>"},{"location":"api/audio-synth/#dataset_gen.audio_synth.synthesizer.SynthConfig","title":"SynthConfig  <code>dataclass</code>","text":"<pre><code>SynthConfig(sample_rate: int = 44100, channels: int = 2, gain: float = 0.5, reverb_enabled: bool = False, reverb_room_size: float = 0.2, reverb_damping: float = 0.5, reverb_width: float = 0.5, reverb_level: float = 0.5, chorus_enabled: bool = False, chorus_depth: float = 8.0, chorus_level: float = 2.0, chorus_nr: int = 3, chorus_speed: float = 0.3)\n</code></pre> <p>Configuration for audio synthesis.</p>"},{"location":"api/audio-synth/#dataset_gen.audio_synth.synthesizer.SoundfontInfo","title":"SoundfontInfo  <code>dataclass</code>","text":"<pre><code>SoundfontInfo(path: Path, name: str, preset_count: int, sfont_id: int)\n</code></pre> <p>Information about a loaded soundfont.</p>"},{"location":"api/audio-synth/#dataset_gen.audio_synth.synthesizer.AudioSynthesizer","title":"AudioSynthesizer","text":"<pre><code>AudioSynthesizer(soundfont_path: Path | str | None = None, config: SynthConfig | None = None)\n</code></pre> <p>Synthesize audio from MIDI using FluidSynth.</p> <p>This class wraps FluidSynth to provide easy MIDI-to-audio conversion with support for multiple soundfonts.</p> <p>Initialize the synthesizer.</p> <p>Parameters:</p> Name Type Description Default <code>soundfont_path</code> <code>Path | str | None</code> <p>Path to .sf2 soundfont file (optional, can load later)</p> <code>None</code> <code>config</code> <code>SynthConfig | None</code> <p>Synthesis configuration</p> <code>None</code> Source code in <code>dataset_gen/audio_synth/synthesizer.py</code> <pre><code>def __init__(\n    self,\n    soundfont_path: Path | str | None = None,\n    config: SynthConfig | None = None,\n):\n    \"\"\"\n    Initialize the synthesizer.\n\n    Args:\n        soundfont_path: Path to .sf2 soundfont file (optional, can load later)\n        config: Synthesis configuration\n    \"\"\"\n    # Initialize attributes first to avoid __del__ errors\n    self.config = config or SynthConfig()\n    self._fs: \"fluidsynth.Synth | None\" = None\n    self._soundfonts: dict[str, SoundfontInfo] = {}\n    self._initialized = False\n\n    if not FLUIDSYNTH_AVAILABLE:\n        raise ImportError(\n            \"FluidSynth is not available. Install it with:\\n\"\n            \"  brew install fluid-synth &amp;&amp; pip install pyfluidsynth  (macOS)\\n\"\n            \"  apt install fluidsynth &amp;&amp; pip install pyfluidsynth    (Linux)\"\n        )\n\n    if soundfont_path:\n        self._init_synth()\n        self.load_soundfont(soundfont_path)\n</code></pre>"},{"location":"api/audio-synth/#dataset_gen.audio_synth.synthesizer.AudioSynthesizer.load_soundfont","title":"load_soundfont","text":"<pre><code>load_soundfont(soundfont_path: Path | str, name: str | None = None) -&gt; str\n</code></pre> <p>Load a soundfont file.</p> <p>Parameters:</p> Name Type Description Default <code>soundfont_path</code> <code>Path | str</code> <p>Path to .sf2 file</p> required <code>name</code> <code>str | None</code> <p>Optional name for referencing this soundfont</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Name of the loaded soundfont</p> Source code in <code>dataset_gen/audio_synth/synthesizer.py</code> <pre><code>def load_soundfont(\n    self,\n    soundfont_path: Path | str,\n    name: str | None = None,\n) -&gt; str:\n    \"\"\"\n    Load a soundfont file.\n\n    Args:\n        soundfont_path: Path to .sf2 file\n        name: Optional name for referencing this soundfont\n\n    Returns:\n        Name of the loaded soundfont\n    \"\"\"\n    self._init_synth()\n\n    path = Path(soundfont_path)\n    if not path.exists():\n        raise FileNotFoundError(f\"Soundfont not found: {path}\")\n\n    if name is None:\n        name = path.stem\n\n    sfont_id = self._fs.sfload(str(path))\n    if sfont_id == -1:\n        raise RuntimeError(f\"Failed to load soundfont: {path}\")\n\n    self._soundfonts[name] = SoundfontInfo(\n        path=path,\n        name=name,\n        preset_count=0,  # FluidSynth doesn't easily expose this\n        sfont_id=sfont_id,\n    )\n\n    return name\n</code></pre>"},{"location":"api/audio-synth/#dataset_gen.audio_synth.synthesizer.AudioSynthesizer.render","title":"render","text":"<pre><code>render(midi_data: bytes | None = None, midi_path: Path | str | None = None, soundfont_name: str | None = None, duration_hint_sec: float | None = None) -&gt; ndarray\n</code></pre> <p>Render MIDI to audio samples.</p> <p>Parameters:</p> Name Type Description Default <code>midi_data</code> <code>bytes | None</code> <p>Raw MIDI bytes</p> <code>None</code> <code>midi_path</code> <code>Path | str | None</code> <p>Path to MIDI file (alternative to midi_data)</p> <code>None</code> <code>soundfont_name</code> <code>str | None</code> <p>Name of soundfont to use (uses first loaded if None)</p> <code>None</code> <code>duration_hint_sec</code> <code>float | None</code> <p>Approximate duration for buffer allocation</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Audio samples as numpy array (shape: [samples, channels])</p> Source code in <code>dataset_gen/audio_synth/synthesizer.py</code> <pre><code>def render(\n    self,\n    midi_data: bytes | None = None,\n    midi_path: Path | str | None = None,\n    soundfont_name: str | None = None,\n    duration_hint_sec: float | None = None,\n) -&gt; np.ndarray:\n    \"\"\"\n    Render MIDI to audio samples.\n\n    Args:\n        midi_data: Raw MIDI bytes\n        midi_path: Path to MIDI file (alternative to midi_data)\n        soundfont_name: Name of soundfont to use (uses first loaded if None)\n        duration_hint_sec: Approximate duration for buffer allocation\n\n    Returns:\n        Audio samples as numpy array (shape: [samples, channels])\n    \"\"\"\n    if midi_data is None and midi_path is None:\n        raise ValueError(\"Must provide either midi_data or midi_path\")\n\n    if not self._soundfonts:\n        raise RuntimeError(\"No soundfont loaded. Call load_soundfont() first.\")\n\n    self._init_synth()\n\n    # Get soundfont to use\n    if soundfont_name is None:\n        soundfont_name = next(iter(self._soundfonts.keys()))\n\n    if soundfont_name not in self._soundfonts:\n        raise KeyError(f\"Soundfont '{soundfont_name}' not loaded\")\n\n    sfont_info = self._soundfonts[soundfont_name]\n\n    # Write MIDI data to temp file if needed\n    if midi_data is not None:\n        with tempfile.NamedTemporaryFile(suffix=\".mid\", delete=False) as f:\n            f.write(midi_data)\n            midi_file = f.name\n        cleanup_midi = True\n    else:\n        midi_file = str(midi_path)\n        cleanup_midi = False\n\n    try:\n        # Parse MIDI to get events and duration\n        import mido\n\n        mid = mido.MidiFile(midi_file)\n        total_duration_sec = mid.length\n\n        if duration_hint_sec:\n            total_duration_sec = max(total_duration_sec, duration_hint_sec)\n\n        # Add padding for reverb tail\n        total_duration_sec += 0.5\n\n        # Calculate buffer size\n        num_samples = int(total_duration_sec * self.config.sample_rate)\n\n        # Reset synth state\n        self._fs.system_reset()\n\n        # Select the soundfont's first program on channel 9 (drums)\n        self._fs.program_select(9, sfont_info.sfont_id, 0, 0)\n\n        # Process MIDI events\n        samples_list = []\n        current_time = 0.0\n\n        for msg in mid:\n            # Advance time\n            if msg.time &gt; 0:\n                wait_samples = int(msg.time * self.config.sample_rate)\n                if wait_samples &gt; 0:\n                    chunk = self._fs.get_samples(wait_samples)\n                    # Reshape to stereo\n                    chunk = np.array(chunk).reshape(-1, 2)\n                    samples_list.append(chunk)\n                current_time += msg.time\n\n            # Process MIDI message\n            if msg.type == \"note_on\":\n                channel = msg.channel if hasattr(msg, \"channel\") else 9\n                self._fs.noteon(channel, msg.note, msg.velocity)\n            elif msg.type == \"note_off\":\n                channel = msg.channel if hasattr(msg, \"channel\") else 9\n                self._fs.noteoff(channel, msg.note)\n            elif msg.type == \"control_change\":\n                channel = msg.channel if hasattr(msg, \"channel\") else 9\n                self._fs.cc(channel, msg.control, msg.value)\n\n        # Render remaining samples (reverb tail)\n        remaining = num_samples - sum(len(s) for s in samples_list)\n        if remaining &gt; 0:\n            chunk = self._fs.get_samples(remaining)\n            chunk = np.array(chunk).reshape(-1, 2)\n            samples_list.append(chunk)\n\n        # Concatenate all samples\n        if samples_list:\n            audio = np.concatenate(samples_list, axis=0)\n        else:\n            audio = np.zeros((num_samples, 2), dtype=np.float32)\n\n        # Normalize to float32 [-1, 1]\n        # FluidSynth returns int16 samples\n        audio = audio.astype(np.float32) / 32768.0\n\n        return audio\n\n    finally:\n        if cleanup_midi:\n            Path(midi_file).unlink(missing_ok=True)\n</code></pre>"},{"location":"api/audio-synth/#dataset_gen.audio_synth.synthesizer.AudioSynthesizer.render_to_file","title":"render_to_file","text":"<pre><code>render_to_file(output_path: Path | str, midi_data: bytes | None = None, midi_path: Path | str | None = None, soundfont_name: str | None = None, format: Literal['wav', 'flac'] = 'flac', subtype: str | None = None) -&gt; Path\n</code></pre> <p>Render MIDI to an audio file.</p> <p>Parameters:</p> Name Type Description Default <code>output_path</code> <code>Path | str</code> <p>Output file path</p> required <code>midi_data</code> <code>bytes | None</code> <p>Raw MIDI bytes</p> <code>None</code> <code>midi_path</code> <code>Path | str | None</code> <p>Path to MIDI file</p> <code>None</code> <code>soundfont_name</code> <code>str | None</code> <p>Soundfont to use</p> <code>None</code> <code>format</code> <code>Literal['wav', 'flac']</code> <p>Output format ('wav' or 'flac')</p> <code>'flac'</code> <code>subtype</code> <code>str | None</code> <p>Soundfile subtype (e.g., 'PCM_16', 'PCM_24')</p> <code>None</code> <p>Returns:</p> Type Description <code>Path</code> <p>Path to the output file</p> Source code in <code>dataset_gen/audio_synth/synthesizer.py</code> <pre><code>def render_to_file(\n    self,\n    output_path: Path | str,\n    midi_data: bytes | None = None,\n    midi_path: Path | str | None = None,\n    soundfont_name: str | None = None,\n    format: Literal[\"wav\", \"flac\"] = \"flac\",\n    subtype: str | None = None,\n) -&gt; Path:\n    \"\"\"\n    Render MIDI to an audio file.\n\n    Args:\n        output_path: Output file path\n        midi_data: Raw MIDI bytes\n        midi_path: Path to MIDI file\n        soundfont_name: Soundfont to use\n        format: Output format ('wav' or 'flac')\n        subtype: Soundfile subtype (e.g., 'PCM_16', 'PCM_24')\n\n    Returns:\n        Path to the output file\n    \"\"\"\n    audio = self.render(\n        midi_data=midi_data,\n        midi_path=midi_path,\n        soundfont_name=soundfont_name,\n    )\n\n    output_path = Path(output_path)\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n\n    # Default subtypes\n    if subtype is None:\n        subtype = \"PCM_24\" if format == \"flac\" else \"PCM_16\"\n\n    sf.write(\n        str(output_path),\n        audio,\n        self.config.sample_rate,\n        format=format.upper(),\n        subtype=subtype,\n    )\n\n    return output_path\n</code></pre>"},{"location":"api/audio-synth/#dataset_gen.audio_synth.synthesizer.AudioSynthesizer.close","title":"close","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Clean up FluidSynth resources.</p> Source code in <code>dataset_gen/audio_synth/synthesizer.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Clean up FluidSynth resources.\"\"\"\n    if self._fs is not None:\n        self._fs.delete()\n        self._fs = None\n        self._initialized = False\n        self._soundfonts.clear()\n</code></pre>"},{"location":"api/audio-synth/#dataset_gen.audio_synth.synthesizer.render_midi_to_audio","title":"render_midi_to_audio","text":"<pre><code>render_midi_to_audio(midi_data: bytes | None = None, midi_path: Path | str | None = None, soundfont_path: Path | str | None = None, output_path: Path | str | None = None, sample_rate: int = 44100, format: Literal['wav', 'flac'] = 'flac') -&gt; ndarray | Path\n</code></pre> <p>Convenience function to render MIDI to audio.</p> <p>Parameters:</p> Name Type Description Default <code>midi_data</code> <code>bytes | None</code> <p>Raw MIDI bytes</p> <code>None</code> <code>midi_path</code> <code>Path | str | None</code> <p>Path to MIDI file</p> <code>None</code> <code>soundfont_path</code> <code>Path | str | None</code> <p>Path to soundfont (required)</p> <code>None</code> <code>output_path</code> <code>Path | str | None</code> <p>If provided, write to file and return path</p> <code>None</code> <code>sample_rate</code> <code>int</code> <p>Sample rate in Hz</p> <code>44100</code> <code>format</code> <code>Literal['wav', 'flac']</code> <p>Output format if writing to file</p> <code>'flac'</code> <p>Returns:</p> Type Description <code>ndarray | Path</code> <p>Audio samples as numpy array, or Path if output_path provided</p> Source code in <code>dataset_gen/audio_synth/synthesizer.py</code> <pre><code>def render_midi_to_audio(\n    midi_data: bytes | None = None,\n    midi_path: Path | str | None = None,\n    soundfont_path: Path | str | None = None,\n    output_path: Path | str | None = None,\n    sample_rate: int = 44100,\n    format: Literal[\"wav\", \"flac\"] = \"flac\",\n) -&gt; np.ndarray | Path:\n    \"\"\"\n    Convenience function to render MIDI to audio.\n\n    Args:\n        midi_data: Raw MIDI bytes\n        midi_path: Path to MIDI file\n        soundfont_path: Path to soundfont (required)\n        output_path: If provided, write to file and return path\n        sample_rate: Sample rate in Hz\n        format: Output format if writing to file\n\n    Returns:\n        Audio samples as numpy array, or Path if output_path provided\n    \"\"\"\n    if soundfont_path is None:\n        raise ValueError(\"soundfont_path is required\")\n\n    config = SynthConfig(sample_rate=sample_rate)\n\n    with AudioSynthesizer(soundfont_path, config) as synth:\n        if output_path:\n            return synth.render_to_file(\n                output_path,\n                midi_data=midi_data,\n                midi_path=midi_path,\n                format=format,\n            )\n        else:\n            return synth.render(\n                midi_data=midi_data,\n                midi_path=midi_path,\n            )\n</code></pre>"},{"location":"api/labels/","title":"Labels Module","text":"<p>The labels module computes hierarchical labels from generated performances at three levels:</p> <ol> <li>Stroke-level: Individual timing and velocity measurements for each stroke</li> <li>Measure-level: Aggregate statistics within each measure (timing consistency, hand balance)</li> <li>Exercise-level: Overall performance scores on a 0-100 scale</li> </ol> <p>Labels are designed for ML training with features like:</p> <ul> <li>Corrected timing errors for grace notes (relative to ideal flam spacing, not grid)</li> <li>Perceptual (sigmoid) scaling for timing accuracy scores</li> <li>Tier confidence scores indicating label ambiguity near skill tier boundaries</li> </ul>"},{"location":"api/labels/#schema","title":"Schema","text":"<p>Pydantic models defining the hierarchical label structure.</p> <p>Pydantic models for hierarchical label schema.</p> <p>Labels are computed at three levels: 1. Per-stroke: Individual timing and velocity measurements 2. Per-measure: Aggregate statistics within each measure 3. Per-exercise: Overall performance scores</p>"},{"location":"api/labels/#dataset_gen.labels.schema.StrokeLabel","title":"StrokeLabel","text":"<p>               Bases: <code>BaseModel</code></p> <p>Labels for a single stroke event.</p>"},{"location":"api/labels/#dataset_gen.labels.schema.MeasureLabel","title":"MeasureLabel","text":"<p>               Bases: <code>BaseModel</code></p> <p>Aggregate labels for a single measure.</p>"},{"location":"api/labels/#dataset_gen.labels.schema.ExerciseScores","title":"ExerciseScores","text":"<p>               Bases: <code>BaseModel</code></p> <p>Composite scores for the entire exercise/performance.</p>"},{"location":"api/labels/#dataset_gen.labels.schema.AudioAugmentation","title":"AudioAugmentation","text":"<p>               Bases: <code>BaseModel</code></p> <p>Parameters describing audio augmentation applied to the sample.</p>"},{"location":"api/labels/#dataset_gen.labels.schema.Sample","title":"Sample","text":"<p>               Bases: <code>BaseModel</code></p> <p>Complete sample with all hierarchical labels.</p> <p>This is the primary data structure for the dataset, containing everything needed for ML training.</p>"},{"location":"api/labels/#label-computation","title":"Label Computation","text":"<p>Functions for computing labels from stroke events and performances.</p> <p>Compute hierarchical labels from stroke events.</p> <p>This module transforms raw stroke events into the three-level label hierarchy used for ML training.</p>"},{"location":"api/labels/#dataset_gen.labels.compute.compute_stroke_labels","title":"compute_stroke_labels","text":"<pre><code>compute_stroke_labels(events: list[StrokeEvent]) -&gt; list[StrokeLabel]\n</code></pre> <p>Convert stroke events to stroke labels.</p> <p>For grace notes, timing_error_ms is recomputed as deviation from ideal flam spacing (25-35ms), not grid deviation. This provides a meaningful timing quality metric for ornamental strokes.</p> <p>Parameters:</p> Name Type Description Default <code>events</code> <code>list[StrokeEvent]</code> <p>List of stroke events from MIDI generation</p> required <p>Returns:</p> Type Description <code>list[StrokeLabel]</code> <p>List of StrokeLabel objects</p> Source code in <code>dataset_gen/labels/compute.py</code> <pre><code>def compute_stroke_labels(events: list[StrokeEvent]) -&gt; list[StrokeLabel]:\n    \"\"\"\n    Convert stroke events to stroke labels.\n\n    For grace notes, timing_error_ms is recomputed as deviation from ideal\n    flam spacing (25-35ms), not grid deviation. This provides a meaningful\n    timing quality metric for ornamental strokes.\n\n    Args:\n        events: List of stroke events from MIDI generation\n\n    Returns:\n        List of StrokeLabel objects\n    \"\"\"\n    # Build index lookup for finding primary strokes\n    events_by_index = {e.index: e for e in events}\n\n    # Ideal flam spacing (center of acceptable range)\n    IDEAL_FLAM_SPACING_MS = 30.0\n\n    labels = []\n    for event in events:\n        # Compute grace-note-specific fields\n        flam_spacing_ms = None\n        timing_error = event.timing_error_ms\n        parent_index = event.parent_stroke_index\n\n        if event.is_grace_note and parent_index is not None:\n            primary = events_by_index.get(parent_index)\n            if primary is not None:\n                # Compute actual flam spacing (primary - grace)\n                flam_spacing_ms = primary.actual_time_ms - event.actual_time_ms\n\n                # Recompute timing_error as deviation from ideal spacing\n                # Positive = too wide (grace too early), Negative = too tight\n                timing_error = flam_spacing_ms - IDEAL_FLAM_SPACING_MS\n\n        label = StrokeLabel(\n            index=event.index,\n            hand=\"R\" if event.hand == Hand.RIGHT else \"L\",\n            stroke_type=event.stroke_type.value,\n            intended_time_ms=event.intended_time_ms,\n            actual_time_ms=event.actual_time_ms,\n            timing_error_ms=timing_error,\n            intended_velocity=event.intended_velocity,\n            actual_velocity=event.actual_velocity,\n            velocity_error=event.velocity_error,\n            is_grace_note=event.is_grace_note,\n            is_accent=event.stroke_type == StrokeType.ACCENT,\n            diddle_position=event.diddle_position,\n            flam_spacing_ms=flam_spacing_ms,\n            parent_stroke_index=parent_index,\n        )\n        labels.append(label)\n    return labels\n</code></pre>"},{"location":"api/labels/#dataset_gen.labels.compute.compute_measure_labels","title":"compute_measure_labels","text":"<pre><code>compute_measure_labels(stroke_labels: list[StrokeLabel], strokes_per_measure: int) -&gt; list[MeasureLabel]\n</code></pre> <p>Compute per-measure aggregate statistics.</p> <p>Parameters:</p> Name Type Description Default <code>stroke_labels</code> <code>list[StrokeLabel]</code> <p>List of stroke labels (with corrected timing for grace notes)</p> required <code>strokes_per_measure</code> <code>int</code> <p>Number of strokes in each measure</p> required <p>Returns:</p> Type Description <code>list[MeasureLabel]</code> <p>List of MeasureLabel objects</p> Source code in <code>dataset_gen/labels/compute.py</code> <pre><code>def compute_measure_labels(\n    stroke_labels: list[StrokeLabel],\n    strokes_per_measure: int,\n) -&gt; list[MeasureLabel]:\n    \"\"\"\n    Compute per-measure aggregate statistics.\n\n    Args:\n        stroke_labels: List of stroke labels (with corrected timing for grace notes)\n        strokes_per_measure: Number of strokes in each measure\n\n    Returns:\n        List of MeasureLabel objects\n    \"\"\"\n    if not stroke_labels:\n        return []\n\n    measures = []\n    n_measures = (len(stroke_labels) + strokes_per_measure - 1) // strokes_per_measure\n\n    for m in range(n_measures):\n        start_idx = m * strokes_per_measure\n        end_idx = min((m + 1) * strokes_per_measure, len(stroke_labels))\n        measure_strokes = stroke_labels[start_idx:end_idx]\n\n        if not measure_strokes:\n            continue\n\n        # Timing statistics (using corrected timing_error_ms from labels)\n        timing_errors = [s.timing_error_ms for s in measure_strokes]\n        timing_mean = np.mean(timing_errors)\n        timing_std = np.std(timing_errors)\n        timing_max = np.max(np.abs(timing_errors))\n\n        # Velocity statistics\n        velocities = [s.actual_velocity for s in measure_strokes]\n        vel_mean = np.mean(velocities)\n        vel_std = np.std(velocities)\n        vel_consistency = 1 - (vel_std / vel_mean) if vel_mean &gt; 0 else 0\n\n        # Hand balance within measure\n        left_strokes = [s for s in measure_strokes if s.hand == \"L\"]\n        right_strokes = [s for s in measure_strokes if s.hand == \"R\"]\n\n        lr_ratio = None\n        lr_timing_diff = None\n        if left_strokes and right_strokes:\n            left_vel = np.mean([s.actual_velocity for s in left_strokes])\n            right_vel = np.mean([s.actual_velocity for s in right_strokes])\n            lr_ratio = left_vel / right_vel if right_vel &gt; 0 else 1.0\n\n            left_timing = np.mean([s.timing_error_ms for s in left_strokes])\n            right_timing = np.mean([s.timing_error_ms for s in right_strokes])\n            lr_timing_diff = left_timing - right_timing\n\n        measure = MeasureLabel(\n            index=m,\n            stroke_start=start_idx,\n            stroke_end=end_idx,\n            timing_mean_error_ms=timing_mean,\n            timing_std_ms=timing_std,\n            timing_max_error_ms=timing_max,\n            velocity_mean=vel_mean,\n            velocity_std=vel_std,\n            velocity_consistency=vel_consistency,\n            lr_velocity_ratio=lr_ratio,\n            lr_timing_diff_ms=lr_timing_diff,\n        )\n        measures.append(measure)\n\n    return measures\n</code></pre>"},{"location":"api/labels/#dataset_gen.labels.compute.compute_exercise_scores","title":"compute_exercise_scores","text":"<pre><code>compute_exercise_scores(stroke_labels: list[StrokeLabel], events: list[StrokeEvent], rudiment: Rudiment) -&gt; ExerciseScores\n</code></pre> <p>Compute overall exercise scores from stroke labels and events.</p> <p>Uses stroke labels for timing metrics (corrected for grace notes) and events for rudiment-specific calculations that need full event data.</p> <p>Scores are normalized to 0-100 where 100 is perfect performance.</p> <p>Parameters:</p> Name Type Description Default <code>stroke_labels</code> <code>list[StrokeLabel]</code> <p>List of stroke labels (with corrected timing)</p> required <code>events</code> <code>list[StrokeEvent]</code> <p>List of stroke events (for rudiment-specific calculations)</p> required <code>rudiment</code> <code>Rudiment</code> <p>The rudiment being performed</p> required <p>Returns:</p> Type Description <code>ExerciseScores</code> <p>ExerciseScores object</p> Source code in <code>dataset_gen/labels/compute.py</code> <pre><code>def compute_exercise_scores(\n    stroke_labels: list[StrokeLabel],\n    events: list[StrokeEvent],\n    rudiment: Rudiment,\n) -&gt; ExerciseScores:\n    \"\"\"\n    Compute overall exercise scores from stroke labels and events.\n\n    Uses stroke labels for timing metrics (corrected for grace notes)\n    and events for rudiment-specific calculations that need full event data.\n\n    Scores are normalized to 0-100 where 100 is perfect performance.\n\n    Args:\n        stroke_labels: List of stroke labels (with corrected timing)\n        events: List of stroke events (for rudiment-specific calculations)\n        rudiment: The rudiment being performed\n\n    Returns:\n        ExerciseScores object\n    \"\"\"\n    if not stroke_labels:\n        return _empty_scores()\n\n    # === TIMING SCORES (use corrected values from labels) ===\n    # Uses perceptual (sigmoid) scaling to match human perception:\n    # - Errors &lt;10ms are nearly imperceptible (high score)\n    # - Errors 20-30ms are noticeable but acceptable (medium score)\n    # - Errors &gt;50ms are clearly audible (low score)\n\n    timing_errors = np.array([s.timing_error_ms for s in stroke_labels])\n    timing_errors_abs = np.abs(timing_errors)\n\n    # Timing accuracy: sigmoid scaling (perceptual)\n    # Center at 25ms (50% score), steepness controlled by /10\n    mean_abs_error = np.mean(timing_errors_abs)\n    timing_accuracy = 100 * (1 / (1 + np.exp((mean_abs_error - 25) / 10)))\n\n    # Timing consistency: sigmoid scaling\n    # Center at 15ms std (50% score)\n    timing_std = np.std(timing_errors)\n    timing_consistency = 100 * (1 / (1 + np.exp((timing_std - 15) / 8)))\n\n    # Tempo stability: based on drift over time (for non-grace strokes only)\n    non_grace_labels = [s for s in stroke_labels if not s.is_grace_note]\n    if len(non_grace_labels) &gt; 1:\n        times = np.array([s.intended_time_ms for s in non_grace_labels])\n        errors = np.array([s.timing_error_ms for s in non_grace_labels])\n        slope = np.polyfit(times, errors, 1)[0] * 1000  # ms/second\n        tempo_stability = max(0, 100 - abs(slope) * 10)\n    else:\n        tempo_stability = 100\n\n    # Subdivision evenness: check inter-onset intervals\n    if len(stroke_labels) &gt; 1:\n        actual_times = np.array([s.actual_time_ms for s in stroke_labels])\n        intended_times = np.array([s.intended_time_ms for s in stroke_labels])\n        actual_ioi = np.diff(actual_times)\n        intended_ioi = np.diff(intended_times)\n        ioi_ratios = actual_ioi / np.maximum(intended_ioi, 1)\n        ioi_variance = np.std(ioi_ratios)\n        subdivision_evenness = max(0, 100 - ioi_variance * 100)\n    else:\n        subdivision_evenness = 100\n\n    # === DYNAMICS SCORES ===\n\n    velocities = np.array([s.actual_velocity for s in stroke_labels])\n\n    # Velocity control: consistency of dynamics\n    vel_std = np.std(velocities)\n    velocity_control = max(0, 100 - vel_std * 2)\n\n    # Accent differentiation\n    accent_strokes = [s for s in stroke_labels if s.is_accent]\n    tap_strokes = [s for s in stroke_labels if s.stroke_type == \"tap\"]\n\n    if accent_strokes and tap_strokes:\n        accent_vel = np.mean([s.actual_velocity for s in accent_strokes])\n        tap_vel = np.mean([s.actual_velocity for s in tap_strokes])\n        diff_db = (accent_vel - tap_vel) / 127 * 20  # Rough dB approximation\n        accent_differentiation = min(100, max(0, diff_db * 8))\n    else:\n        accent_differentiation = 80\n\n    # Accent accuracy: are accents on correct beats?\n    if accent_strokes:\n        correct_accents = sum(1 for s in accent_strokes if s.actual_velocity &gt; np.mean(velocities))\n        accent_accuracy = (correct_accents / len(accent_strokes)) * 100\n    else:\n        accent_accuracy = 100\n\n    # === HAND BALANCE SCORES ===\n    # Now incorporates BOTH velocity AND timing balance (50% each)\n    # This addresses the ceiling effect where velocity-only was nearly always high\n\n    left_strokes = [s for s in stroke_labels if s.hand == \"L\"]\n    right_strokes = [s for s in stroke_labels if s.hand == \"R\"]\n\n    if left_strokes and right_strokes:\n        # Velocity balance (0-100)\n        left_vel = np.mean([s.actual_velocity for s in left_strokes])\n        right_vel = np.mean([s.actual_velocity for s in right_strokes])\n        vel_ratio = min(left_vel, right_vel) / max(left_vel, right_vel)\n        velocity_balance = vel_ratio * 100\n\n        # Timing balance (0-100): based on timing consistency difference between hands\n        left_timing_abs = np.mean([np.abs(s.timing_error_ms) for s in left_strokes])\n        right_timing_abs = np.mean([np.abs(s.timing_error_ms) for s in right_strokes])\n\n        # Timing ratio: how similar are the hands in timing accuracy?\n        # Perfect balance = both hands have same mean absolute error\n        max_timing = max(left_timing_abs, right_timing_abs, 1)  # Avoid div by zero\n        min_timing = min(left_timing_abs, right_timing_abs)\n        timing_ratio = min_timing / max_timing\n        timing_balance = timing_ratio * 100\n\n        # Combined hand balance: 50% velocity, 50% timing\n        hand_balance = 0.5 * velocity_balance + 0.5 * timing_balance\n\n        # Weak hand index: which hand is weaker overall?\n        # 0 = left weaker, 100 = right weaker, 50 = balanced\n        total_timing = left_timing_abs + right_timing_abs\n        weak_hand_index = (left_timing_abs / total_timing * 100) if total_timing &gt; 0 else 50\n    else:\n        hand_balance = 100\n        weak_hand_index = 50\n\n    # === RUDIMENT-SPECIFIC SCORES (use events for these) ===\n\n    flam_quality = None\n    diddle_quality = None\n    roll_sustain = None\n\n    if rudiment.category == RudimentCategory.FLAM:\n        flam_quality = _compute_flam_quality(events)\n\n    if rudiment.category in (RudimentCategory.DIDDLE, RudimentCategory.ROLL):\n        diddle_quality = _compute_diddle_quality(events)\n\n    if rudiment.category == RudimentCategory.ROLL:\n        roll_sustain = _compute_roll_sustain(events)\n\n    # === DERIVED SCORES ===\n\n    groove_feel = compute_groove_feel_proxy(events)\n\n    # Overall score: weighted average of components\n    weights = {\n        \"timing_accuracy\": 0.2,\n        \"timing_consistency\": 0.15,\n        \"tempo_stability\": 0.1,\n        \"subdivision_evenness\": 0.1,\n        \"velocity_control\": 0.1,\n        \"accent_differentiation\": 0.1,\n        \"accent_accuracy\": 0.1,\n        \"hand_balance\": 0.15,\n    }\n    overall = sum(\n        weights[k] * v\n        for k, v in {\n            \"timing_accuracy\": timing_accuracy,\n            \"timing_consistency\": timing_consistency,\n            \"tempo_stability\": tempo_stability,\n            \"subdivision_evenness\": subdivision_evenness,\n            \"velocity_control\": velocity_control,\n            \"accent_differentiation\": accent_differentiation,\n            \"accent_accuracy\": accent_accuracy,\n            \"hand_balance\": hand_balance,\n        }.items()\n    )\n\n    return ExerciseScores(\n        timing_accuracy=timing_accuracy,\n        timing_consistency=timing_consistency,\n        tempo_stability=tempo_stability,\n        subdivision_evenness=subdivision_evenness,\n        velocity_control=velocity_control,\n        accent_differentiation=accent_differentiation,\n        accent_accuracy=accent_accuracy,\n        hand_balance=hand_balance,\n        weak_hand_index=weak_hand_index,\n        flam_quality=flam_quality,\n        diddle_quality=diddle_quality,\n        roll_sustain=roll_sustain,\n        groove_feel_proxy=groove_feel,\n        overall_score=overall,\n    )\n</code></pre>"},{"location":"api/labels/#dataset_gen.labels.compute.compute_tier_confidence","title":"compute_tier_confidence","text":"<pre><code>compute_tier_confidence(overall_score: float, skill_tier: str) -&gt; float\n</code></pre> <p>Compute confidence that the skill_tier label is unambiguous.</p> <p>Uses Gaussian likelihood based on expected score distributions per tier. Returns 0-1 where: - 1.0 = score is at the center of the tier's expected distribution - 0.0 = score is far from the tier's expected range (likely mislabeled)</p> <p>A sample near the boundary between tiers will have lower confidence, indicating potential label noise for classification tasks.</p> Source code in <code>dataset_gen/labels/compute.py</code> <pre><code>def compute_tier_confidence(overall_score: float, skill_tier: str) -&gt; float:\n    \"\"\"\n    Compute confidence that the skill_tier label is unambiguous.\n\n    Uses Gaussian likelihood based on expected score distributions per tier.\n    Returns 0-1 where:\n    - 1.0 = score is at the center of the tier's expected distribution\n    - 0.0 = score is far from the tier's expected range (likely mislabeled)\n\n    A sample near the boundary between tiers will have lower confidence,\n    indicating potential label noise for classification tasks.\n    \"\"\"\n    if skill_tier not in TIER_SCORE_DISTRIBUTIONS:\n        return 0.5  # Unknown tier\n\n    mean, std = TIER_SCORE_DISTRIBUTIONS[skill_tier]\n\n    # Gaussian likelihood (normalized to 0-1)\n    # At mean: confidence = 1.0\n    # At 2 std from mean: confidence \u2248 0.14\n    z_score = abs(overall_score - mean) / std\n    confidence = np.exp(-0.5 * z_score**2)\n\n    return float(confidence)\n</code></pre>"},{"location":"api/labels/#dataset_gen.labels.compute.compute_skill_tier_binary","title":"compute_skill_tier_binary","text":"<pre><code>compute_skill_tier_binary(skill_tier: str) -&gt; str\n</code></pre> <p>Convert 4-class skill tier to 2-class binary label.</p> <p>This alternative labeling reduces class overlap issues: - novice: beginner, intermediate - skilled: advanced, professional</p> <p>For use when 4-class classification ceiling is too low due to label noise.</p> Source code in <code>dataset_gen/labels/compute.py</code> <pre><code>def compute_skill_tier_binary(skill_tier: str) -&gt; str:\n    \"\"\"\n    Convert 4-class skill tier to 2-class binary label.\n\n    This alternative labeling reduces class overlap issues:\n    - novice: beginner, intermediate\n    - skilled: advanced, professional\n\n    For use when 4-class classification ceiling is too low due to label noise.\n    \"\"\"\n    if skill_tier in (\"beginner\", \"intermediate\"):\n        return \"novice\"\n    else:\n        return \"skilled\"\n</code></pre>"},{"location":"api/labels/#dataset_gen.labels.compute.compute_sample_labels","title":"compute_sample_labels","text":"<pre><code>compute_sample_labels(performance: GeneratedPerformance, rudiment: Rudiment, profile: PlayerProfile) -&gt; Sample\n</code></pre> <p>Compute all hierarchical labels for a generated performance.</p> <p>Parameters:</p> Name Type Description Default <code>performance</code> <code>GeneratedPerformance</code> <p>The generated MIDI performance</p> required <code>rudiment</code> <code>Rudiment</code> <p>The rudiment definition</p> required <code>profile</code> <code>PlayerProfile</code> <p>The player profile</p> required <p>Returns:</p> Type Description <code>Sample</code> <p>Complete Sample object with all labels</p> Source code in <code>dataset_gen/labels/compute.py</code> <pre><code>def compute_sample_labels(\n    performance: GeneratedPerformance,\n    rudiment: Rudiment,\n    profile: PlayerProfile,\n) -&gt; Sample:\n    \"\"\"\n    Compute all hierarchical labels for a generated performance.\n\n    Args:\n        performance: The generated MIDI performance\n        rudiment: The rudiment definition\n        profile: The player profile\n\n    Returns:\n        Complete Sample object with all labels\n    \"\"\"\n    stroke_labels = compute_stroke_labels(performance.strokes)\n\n    # Calculate strokes per measure from rudiment pattern\n    strokes_per_measure = len(rudiment.pattern.strokes)\n\n    # Use stroke_labels for aggregations (has corrected timing for grace notes)\n    measure_labels = compute_measure_labels(stroke_labels, strokes_per_measure)\n    exercise_scores = compute_exercise_scores(stroke_labels, performance.strokes, rudiment)\n\n    # Compute tier confidence based on how central the score is to the tier's distribution\n    skill_tier_str = profile.skill_tier.value\n    tier_confidence = compute_tier_confidence(exercise_scores.overall_score, skill_tier_str)\n    exercise_scores.tier_confidence = tier_confidence\n\n    # Compute binary skill tier (novice vs skilled)\n    skill_tier_binary = compute_skill_tier_binary(skill_tier_str)\n\n    return Sample(\n        sample_id=performance.id,\n        profile_id=profile.id,\n        rudiment_slug=rudiment.slug,\n        tempo_bpm=performance.tempo_bpm,\n        duration_sec=performance.duration_sec,\n        num_cycles=performance.num_cycles,\n        skill_tier=skill_tier_str,\n        skill_tier_binary=skill_tier_binary,\n        dominant_hand=profile.dominant_hand,\n        strokes=stroke_labels,\n        measures=measure_labels,\n        exercise_scores=exercise_scores,\n    )\n</code></pre>"},{"location":"api/labels/#groove-analysis","title":"Groove Analysis","text":"<p>Groove feel heuristic proxy computation that distinguishes intentional microtiming from sloppy playing.</p> <p>Groove feel heuristic proxy computation.</p> <p>Since \"groove\" is subjective and hard to define, we use computed proxies that capture aspects of intentional vs. sloppy microtiming.</p>"},{"location":"api/labels/#dataset_gen.labels.groove.GrooveMetrics","title":"GrooveMetrics  <code>dataclass</code>","text":"<pre><code>GrooveMetrics(pattern_consistency: float, swing_ratio: float | None, accent_timing_bias: float, groove_feel: float)\n</code></pre> <p>Detailed groove analysis metrics.</p>"},{"location":"api/labels/#dataset_gen.labels.groove.compute_groove_feel_proxy","title":"compute_groove_feel_proxy","text":"<pre><code>compute_groove_feel_proxy(events: list[StrokeEvent]) -&gt; float\n</code></pre> <p>Compute a groove feel proxy score.</p> <p>The proxy distinguishes between: - Intentional microtiming (groove): Consistent patterns of deviation - Sloppy playing: Random, inconsistent errors</p> <p>Parameters:</p> Name Type Description Default <code>events</code> <code>list[StrokeEvent]</code> <p>List of stroke events</p> required <p>Returns:</p> Type Description <code>float</code> <p>Score from 0 (sloppy) to 1 (good groove or perfectly accurate)</p> Source code in <code>dataset_gen/labels/groove.py</code> <pre><code>def compute_groove_feel_proxy(events: list[StrokeEvent]) -&gt; float:\n    \"\"\"\n    Compute a groove feel proxy score.\n\n    The proxy distinguishes between:\n    - Intentional microtiming (groove): Consistent patterns of deviation\n    - Sloppy playing: Random, inconsistent errors\n\n    Args:\n        events: List of stroke events\n\n    Returns:\n        Score from 0 (sloppy) to 1 (good groove or perfectly accurate)\n    \"\"\"\n    if len(events) &lt; 4:\n        return 0.5  # Not enough data\n\n    metrics = compute_groove_metrics(events)\n    return metrics.groove_feel\n</code></pre>"},{"location":"api/labels/#dataset_gen.labels.groove.compute_groove_metrics","title":"compute_groove_metrics","text":"<pre><code>compute_groove_metrics(events: list[StrokeEvent]) -&gt; GrooveMetrics\n</code></pre> <p>Compute detailed groove metrics.</p> <p>Parameters:</p> Name Type Description Default <code>events</code> <code>list[StrokeEvent]</code> <p>List of stroke events</p> required <p>Returns:</p> Type Description <code>GrooveMetrics</code> <p>GrooveMetrics object with all computed values</p> Source code in <code>dataset_gen/labels/groove.py</code> <pre><code>def compute_groove_metrics(events: list[StrokeEvent]) -&gt; GrooveMetrics:\n    \"\"\"\n    Compute detailed groove metrics.\n\n    Args:\n        events: List of stroke events\n\n    Returns:\n        GrooveMetrics object with all computed values\n    \"\"\"\n    timing_errors = np.array([e.timing_error_ms for e in events])\n\n    # 1. PATTERN CONSISTENCY\n    # Use autocorrelation to detect systematic timing patterns\n    # High autocorrelation = consistent patterns (good groove or good timing)\n    # Low autocorrelation = random errors (sloppy)\n    pattern_consistency = _compute_autocorrelation(timing_errors)\n\n    # 2. SWING DETECTION\n    # Check for consistent push/pull on alternate notes\n    swing_ratio = _detect_swing(events)\n\n    # 3. ACCENT TIMING BIAS\n    # Check if accented notes consistently lead or lag\n    accent_bias = _compute_accent_timing_bias(events)\n\n    # 4. COMBINE INTO GROOVE FEEL\n    # High groove = either accurate OR consistently deviant\n    # Low groove = random, inconsistent errors\n\n    # Base score on timing accuracy\n    mean_abs_error = np.mean(np.abs(timing_errors))\n    accuracy_score = max(0, 1 - mean_abs_error / 50)  # 0-50ms -&gt; 1-0\n\n    # Boost for pattern consistency (intentional deviations)\n    consistency_boost = pattern_consistency * 0.3\n\n    # Small boost for detected swing\n    swing_boost = 0.1 if swing_ratio is not None and 0.55 &lt; swing_ratio &lt; 0.75 else 0\n\n    groove_feel = min(1.0, accuracy_score + consistency_boost + swing_boost)\n\n    return GrooveMetrics(\n        pattern_consistency=pattern_consistency,\n        swing_ratio=swing_ratio,\n        accent_timing_bias=accent_bias,\n        groove_feel=groove_feel,\n    )\n</code></pre>"},{"location":"api/labels/#dataset_gen.labels.groove.analyze_groove_quality","title":"analyze_groove_quality","text":"<pre><code>analyze_groove_quality(events: list[StrokeEvent]) -&gt; dict\n</code></pre> <p>Produce a detailed groove analysis for debugging/inspection.</p> <p>Parameters:</p> Name Type Description Default <code>events</code> <code>list[StrokeEvent]</code> <p>List of stroke events</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with detailed groove analysis</p> Source code in <code>dataset_gen/labels/groove.py</code> <pre><code>def analyze_groove_quality(events: list[StrokeEvent]) -&gt; dict:\n    \"\"\"\n    Produce a detailed groove analysis for debugging/inspection.\n\n    Args:\n        events: List of stroke events\n\n    Returns:\n        Dictionary with detailed groove analysis\n    \"\"\"\n    metrics = compute_groove_metrics(events)\n    timing_errors = np.array([e.timing_error_ms for e in events])\n\n    return {\n        \"groove_feel_proxy\": metrics.groove_feel,\n        \"pattern_consistency\": metrics.pattern_consistency,\n        \"swing_ratio\": metrics.swing_ratio,\n        \"accent_timing_bias_ms\": metrics.accent_timing_bias,\n        \"timing_stats\": {\n            \"mean_error_ms\": float(np.mean(timing_errors)),\n            \"std_error_ms\": float(np.std(timing_errors)),\n            \"max_error_ms\": float(np.max(np.abs(timing_errors))),\n        },\n        \"interpretation\": _interpret_groove(metrics),\n    }\n</code></pre>"},{"location":"api/midi-gen/","title":"MIDI Generation Module","text":"<p>The MIDI generation module creates synthetic drum performances from rudiment definitions and player profiles. It applies realistic timing deviations, velocity variations, and articulation-specific processing to produce varied but consistent performances.</p> <p>The generation process:</p> <ol> <li>Creates an ideal timing grid from the rudiment pattern</li> <li>Applies player-specific deviations based on profile dimensions</li> <li>Handles articulation-specific timing (flam spacing, diddle ratios)</li> <li>Converts stroke events to MIDI bytes</li> </ol>"},{"location":"api/midi-gen/#generator","title":"Generator","text":"<p>The main MIDI generation engine that transforms rudiments and profiles into performances.</p> <p>MIDI generation engine for synthetic drum performances.</p> <p>This module generates MIDI sequences from rudiment definitions and player profiles, applying realistic timing deviations, velocity variations, and articulations.</p>"},{"location":"api/midi-gen/#dataset_gen.midi_gen.generator.StrokeEvent","title":"StrokeEvent  <code>dataclass</code>","text":"<pre><code>StrokeEvent(index: int, hand: Hand, stroke_type: StrokeType, intended_time_ms: float, actual_time_ms: float, intended_velocity: int, actual_velocity: int, is_grace_note: bool = False, parent_stroke_index: int | None = None, diddle_position: int | None = None)\n</code></pre> <p>A single stroke event with timing and velocity information.</p>"},{"location":"api/midi-gen/#dataset_gen.midi_gen.generator.StrokeEvent.timing_error_ms","title":"timing_error_ms  <code>property</code>","text":"<pre><code>timing_error_ms: float\n</code></pre> <p>Deviation from intended timing.</p>"},{"location":"api/midi-gen/#dataset_gen.midi_gen.generator.StrokeEvent.velocity_error","title":"velocity_error  <code>property</code>","text":"<pre><code>velocity_error: int\n</code></pre> <p>Deviation from intended velocity.</p>"},{"location":"api/midi-gen/#dataset_gen.midi_gen.generator.GeneratedPerformance","title":"GeneratedPerformance  <code>dataclass</code>","text":"<pre><code>GeneratedPerformance(id: str = (lambda: str(uuid4()))(), rudiment_slug: str = '', profile_id: str = '', tempo_bpm: int = 120, duration_sec: float = 0.0, num_cycles: int = 1, strokes: list[StrokeEvent] = list(), midi_data: bytes | None = None)\n</code></pre> <p>Complete generated performance with all stroke events and metadata.</p>"},{"location":"api/midi-gen/#dataset_gen.midi_gen.generator.GeneratedPerformance.to_midi_file","title":"to_midi_file","text":"<pre><code>to_midi_file(path: Path | str) -&gt; None\n</code></pre> <p>Write MIDI data to file.</p> Source code in <code>dataset_gen/midi_gen/generator.py</code> <pre><code>def to_midi_file(self, path: Path | str) -&gt; None:\n    \"\"\"Write MIDI data to file.\"\"\"\n    if self.midi_data is None:\n        raise ValueError(\"No MIDI data to write\")\n    Path(path).write_bytes(self.midi_data)\n</code></pre>"},{"location":"api/midi-gen/#dataset_gen.midi_gen.generator.MIDIGenerator","title":"MIDIGenerator","text":"<pre><code>MIDIGenerator(seed: int | None = None, ticks_per_beat: int = 480)\n</code></pre> <p>Generate MIDI performances from rudiments and player profiles.</p> <p>The generator applies the player's execution dimensions to create realistic deviations in timing and velocity.</p> <p>Initialize the generator.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int | None</code> <p>Random seed for reproducibility</p> <code>None</code> <code>ticks_per_beat</code> <code>int</code> <p>MIDI resolution (PPQ)</p> <code>480</code> Source code in <code>dataset_gen/midi_gen/generator.py</code> <pre><code>def __init__(\n    self,\n    seed: int | None = None,\n    ticks_per_beat: int = 480,\n):\n    \"\"\"\n    Initialize the generator.\n\n    Args:\n        seed: Random seed for reproducibility\n        ticks_per_beat: MIDI resolution (PPQ)\n    \"\"\"\n    self.rng = np.random.default_rng(seed)\n    self.ticks_per_beat = ticks_per_beat\n</code></pre>"},{"location":"api/midi-gen/#dataset_gen.midi_gen.generator.MIDIGenerator.generate","title":"generate","text":"<pre><code>generate(rudiment: Rudiment, profile: PlayerProfile, tempo_bpm: int = 120, num_cycles: int | None = None, target_duration_sec: float | None = None, include_midi: bool = True) -&gt; GeneratedPerformance\n</code></pre> <p>Generate a performance of a rudiment.</p> <p>Parameters:</p> Name Type Description Default <code>rudiment</code> <code>Rudiment</code> <p>The rudiment to perform</p> required <code>profile</code> <code>PlayerProfile</code> <p>The player profile defining execution characteristics</p> required <code>tempo_bpm</code> <code>int</code> <p>Tempo in BPM</p> <code>120</code> <code>num_cycles</code> <code>int | None</code> <p>How many times to repeat the rudiment pattern (if target_duration_sec not set)</p> <code>None</code> <code>target_duration_sec</code> <code>float | None</code> <p>Target duration in seconds (overrides num_cycles if set)</p> <code>None</code> <code>include_midi</code> <code>bool</code> <p>Whether to generate MIDI bytes</p> <code>True</code> <p>Returns:</p> Type Description <code>GeneratedPerformance</code> <p>GeneratedPerformance with stroke events and optional MIDI data</p> Source code in <code>dataset_gen/midi_gen/generator.py</code> <pre><code>def generate(\n    self,\n    rudiment: Rudiment,\n    profile: PlayerProfile,\n    tempo_bpm: int = 120,\n    num_cycles: int | None = None,\n    target_duration_sec: float | None = None,\n    include_midi: bool = True,\n) -&gt; GeneratedPerformance:\n    \"\"\"\n    Generate a performance of a rudiment.\n\n    Args:\n        rudiment: The rudiment to perform\n        profile: The player profile defining execution characteristics\n        tempo_bpm: Tempo in BPM\n        num_cycles: How many times to repeat the rudiment pattern (if target_duration_sec not set)\n        target_duration_sec: Target duration in seconds (overrides num_cycles if set)\n        include_midi: Whether to generate MIDI bytes\n\n    Returns:\n        GeneratedPerformance with stroke events and optional MIDI data\n    \"\"\"\n    # Calculate num_cycles from target duration if specified\n    if target_duration_sec is not None:\n        # Calculate how long one cycle takes at this tempo\n        cycle_duration = rudiment.duration_at_tempo(tempo_bpm, num_cycles=1)\n        # Calculate cycles needed (at least 1, round to nearest int)\n        num_cycles = max(1, round(target_duration_sec / cycle_duration))\n    elif num_cycles is None:\n        num_cycles = 4  # Default fallback\n\n    # Generate ideal timing grid\n    ideal_events = self._generate_ideal_events(rudiment, tempo_bpm, num_cycles)\n\n    # Apply player deviations\n    actual_events = self._apply_deviations(ideal_events, profile, rudiment, tempo_bpm)\n\n    # Calculate duration\n    if actual_events:\n        duration_sec = max(e.actual_time_ms for e in actual_events) / 1000.0 + 0.5\n    else:\n        duration_sec = 0.0\n\n    performance = GeneratedPerformance(\n        rudiment_slug=rudiment.slug,\n        profile_id=profile.id,\n        tempo_bpm=tempo_bpm,\n        duration_sec=duration_sec,\n        num_cycles=num_cycles,\n        strokes=actual_events,\n    )\n\n    if include_midi:\n        performance.midi_data = self._events_to_midi(actual_events, tempo_bpm)\n\n    return performance\n</code></pre>"},{"location":"api/midi-gen/#dataset_gen.midi_gen.generator.generate_performance","title":"generate_performance","text":"<pre><code>generate_performance(rudiment: Rudiment, profile: PlayerProfile, tempo_bpm: int = 120, num_cycles: int | None = None, target_duration_sec: float | None = None, seed: int | None = None) -&gt; GeneratedPerformance\n</code></pre> <p>Convenience function to generate a single performance.</p> <p>Parameters:</p> Name Type Description Default <code>rudiment</code> <code>Rudiment</code> <p>The rudiment to perform</p> required <code>profile</code> <code>PlayerProfile</code> <p>Player profile</p> required <code>tempo_bpm</code> <code>int</code> <p>Tempo</p> <code>120</code> <code>num_cycles</code> <code>int | None</code> <p>Number of pattern repetitions (if target_duration_sec not set)</p> <code>None</code> <code>target_duration_sec</code> <code>float | None</code> <p>Target duration in seconds (overrides num_cycles)</p> <code>None</code> <code>seed</code> <code>int | None</code> <p>Random seed</p> <code>None</code> <p>Returns:</p> Type Description <code>GeneratedPerformance</code> <p>GeneratedPerformance</p> Source code in <code>dataset_gen/midi_gen/generator.py</code> <pre><code>def generate_performance(\n    rudiment: Rudiment,\n    profile: PlayerProfile,\n    tempo_bpm: int = 120,\n    num_cycles: int | None = None,\n    target_duration_sec: float | None = None,\n    seed: int | None = None,\n) -&gt; GeneratedPerformance:\n    \"\"\"\n    Convenience function to generate a single performance.\n\n    Args:\n        rudiment: The rudiment to perform\n        profile: Player profile\n        tempo_bpm: Tempo\n        num_cycles: Number of pattern repetitions (if target_duration_sec not set)\n        target_duration_sec: Target duration in seconds (overrides num_cycles)\n        seed: Random seed\n\n    Returns:\n        GeneratedPerformance\n    \"\"\"\n    generator = MIDIGenerator(seed=seed)\n    return generator.generate(\n        rudiment,\n        profile,\n        tempo_bpm,\n        num_cycles=num_cycles,\n        target_duration_sec=target_duration_sec,\n    )\n</code></pre>"},{"location":"api/midi-gen/#dataset_gen.midi_gen.generator.regenerate_midi","title":"regenerate_midi","text":"<pre><code>regenerate_midi(performance: GeneratedPerformance, ticks_per_beat: int = 480) -&gt; bytes\n</code></pre> <p>Regenerate MIDI data from a performance's stroke events.</p> <p>Use this after modifying stroke timings/velocities (e.g., via ArticulationEngine) to ensure MIDI matches the labels.</p> <p>Parameters:</p> Name Type Description Default <code>performance</code> <code>GeneratedPerformance</code> <p>Performance with (possibly modified) stroke events</p> required <code>ticks_per_beat</code> <code>int</code> <p>MIDI resolution</p> <code>480</code> <p>Returns:</p> Type Description <code>bytes</code> <p>New MIDI data as bytes</p> Source code in <code>dataset_gen/midi_gen/generator.py</code> <pre><code>def regenerate_midi(\n    performance: GeneratedPerformance,\n    ticks_per_beat: int = 480,\n) -&gt; bytes:\n    \"\"\"\n    Regenerate MIDI data from a performance's stroke events.\n\n    Use this after modifying stroke timings/velocities (e.g., via ArticulationEngine)\n    to ensure MIDI matches the labels.\n\n    Args:\n        performance: Performance with (possibly modified) stroke events\n        ticks_per_beat: MIDI resolution\n\n    Returns:\n        New MIDI data as bytes\n    \"\"\"\n    generator = MIDIGenerator(ticks_per_beat=ticks_per_beat)\n    return generator._events_to_midi(performance.strokes, performance.tempo_bpm)\n</code></pre>"},{"location":"api/midi-gen/#articulations","title":"Articulations","text":"<p>Articulation-specific processing for flams, drags, diddles, and rolls. Refines timing and velocity relationships based on rudiment type and player skill.</p> <p>Articulation-specific processing for different rudiment types.</p> <p>This module handles the nuances of flams, diddles, rolls, and buzzes, applying appropriate timing and velocity relationships.</p>"},{"location":"api/midi-gen/#dataset_gen.midi_gen.articulations.ArticulationParams","title":"ArticulationParams  <code>dataclass</code>","text":"<pre><code>ArticulationParams(flam_spacing_ms: float = 30.0, flam_spacing_variance: float = 0.1, flam_grace_velocity_ratio: float = 0.65, diddle_ratio: float = 1.0, diddle_ratio_variance: float = 0.05, diddle_velocity_decay: float = 0.95, roll_velocity_decay_per_stroke: float = 0.02, roll_acceleration_factor: float = 0.0, buzz_strokes_per_primary: int = 5, buzz_velocity_decay: float = 0.85)\n</code></pre> <p>Parameters for articulation processing.</p>"},{"location":"api/midi-gen/#dataset_gen.midi_gen.articulations.ArticulationEngine","title":"ArticulationEngine","text":"<pre><code>ArticulationEngine(seed: int | None = None)\n</code></pre> <p>Process stroke events with rudiment-specific articulation rules.</p> <p>This engine refines the base timing and velocity deviations with articulation-specific relationships.</p> Source code in <code>dataset_gen/midi_gen/articulations.py</code> <pre><code>def __init__(self, seed: int | None = None):\n    self.rng = np.random.default_rng(seed)\n</code></pre>"},{"location":"api/midi-gen/#dataset_gen.midi_gen.articulations.ArticulationEngine.process","title":"process","text":"<pre><code>process(events: list[StrokeEvent], rudiment: Rudiment, profile: PlayerProfile) -&gt; list[StrokeEvent]\n</code></pre> <p>Apply articulation-specific processing to stroke events.</p> <p>Parameters:</p> Name Type Description Default <code>events</code> <code>list[StrokeEvent]</code> <p>List of stroke events to process</p> required <code>rudiment</code> <code>Rudiment</code> <p>The rudiment being performed</p> required <code>profile</code> <code>PlayerProfile</code> <p>Player profile with articulation parameters</p> required <p>Returns:</p> Type Description <code>list[StrokeEvent]</code> <p>Processed stroke events with refined timing/velocity</p> Source code in <code>dataset_gen/midi_gen/articulations.py</code> <pre><code>def process(\n    self,\n    events: list[StrokeEvent],\n    rudiment: Rudiment,\n    profile: PlayerProfile,\n) -&gt; list[StrokeEvent]:\n    \"\"\"\n    Apply articulation-specific processing to stroke events.\n\n    Args:\n        events: List of stroke events to process\n        rudiment: The rudiment being performed\n        profile: Player profile with articulation parameters\n\n    Returns:\n        Processed stroke events with refined timing/velocity\n    \"\"\"\n    dims = profile.dimensions.rudiment_specific\n\n    # Process based on rudiment category\n    if rudiment.category == RudimentCategory.FLAM:\n        events = self._process_flams(events, dims)\n    elif rudiment.category == RudimentCategory.DRAG:\n        events = self._process_drags(events, dims)\n\n    # Process diddles in all relevant rudiments\n    events = self._process_diddles(events, dims)\n\n    # Process rolls\n    if rudiment.category == RudimentCategory.ROLL:\n        events = self._process_rolls(events, dims, rudiment)\n\n    return events\n</code></pre>"},{"location":"api/midi-gen/#dataset_gen.midi_gen.articulations.apply_flam_spacing","title":"apply_flam_spacing","text":"<pre><code>apply_flam_spacing(events: list[StrokeEvent], spacing_ms: float, variance: float = 0.1, rng: Generator | None = None) -&gt; list[StrokeEvent]\n</code></pre> <p>Convenience function to apply flam spacing to events.</p> <p>Parameters:</p> Name Type Description Default <code>events</code> <code>list[StrokeEvent]</code> <p>List of stroke events</p> required <code>spacing_ms</code> <code>float</code> <p>Target spacing between grace and primary</p> required <code>variance</code> <code>float</code> <p>Variance as proportion of spacing</p> <code>0.1</code> <code>rng</code> <code>Generator | None</code> <p>Random number generator</p> <code>None</code> <p>Returns:</p> Type Description <code>list[StrokeEvent]</code> <p>Modified events</p> Source code in <code>dataset_gen/midi_gen/articulations.py</code> <pre><code>def apply_flam_spacing(\n    events: list[StrokeEvent],\n    spacing_ms: float,\n    variance: float = 0.1,\n    rng: np.random.Generator | None = None,\n) -&gt; list[StrokeEvent]:\n    \"\"\"\n    Convenience function to apply flam spacing to events.\n\n    Args:\n        events: List of stroke events\n        spacing_ms: Target spacing between grace and primary\n        variance: Variance as proportion of spacing\n        rng: Random number generator\n\n    Returns:\n        Modified events\n    \"\"\"\n    if rng is None:\n        rng = np.random.default_rng()\n\n    for event in events:\n        if event.is_grace_note and event.parent_stroke_index is not None:\n            primary = next((e for e in events if e.index == event.parent_stroke_index), None)\n            if primary:\n                actual_spacing = max(5, rng.normal(spacing_ms, spacing_ms * variance))\n                event.actual_time_ms = primary.actual_time_ms - actual_spacing\n\n    return events\n</code></pre>"},{"location":"api/midi-gen/#dataset_gen.midi_gen.articulations.apply_diddle_timing","title":"apply_diddle_timing","text":"<pre><code>apply_diddle_timing(events: list[StrokeEvent], evenness: float = 1.0, variance: float = 0.05, rng: Generator | None = None) -&gt; list[StrokeEvent]\n</code></pre> <p>Convenience function to apply diddle timing adjustments.</p> <p>Parameters:</p> Name Type Description Default <code>events</code> <code>list[StrokeEvent]</code> <p>List of stroke events</p> required <code>evenness</code> <code>float</code> <p>Target ratio between first and second stroke (1.0 = even)</p> <code>1.0</code> <code>variance</code> <code>float</code> <p>Variance in evenness</p> <code>0.05</code> <code>rng</code> <code>Generator | None</code> <p>Random number generator</p> <code>None</code> <p>Returns:</p> Type Description <code>list[StrokeEvent]</code> <p>Modified events</p> Source code in <code>dataset_gen/midi_gen/articulations.py</code> <pre><code>def apply_diddle_timing(\n    events: list[StrokeEvent],\n    evenness: float = 1.0,\n    variance: float = 0.05,\n    rng: np.random.Generator | None = None,\n) -&gt; list[StrokeEvent]:\n    \"\"\"\n    Convenience function to apply diddle timing adjustments.\n\n    Args:\n        events: List of stroke events\n        evenness: Target ratio between first and second stroke (1.0 = even)\n        variance: Variance in evenness\n        rng: Random number generator\n\n    Returns:\n        Modified events\n    \"\"\"\n    if rng is None:\n        rng = np.random.default_rng()\n\n    i = 0\n    while i &lt; len(events) - 1:\n        if (\n            events[i].stroke_type == StrokeType.DIDDLE\n            and events[i].diddle_position == 1\n            and events[i + 1].stroke_type == StrokeType.DIDDLE\n            and events[i + 1].diddle_position == 2\n        ):\n            first, second = events[i], events[i + 1]\n            gap = second.intended_time_ms - first.intended_time_ms\n            actual_ratio = max(0.5, min(1.5, rng.normal(evenness, variance)))\n            second.actual_time_ms = first.actual_time_ms + gap * actual_ratio\n            i += 2\n        else:\n            i += 1\n\n    return events\n</code></pre>"},{"location":"api/midi-gen/#dataset_gen.midi_gen.articulations.apply_roll_velocity_decay","title":"apply_roll_velocity_decay","text":"<pre><code>apply_roll_velocity_decay(events: list[StrokeEvent], decay_rate: float = 0.02) -&gt; list[StrokeEvent]\n</code></pre> <p>Apply velocity decay across a roll.</p> <p>Parameters:</p> Name Type Description Default <code>events</code> <code>list[StrokeEvent]</code> <p>List of stroke events</p> required <code>decay_rate</code> <code>float</code> <p>Decay per stroke (0.02 = 2% per stroke)</p> <code>0.02</code> <p>Returns:</p> Type Description <code>list[StrokeEvent]</code> <p>Modified events</p> Source code in <code>dataset_gen/midi_gen/articulations.py</code> <pre><code>def apply_roll_velocity_decay(\n    events: list[StrokeEvent],\n    decay_rate: float = 0.02,\n) -&gt; list[StrokeEvent]:\n    \"\"\"\n    Apply velocity decay across a roll.\n\n    Args:\n        events: List of stroke events\n        decay_rate: Decay per stroke (0.02 = 2% per stroke)\n\n    Returns:\n        Modified events\n    \"\"\"\n    for i, event in enumerate(events):\n        decay = 1.0 - (i * decay_rate)\n        decay = max(0.5, decay)\n        event.actual_velocity = int(np.clip(event.actual_velocity * decay, 1, 127))\n\n    return events\n</code></pre>"},{"location":"api/pipeline/","title":"Pipeline Module","text":"<p>The pipeline module orchestrates the complete dataset generation process, from player profiles to stored samples. It coordinates all components of the generation pipeline:</p> <ol> <li>Generate player profiles with correlated skill dimensions</li> <li>Generate MIDI performances with realistic timing/velocity</li> <li>Render audio via FluidSynth (multiple soundfonts)</li> <li>Apply audio augmentation (rooms, mics, compression, noise)</li> <li>Compute hierarchical labels (stroke, measure, exercise)</li> <li>Save to disk with profile-based train/val/test splits</li> </ol>"},{"location":"api/pipeline/#dataset-generator","title":"Dataset Generator","text":"<p>Main orchestration for dataset generation with configurable scale and augmentation options.</p> <p>SOUSA Dataset Generation Pipeline.</p> <p>Main orchestration for generating the Synthetic Open Unified Snare Assessment dataset: 1. Generate player profiles with correlated skill dimensions 2. Generate MIDI performances with realistic timing/velocity 3. Render audio via FluidSynth (multiple soundfonts) 4. Apply audio augmentation (rooms, mics, compression, noise) 5. Compute hierarchical labels (stroke, measure, exercise) 6. Save to disk with profile-based train/val/test splits</p>"},{"location":"api/pipeline/#dataset_gen.pipeline.generate.GenerationConfig","title":"GenerationConfig  <code>dataclass</code>","text":"<pre><code>GenerationConfig(output_dir: Path = (lambda: Path('output/dataset'))(), num_profiles: int = 100, samples_per_profile: int = 40, num_cycles_per_sample: int = 4, target_duration_sec: float | None = None, skill_distribution: dict[SkillTier, float] = (lambda: {BEGINNER: 0.25, INTERMEDIATE: 0.35, ADVANCED: 0.25, PROFESSIONAL: 0.15})(), tempo_range: tuple[int, int] = (60, 180), tempos_per_rudiment: int = 3, generate_audio: bool = True, soundfont_path: Path | None = None, soundfont_paths: list[Path] | None = None, sample_rate: int = 44100, audio_format: str = 'flac', apply_augmentation: bool = True, augmentation_presets: list[AugmentationPreset] | None = None, augmentations_per_sample: int = 1, train_ratio: float = 0.7, val_ratio: float = 0.15, test_ratio: float = 0.15, seed: int = 42, batch_size: int = 100, verbose: bool = True)\n</code></pre> <p>Configuration for dataset generation.</p>"},{"location":"api/pipeline/#dataset_gen.pipeline.generate.GenerationProgress","title":"GenerationProgress  <code>dataclass</code>","text":"<pre><code>GenerationProgress(total_samples: int = 0, completed_samples: int = 0, failed_samples: int = 0, current_profile: str = '', current_rudiment: str = '')\n</code></pre> <p>Track generation progress.</p>"},{"location":"api/pipeline/#dataset_gen.pipeline.generate.DatasetGenerator","title":"DatasetGenerator","text":"<pre><code>DatasetGenerator(config: GenerationConfig)\n</code></pre> <p>Generate complete synthetic drum rudiment dataset.</p> <p>Orchestrates the full pipeline from profiles to stored samples.</p> <p>Initialize generator.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>GenerationConfig</code> <p>Generation configuration</p> required Source code in <code>dataset_gen/pipeline/generate.py</code> <pre><code>def __init__(self, config: GenerationConfig):\n    \"\"\"\n    Initialize generator.\n\n    Args:\n        config: Generation configuration\n    \"\"\"\n    self.config = config\n    self.rng = np.random.default_rng(config.seed)\n\n    # Initialize components\n    self._midi_gen = MIDIGenerator(seed=config.seed)\n    self._articulation_engine = ArticulationEngine(seed=config.seed + 1)\n\n    # Audio synthesis (if available and configured)\n    self._synth: AudioSynthesizer | None = None\n    self._soundfont_names: list[str] = []\n    if config.generate_audio and config.soundfont_paths:\n        if SYNTH_AVAILABLE:\n            synth_config = SynthConfig(sample_rate=config.sample_rate)\n            self._synth = AudioSynthesizer(soundfont_path=None, config=synth_config)\n            # Load all soundfonts\n            for sf_path in config.soundfont_paths:\n                try:\n                    name = self._synth.load_soundfont(sf_path)\n                    self._soundfont_names.append(name)\n                    logger.info(f\"Loaded soundfont: {name}\")\n                except Exception as e:\n                    logger.warning(f\"Failed to load soundfont {sf_path}: {e}\")\n            if not self._soundfont_names:\n                logger.warning(\"No soundfonts loaded successfully\")\n                self._synth.close()\n                self._synth = None\n        else:\n            logger.warning(\"Audio synthesis not available (FluidSynth not installed)\")\n\n    # Augmentation pipeline\n    self._augmenter: AugmentationPipeline | None = None\n    if config.apply_augmentation:\n        self._augmenter = AugmentationPipeline(sample_rate=config.sample_rate)\n\n    # Storage\n    storage_config = StorageConfig(\n        output_dir=config.output_dir,\n        audio_format=config.audio_format,\n        sample_rate=config.sample_rate,\n    )\n    self._writer = DatasetWriter(storage_config)\n\n    # Progress tracking\n    self.progress = GenerationProgress()\n\n    # Profile numbering for readable filenames\n    self._profile_numbers: dict[str, int] = {}\n    self._profile_counter = 0\n\n    # Callbacks\n    self._progress_callback: Callable[[GenerationProgress], None] | None = None\n</code></pre>"},{"location":"api/pipeline/#dataset_gen.pipeline.generate.DatasetGenerator.set_progress_callback","title":"set_progress_callback","text":"<pre><code>set_progress_callback(callback: Callable[[GenerationProgress], None]) -&gt; None\n</code></pre> <p>Set callback for progress updates.</p> Source code in <code>dataset_gen/pipeline/generate.py</code> <pre><code>def set_progress_callback(\n    self,\n    callback: Callable[[GenerationProgress], None],\n) -&gt; None:\n    \"\"\"Set callback for progress updates.\"\"\"\n    self._progress_callback = callback\n</code></pre>"},{"location":"api/pipeline/#dataset_gen.pipeline.generate.DatasetGenerator.generate","title":"generate","text":"<pre><code>generate(rudiments: list[Rudiment] | None = None, rudiment_dir: Path | str | None = None) -&gt; SplitAssignment\n</code></pre> <p>Generate the complete dataset.</p> <p>Parameters:</p> Name Type Description Default <code>rudiments</code> <code>list[Rudiment] | None</code> <p>List of rudiments to use (optional)</p> <code>None</code> <code>rudiment_dir</code> <code>Path | str | None</code> <p>Directory containing rudiment YAML files (optional)</p> <code>None</code> <p>Returns:</p> Type Description <code>SplitAssignment</code> <p>SplitAssignment for the generated profiles</p> Source code in <code>dataset_gen/pipeline/generate.py</code> <pre><code>def generate(\n    self,\n    rudiments: list[Rudiment] | None = None,\n    rudiment_dir: Path | str | None = None,\n) -&gt; SplitAssignment:\n    \"\"\"\n    Generate the complete dataset.\n\n    Args:\n        rudiments: List of rudiments to use (optional)\n        rudiment_dir: Directory containing rudiment YAML files (optional)\n\n    Returns:\n        SplitAssignment for the generated profiles\n    \"\"\"\n    # Load rudiments\n    if rudiments is None:\n        if rudiment_dir:\n            rudiments_dict = load_all_rudiments(Path(rudiment_dir))\n        else:\n            # Try default location\n            default_dir = Path(__file__).parent.parent / \"rudiments\" / \"definitions\"\n            if default_dir.exists():\n                rudiments_dict = load_all_rudiments(default_dir)\n            else:\n                raise ValueError(\"No rudiments provided and default directory not found\")\n        # Convert dict to list (load_all_rudiments returns dict[slug, Rudiment])\n        rudiments = list(rudiments_dict.values())\n\n    logger.info(f\"Loaded {len(rudiments)} rudiments\")\n\n    # Generate profiles\n    profiles = self._generate_profiles()\n    logger.info(f\"Generated {len(profiles)} player profiles\")\n\n    # Generate splits\n    split_config = SplitConfig(\n        train_ratio=self.config.train_ratio,\n        val_ratio=self.config.val_ratio,\n        test_ratio=self.config.test_ratio,\n        seed=self.config.seed,\n    )\n    split_generator = SplitGenerator(split_config)\n    splits = split_generator.generate_splits(profiles)\n\n    # Save splits\n    split_generator.save_splits(\n        splits,\n        self.config.output_dir / \"splits.json\",\n    )\n\n    # Calculate total samples\n    self.progress.total_samples = (\n        len(profiles)\n        * len(rudiments)\n        * self.config.tempos_per_rudiment\n        * self.config.augmentations_per_sample\n    )\n\n    logger.info(f\"Generating {self.progress.total_samples} samples\")\n\n    # Generate samples\n    for profile in profiles:\n        self._generate_profile_samples(profile, rudiments, splits)\n\n    # Flush remaining samples\n    self._writer.flush()\n\n    logger.info(f\"Generation complete: {self.progress.completed_samples} samples\")\n\n    return splits\n</code></pre>"},{"location":"api/pipeline/#dataset_gen.pipeline.generate.DatasetGenerator.generate_single","title":"generate_single","text":"<pre><code>generate_single(rudiment: Rudiment, profile: PlayerProfile | None = None, tempo: int = 120, preset: AugmentationPreset | None = None) -&gt; tuple[Sample, bytes | None, ndarray | None]\n</code></pre> <p>Generate a single sample for testing/inspection.</p> <p>Parameters:</p> Name Type Description Default <code>rudiment</code> <code>Rudiment</code> <p>Rudiment to perform</p> required <code>profile</code> <code>PlayerProfile | None</code> <p>Player profile (generated if None)</p> <code>None</code> <code>tempo</code> <code>int</code> <p>Tempo in BPM</p> <code>120</code> <code>preset</code> <code>AugmentationPreset | None</code> <p>Augmentation preset</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[Sample, bytes | None, ndarray | None]</code> <p>Tuple of (sample, midi_data, audio_data)</p> Source code in <code>dataset_gen/pipeline/generate.py</code> <pre><code>def generate_single(\n    self,\n    rudiment: Rudiment,\n    profile: PlayerProfile | None = None,\n    tempo: int = 120,\n    preset: AugmentationPreset | None = None,\n) -&gt; tuple[Sample, bytes | None, np.ndarray | None]:\n    \"\"\"\n    Generate a single sample for testing/inspection.\n\n    Args:\n        rudiment: Rudiment to perform\n        profile: Player profile (generated if None)\n        tempo: Tempo in BPM\n        preset: Augmentation preset\n\n    Returns:\n        Tuple of (sample, midi_data, audio_data)\n    \"\"\"\n    if profile is None:\n        profile = generate_profile(SkillTier.INTERMEDIATE, rng=self.rng)\n\n    # Generate MIDI\n    performance = self._midi_gen.generate(\n        rudiment=rudiment,\n        profile=profile,\n        tempo_bpm=tempo,\n        num_cycles=(\n            self.config.num_cycles_per_sample\n            if self.config.target_duration_sec is None\n            else None\n        ),\n        target_duration_sec=self.config.target_duration_sec,\n        include_midi=True,\n    )\n\n    # Apply articulations\n    performance.strokes = self._articulation_engine.process(\n        performance.strokes,\n        rudiment,\n        profile,\n    )\n\n    # Regenerate MIDI to match modified strokes\n    performance.midi_data = regenerate_midi(performance)\n\n    # Compute labels\n    sample = compute_sample_labels(performance, rudiment, profile)\n\n    # Render audio (use first soundfont if available)\n    audio_data = None\n    soundfont_name = self._soundfont_names[0] if self._soundfont_names else None\n    if self._synth is not None and performance.midi_data:\n        try:\n            audio_data = self._synth.render(\n                midi_data=performance.midi_data,\n                duration_hint_sec=performance.duration_sec,\n                soundfont_name=soundfont_name,\n            )\n\n            # Apply augmentation\n            if preset and self._augmenter:\n                aug_config = AugmentationConfig.from_preset(preset)\n                audio_data = self._augmenter.process(audio_data, aug_config)\n\n        except Exception as e:\n            logger.warning(f\"Audio generation failed: {e}\")\n\n    return sample, performance.midi_data, audio_data\n</code></pre>"},{"location":"api/pipeline/#dataset_gen.pipeline.generate.DatasetGenerator.close","title":"close","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Clean up resources.</p> Source code in <code>dataset_gen/pipeline/generate.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Clean up resources.\"\"\"\n    if self._synth:\n        self._synth.close()\n</code></pre>"},{"location":"api/pipeline/#dataset_gen.pipeline.generate.generate_dataset","title":"generate_dataset","text":"<pre><code>generate_dataset(output_dir: Path | str, num_profiles: int = 100, soundfont_path: Path | str | None = None, rudiment_dir: Path | str | None = None, seed: int = 42, verbose: bool = True) -&gt; SplitAssignment\n</code></pre> <p>Convenience function to generate a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> <code>Path | str</code> <p>Output directory for dataset</p> required <code>num_profiles</code> <code>int</code> <p>Number of player profiles to generate</p> <code>100</code> <code>soundfont_path</code> <code>Path | str | None</code> <p>Path to soundfont for audio synthesis</p> <code>None</code> <code>rudiment_dir</code> <code>Path | str | None</code> <p>Directory containing rudiment definitions</p> <code>None</code> <code>seed</code> <code>int</code> <p>Random seed</p> <code>42</code> <code>verbose</code> <code>bool</code> <p>Whether to log progress</p> <code>True</code> <p>Returns:</p> Type Description <code>SplitAssignment</code> <p>SplitAssignment for the generated profiles</p> Source code in <code>dataset_gen/pipeline/generate.py</code> <pre><code>def generate_dataset(\n    output_dir: Path | str,\n    num_profiles: int = 100,\n    soundfont_path: Path | str | None = None,\n    rudiment_dir: Path | str | None = None,\n    seed: int = 42,\n    verbose: bool = True,\n) -&gt; SplitAssignment:\n    \"\"\"\n    Convenience function to generate a dataset.\n\n    Args:\n        output_dir: Output directory for dataset\n        num_profiles: Number of player profiles to generate\n        soundfont_path: Path to soundfont for audio synthesis\n        rudiment_dir: Directory containing rudiment definitions\n        seed: Random seed\n        verbose: Whether to log progress\n\n    Returns:\n        SplitAssignment for the generated profiles\n    \"\"\"\n    config = GenerationConfig(\n        output_dir=Path(output_dir),\n        num_profiles=num_profiles,\n        soundfont_path=Path(soundfont_path) if soundfont_path else None,\n        seed=seed,\n        verbose=verbose,\n    )\n\n    if verbose:\n        logging.basicConfig(level=logging.INFO)\n\n    generator = DatasetGenerator(config)\n\n    try:\n        return generator.generate(rudiment_dir=rudiment_dir)\n    finally:\n        generator.close()\n</code></pre>"},{"location":"api/pipeline/#storage","title":"Storage","text":"<p>Utilities for writing samples to disk in efficient formats (Parquet for labels, FLAC for audio, MIDI files).</p> <p>Storage utilities for dataset persistence.</p> <p>This module handles writing samples to disk in efficient formats: - Parquet for labels (hierarchical, compressed) - FLAC for audio - Standard MIDI files</p>"},{"location":"api/pipeline/#dataset_gen.pipeline.storage.StorageConfig","title":"StorageConfig  <code>dataclass</code>","text":"<pre><code>StorageConfig(output_dir: Path, audio_format: Literal['flac', 'wav'] = 'flac', audio_subtype: str = 'PCM_24', sample_rate: int = 44100, midi_subdir: str = 'midi', audio_subdir: str = 'audio', labels_subdir: str = 'labels', compression: str = 'snappy', row_group_size: int = 1000, create_index: bool = True)\n</code></pre> <p>Configuration for dataset storage.</p>"},{"location":"api/pipeline/#dataset_gen.pipeline.storage.DatasetWriter","title":"DatasetWriter","text":"<pre><code>DatasetWriter(config: StorageConfig)\n</code></pre> <p>Write dataset samples to disk in organized format.</p> <p>Directory structure: output_dir/ \u251c\u2500\u2500 midi/ \u2502   \u2514\u2500\u2500 {sample_id}.mid \u251c\u2500\u2500 audio/ \u2502   \u2514\u2500\u2500 {sample_id}.flac \u251c\u2500\u2500 labels/ \u2502   \u251c\u2500\u2500 strokes.parquet \u2502   \u251c\u2500\u2500 measures.parquet \u2502   \u251c\u2500\u2500 exercises.parquet \u2502   \u2514\u2500\u2500 samples.parquet \u2514\u2500\u2500 index.json</p> <p>Initialize dataset writer.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>StorageConfig</code> <p>Storage configuration</p> required Source code in <code>dataset_gen/pipeline/storage.py</code> <pre><code>def __init__(self, config: StorageConfig):\n    \"\"\"\n    Initialize dataset writer.\n\n    Args:\n        config: Storage configuration\n    \"\"\"\n    self.config = config\n    self._setup_directories()\n\n    # Accumulators for batch writing\n    self._stroke_records: list[dict] = []\n    self._measure_records: list[dict] = []\n    self._exercise_records: list[dict] = []\n    self._sample_records: list[dict] = []\n\n    # Index tracking\n    self._sample_ids: list[str] = []\n</code></pre>"},{"location":"api/pipeline/#dataset_gen.pipeline.storage.DatasetWriter.write_sample","title":"write_sample","text":"<pre><code>write_sample(sample: Sample, midi_data: bytes | None = None, audio_data: ndarray | None = None) -&gt; dict[str, Path]\n</code></pre> <p>Write a single sample to disk.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>Sample</code> <p>Sample with labels</p> required <code>midi_data</code> <code>bytes | None</code> <p>Raw MIDI bytes (optional)</p> <code>None</code> <code>audio_data</code> <code>ndarray | None</code> <p>Audio samples as numpy array (optional)</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Path]</code> <p>Dictionary of written file paths</p> Source code in <code>dataset_gen/pipeline/storage.py</code> <pre><code>def write_sample(\n    self,\n    sample: Sample,\n    midi_data: bytes | None = None,\n    audio_data: np.ndarray | None = None,\n) -&gt; dict[str, Path]:\n    \"\"\"\n    Write a single sample to disk.\n\n    Args:\n        sample: Sample with labels\n        midi_data: Raw MIDI bytes (optional)\n        audio_data: Audio samples as numpy array (optional)\n\n    Returns:\n        Dictionary of written file paths\n    \"\"\"\n    paths = {}\n\n    # Write MIDI\n    if midi_data is not None:\n        midi_path = self._write_midi(sample.sample_id, midi_data)\n        paths[\"midi\"] = midi_path\n        sample.midi_path = str(midi_path.relative_to(self.config.output_dir))\n\n    # Write audio\n    if audio_data is not None:\n        audio_path = self._write_audio(sample.sample_id, audio_data)\n        paths[\"audio\"] = audio_path\n        sample.audio_path = str(audio_path.relative_to(self.config.output_dir))\n\n    # Accumulate label records\n    self._accumulate_labels(sample)\n    self._sample_ids.append(sample.sample_id)\n\n    return paths\n</code></pre>"},{"location":"api/pipeline/#dataset_gen.pipeline.storage.DatasetWriter.flush","title":"flush","text":"<pre><code>flush() -&gt; dict[str, Path]\n</code></pre> <p>Write accumulated labels to Parquet files.</p> <p>Returns:</p> Type Description <code>dict[str, Path]</code> <p>Dictionary of written Parquet file paths</p> Source code in <code>dataset_gen/pipeline/storage.py</code> <pre><code>def flush(self) -&gt; dict[str, Path]:\n    \"\"\"\n    Write accumulated labels to Parquet files.\n\n    Returns:\n        Dictionary of written Parquet file paths\n    \"\"\"\n    paths = {}\n    labels_dir = self.config.output_dir / self.config.labels_subdir\n\n    # Write strokes\n    if self._stroke_records:\n        strokes_path = labels_dir / \"strokes.parquet\"\n        self._write_parquet(self._stroke_records, strokes_path)\n        paths[\"strokes\"] = strokes_path\n        self._stroke_records = []\n\n    # Write measures\n    if self._measure_records:\n        measures_path = labels_dir / \"measures.parquet\"\n        self._write_parquet(self._measure_records, measures_path)\n        paths[\"measures\"] = measures_path\n        self._measure_records = []\n\n    # Write exercises\n    if self._exercise_records:\n        exercises_path = labels_dir / \"exercises.parquet\"\n        self._write_parquet(self._exercise_records, exercises_path)\n        paths[\"exercises\"] = exercises_path\n        self._exercise_records = []\n\n    # Write samples\n    if self._sample_records:\n        samples_path = labels_dir / \"samples.parquet\"\n        self._write_parquet(self._sample_records, samples_path)\n        paths[\"samples\"] = samples_path\n        self._sample_records = []\n\n    # Write index\n    if self.config.create_index:\n        index_path = self._write_index()\n        paths[\"index\"] = index_path\n\n    return paths\n</code></pre>"},{"location":"api/pipeline/#dataset_gen.pipeline.storage.DatasetWriter.get_stats","title":"get_stats","text":"<pre><code>get_stats() -&gt; dict\n</code></pre> <p>Get statistics about accumulated data.</p> Source code in <code>dataset_gen/pipeline/storage.py</code> <pre><code>def get_stats(self) -&gt; dict:\n    \"\"\"Get statistics about accumulated data.\"\"\"\n    return {\n        \"samples_pending\": len(self._sample_records),\n        \"strokes_pending\": len(self._stroke_records),\n        \"measures_pending\": len(self._measure_records),\n        \"exercises_pending\": len(self._exercise_records),\n        \"total_samples_written\": len(self._sample_ids),\n    }\n</code></pre>"},{"location":"api/pipeline/#dataset_gen.pipeline.storage.ParquetReader","title":"ParquetReader","text":"<pre><code>ParquetReader(dataset_dir: Path | str)\n</code></pre> <p>Read dataset from Parquet files.</p> <p>Initialize reader.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_dir</code> <code>Path | str</code> <p>Path to dataset directory</p> required Source code in <code>dataset_gen/pipeline/storage.py</code> <pre><code>def __init__(self, dataset_dir: Path | str):\n    \"\"\"\n    Initialize reader.\n\n    Args:\n        dataset_dir: Path to dataset directory\n    \"\"\"\n    self.dataset_dir = Path(dataset_dir)\n    self.labels_dir = self.dataset_dir / \"labels\"\n</code></pre>"},{"location":"api/pipeline/#dataset_gen.pipeline.storage.ParquetReader.load_samples","title":"load_samples","text":"<pre><code>load_samples() -&gt; DataFrame\n</code></pre> <p>Load sample metadata.</p> Source code in <code>dataset_gen/pipeline/storage.py</code> <pre><code>def load_samples(self) -&gt; pd.DataFrame:\n    \"\"\"Load sample metadata.\"\"\"\n    return pd.read_parquet(self.labels_dir / \"samples.parquet\")\n</code></pre>"},{"location":"api/pipeline/#dataset_gen.pipeline.storage.ParquetReader.load_strokes","title":"load_strokes","text":"<pre><code>load_strokes(sample_ids: list[str] | None = None) -&gt; DataFrame\n</code></pre> <p>Load stroke-level labels, optionally filtered by sample IDs.</p> Source code in <code>dataset_gen/pipeline/storage.py</code> <pre><code>def load_strokes(self, sample_ids: list[str] | None = None) -&gt; pd.DataFrame:\n    \"\"\"Load stroke-level labels, optionally filtered by sample IDs.\"\"\"\n    df = pd.read_parquet(self.labels_dir / \"strokes.parquet\")\n    if sample_ids is not None:\n        df = df[df[\"sample_id\"].isin(sample_ids)]\n    return df\n</code></pre>"},{"location":"api/pipeline/#dataset_gen.pipeline.storage.ParquetReader.load_measures","title":"load_measures","text":"<pre><code>load_measures(sample_ids: list[str] | None = None) -&gt; DataFrame\n</code></pre> <p>Load measure-level labels, optionally filtered by sample IDs.</p> Source code in <code>dataset_gen/pipeline/storage.py</code> <pre><code>def load_measures(self, sample_ids: list[str] | None = None) -&gt; pd.DataFrame:\n    \"\"\"Load measure-level labels, optionally filtered by sample IDs.\"\"\"\n    df = pd.read_parquet(self.labels_dir / \"measures.parquet\")\n    if sample_ids is not None:\n        df = df[df[\"sample_id\"].isin(sample_ids)]\n    return df\n</code></pre>"},{"location":"api/pipeline/#dataset_gen.pipeline.storage.ParquetReader.load_exercises","title":"load_exercises","text":"<pre><code>load_exercises(sample_ids: list[str] | None = None) -&gt; DataFrame\n</code></pre> <p>Load exercise-level scores, optionally filtered by sample IDs.</p> Source code in <code>dataset_gen/pipeline/storage.py</code> <pre><code>def load_exercises(self, sample_ids: list[str] | None = None) -&gt; pd.DataFrame:\n    \"\"\"Load exercise-level scores, optionally filtered by sample IDs.\"\"\"\n    df = pd.read_parquet(self.labels_dir / \"exercises.parquet\")\n    if sample_ids is not None:\n        df = df[df[\"sample_id\"].isin(sample_ids)]\n    return df\n</code></pre>"},{"location":"api/pipeline/#dataset_gen.pipeline.storage.ParquetReader.load_index","title":"load_index","text":"<pre><code>load_index() -&gt; dict\n</code></pre> <p>Load dataset index.</p> Source code in <code>dataset_gen/pipeline/storage.py</code> <pre><code>def load_index(self) -&gt; dict:\n    \"\"\"Load dataset index.\"\"\"\n    with open(self.dataset_dir / \"index.json\") as f:\n        return json.load(f)\n</code></pre>"},{"location":"api/pipeline/#dataset_gen.pipeline.storage.ParquetReader.get_sample","title":"get_sample","text":"<pre><code>get_sample(sample_id: str) -&gt; dict\n</code></pre> <p>Load all data for a single sample.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with sample metadata, strokes, measures, and scores</p> Source code in <code>dataset_gen/pipeline/storage.py</code> <pre><code>def get_sample(self, sample_id: str) -&gt; dict:\n    \"\"\"\n    Load all data for a single sample.\n\n    Returns:\n        Dictionary with sample metadata, strokes, measures, and scores\n    \"\"\"\n    samples = self.load_samples()\n    sample_row = samples[samples[\"sample_id\"] == sample_id].iloc[0]\n\n    return {\n        \"metadata\": sample_row.to_dict(),\n        \"strokes\": self.load_strokes([sample_id]).to_dict(orient=\"records\"),\n        \"measures\": self.load_measures([sample_id]).to_dict(orient=\"records\"),\n        \"scores\": self.load_exercises([sample_id]).iloc[0].to_dict(),\n    }\n</code></pre>"},{"location":"api/pipeline/#dataset_gen.pipeline.storage.write_sample","title":"write_sample","text":"<pre><code>write_sample(sample: Sample, output_dir: Path | str, midi_data: bytes | None = None, audio_data: ndarray | None = None, audio_format: str = 'flac') -&gt; dict[str, Path]\n</code></pre> <p>Convenience function to write a single sample.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>Sample</code> <p>Sample with labels</p> required <code>output_dir</code> <code>Path | str</code> <p>Output directory</p> required <code>midi_data</code> <code>bytes | None</code> <p>Raw MIDI bytes</p> <code>None</code> <code>audio_data</code> <code>ndarray | None</code> <p>Audio samples</p> <code>None</code> <code>audio_format</code> <code>str</code> <p>Output audio format</p> <code>'flac'</code> <p>Returns:</p> Type Description <code>dict[str, Path]</code> <p>Dictionary of written file paths</p> Source code in <code>dataset_gen/pipeline/storage.py</code> <pre><code>def write_sample(\n    sample: Sample,\n    output_dir: Path | str,\n    midi_data: bytes | None = None,\n    audio_data: np.ndarray | None = None,\n    audio_format: str = \"flac\",\n) -&gt; dict[str, Path]:\n    \"\"\"\n    Convenience function to write a single sample.\n\n    Args:\n        sample: Sample with labels\n        output_dir: Output directory\n        midi_data: Raw MIDI bytes\n        audio_data: Audio samples\n        audio_format: Output audio format\n\n    Returns:\n        Dictionary of written file paths\n    \"\"\"\n    config = StorageConfig(\n        output_dir=Path(output_dir),\n        audio_format=audio_format,\n    )\n    writer = DatasetWriter(config)\n    paths = writer.write_sample(sample, midi_data, audio_data)\n    writer.flush()\n    return paths\n</code></pre>"},{"location":"api/pipeline/#dataset_gen.pipeline.storage.write_batch","title":"write_batch","text":"<pre><code>write_batch(samples: list[Sample], output_dir: Path | str, midi_data_list: list[bytes | None] | None = None, audio_data_list: list[ndarray | None] | None = None, audio_format: str = 'flac') -&gt; dict[str, Path]\n</code></pre> <p>Convenience function to write a batch of samples.</p> <p>Parameters:</p> Name Type Description Default <code>samples</code> <code>list[Sample]</code> <p>List of samples with labels</p> required <code>output_dir</code> <code>Path | str</code> <p>Output directory</p> required <code>midi_data_list</code> <code>list[bytes | None] | None</code> <p>List of MIDI bytes (parallel to samples)</p> <code>None</code> <code>audio_data_list</code> <code>list[ndarray | None] | None</code> <p>List of audio arrays (parallel to samples)</p> <code>None</code> <code>audio_format</code> <code>str</code> <p>Output audio format</p> <code>'flac'</code> <p>Returns:</p> Type Description <code>dict[str, Path]</code> <p>Dictionary of written Parquet file paths</p> Source code in <code>dataset_gen/pipeline/storage.py</code> <pre><code>def write_batch(\n    samples: list[Sample],\n    output_dir: Path | str,\n    midi_data_list: list[bytes | None] | None = None,\n    audio_data_list: list[np.ndarray | None] | None = None,\n    audio_format: str = \"flac\",\n) -&gt; dict[str, Path]:\n    \"\"\"\n    Convenience function to write a batch of samples.\n\n    Args:\n        samples: List of samples with labels\n        output_dir: Output directory\n        midi_data_list: List of MIDI bytes (parallel to samples)\n        audio_data_list: List of audio arrays (parallel to samples)\n        audio_format: Output audio format\n\n    Returns:\n        Dictionary of written Parquet file paths\n    \"\"\"\n    config = StorageConfig(\n        output_dir=Path(output_dir),\n        audio_format=audio_format,\n    )\n    writer = DatasetWriter(config)\n\n    if midi_data_list is None:\n        midi_data_list = [None] * len(samples)\n    if audio_data_list is None:\n        audio_data_list = [None] * len(samples)\n\n    for sample, midi, audio in zip(samples, midi_data_list, audio_data_list):\n        writer.write_sample(sample, midi, audio)\n\n    return writer.flush()\n</code></pre>"},{"location":"api/profiles/","title":"Profiles Module","text":"<p>The profiles module handles player profile generation with skill-tier-based archetypes. Each profile represents a unique \"virtual drummer\" with consistent execution characteristics that determine how they perform rudiments.</p> <p>Profiles capture execution dimensions across several categories:</p> <ul> <li>Timing: Accuracy, consistency, tempo drift, subdivision evenness</li> <li>Dynamics: Velocity mean/variance, accent differentiation and accuracy</li> <li>Hand Balance: Left/right velocity ratio, timing bias, consistency delta</li> <li>Rudiment-Specific: Flam spacing, diddle evenness, roll sustain, buzz consistency</li> </ul> <p>The archetype system generates profiles with realistic correlations between these dimensions based on skill tier (beginner through professional), drawing from music cognition literature for parameter ranges.</p> <p>Player profile definitions and archetype generators.</p> <p>Defines the execution dimensions that characterize drum performance quality and provides archetypes for different skill levels with realistic correlations.</p>"},{"location":"api/profiles/#dataset_gen.profiles.archetypes.SkillTier","title":"SkillTier","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Broad skill categories for player profiles.</p>"},{"location":"api/profiles/#dataset_gen.profiles.archetypes.PlayerProfile","title":"PlayerProfile","text":"<p>               Bases: <code>BaseModel</code></p> <p>Complete player profile for generating synthetic performances.</p> <p>Each profile represents a unique \"virtual drummer\" with consistent execution characteristics across multiple rudiments.</p>"},{"location":"api/profiles/#dataset_gen.profiles.archetypes.PlayerProfile.get_tempo_penalty","title":"get_tempo_penalty","text":"<pre><code>get_tempo_penalty(tempo: int) -&gt; float\n</code></pre> <p>Calculate performance penalty for playing outside comfort range.</p> <p>Returns a multiplier (1.0 = no penalty, &gt;1.0 = worse performance).</p> Source code in <code>dataset_gen/profiles/archetypes.py</code> <pre><code>def get_tempo_penalty(self, tempo: int) -&gt; float:\n    \"\"\"\n    Calculate performance penalty for playing outside comfort range.\n\n    Returns a multiplier (1.0 = no penalty, &gt;1.0 = worse performance).\n    \"\"\"\n    low, high = self.tempo_comfort_range\n    if low &lt;= tempo &lt;= high:\n        return 1.0\n\n    # Linear penalty outside comfort zone\n    if tempo &lt; low:\n        return 1.0 + (low - tempo) * 0.01\n    else:\n        return 1.0 + (tempo - high) * 0.01\n</code></pre>"},{"location":"api/profiles/#dataset_gen.profiles.archetypes.PlayerProfile.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; dict\n</code></pre> <p>Serialize profile to dictionary for storage/transfer.</p> Source code in <code>dataset_gen/profiles/archetypes.py</code> <pre><code>def to_dict(self) -&gt; dict:\n    \"\"\"Serialize profile to dictionary for storage/transfer.\"\"\"\n    data = self.model_dump()\n    # Convert enum to string for JSON compatibility\n    data[\"skill_tier\"] = self.skill_tier.value\n    return data\n</code></pre>"},{"location":"api/profiles/#dataset_gen.profiles.archetypes.PlayerProfile.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(data: dict) -&gt; 'PlayerProfile'\n</code></pre> <p>Deserialize profile from dictionary.</p> Source code in <code>dataset_gen/profiles/archetypes.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: dict) -&gt; \"PlayerProfile\":\n    \"\"\"Deserialize profile from dictionary.\"\"\"\n    # Convert string back to enum if needed\n    if isinstance(data.get(\"skill_tier\"), str):\n        data = data.copy()\n        data[\"skill_tier\"] = SkillTier(data[\"skill_tier\"])\n    return cls.model_validate(data)\n</code></pre>"},{"location":"api/profiles/#dataset_gen.profiles.archetypes.TimingDimensions","title":"TimingDimensions","text":"<p>               Bases: <code>BaseModel</code></p> <p>Timing-related execution dimensions.</p>"},{"location":"api/profiles/#dataset_gen.profiles.archetypes.DynamicsDimensions","title":"DynamicsDimensions","text":"<p>               Bases: <code>BaseModel</code></p> <p>Dynamics-related execution dimensions.</p>"},{"location":"api/profiles/#dataset_gen.profiles.archetypes.HandBalanceDimensions","title":"HandBalanceDimensions","text":"<p>               Bases: <code>BaseModel</code></p> <p>Hand balance execution dimensions.</p>"},{"location":"api/profiles/#dataset_gen.profiles.archetypes.RudimentSpecificDimensions","title":"RudimentSpecificDimensions","text":"<p>               Bases: <code>BaseModel</code></p> <p>Rudiment-specific execution dimensions.</p>"},{"location":"api/profiles/#dataset_gen.profiles.archetypes.ExecutionDimensions","title":"ExecutionDimensions","text":"<p>               Bases: <code>BaseModel</code></p> <p>Complete execution dimensions for a player profile.</p> <p>These capture all the measurable aspects of how a drummer executes rudiments.</p>"},{"location":"api/profiles/#dataset_gen.profiles.archetypes.generate_profile","title":"generate_profile","text":"<pre><code>generate_profile(skill_tier: SkillTier, rng: Generator | None = None, profile_id: str | None = None) -&gt; PlayerProfile\n</code></pre> <p>Generate a player profile for a given skill tier.</p> <p>Parameters:</p> Name Type Description Default <code>skill_tier</code> <code>SkillTier</code> <p>The skill level archetype to base the profile on</p> required <code>rng</code> <code>Generator | None</code> <p>Random number generator for reproducibility</p> <code>None</code> <code>profile_id</code> <code>str | None</code> <p>Optional specific ID for the profile</p> <code>None</code> <p>Returns:</p> Type Description <code>PlayerProfile</code> <p>A PlayerProfile with sampled execution dimensions</p> Source code in <code>dataset_gen/profiles/archetypes.py</code> <pre><code>def generate_profile(\n    skill_tier: SkillTier,\n    rng: np.random.Generator | None = None,\n    profile_id: str | None = None,\n) -&gt; PlayerProfile:\n    \"\"\"\n    Generate a player profile for a given skill tier.\n\n    Args:\n        skill_tier: The skill level archetype to base the profile on\n        rng: Random number generator for reproducibility\n        profile_id: Optional specific ID for the profile\n\n    Returns:\n        A PlayerProfile with sampled execution dimensions\n    \"\"\"\n    if rng is None:\n        rng = np.random.default_rng()\n\n    params = ARCHETYPE_PARAMS[skill_tier]\n\n    # Sample timing dimensions\n    timing = TimingDimensions(\n        timing_accuracy=_clamp(_sample_param(params[\"timing\"][\"timing_accuracy\"], rng), 0, 100),\n        timing_consistency=_clamp(_sample_param(params[\"timing\"][\"timing_consistency\"], rng), 0, 1),\n        tempo_drift=_clamp(_sample_param(params[\"timing\"][\"tempo_drift\"], rng), -0.5, 0.5),\n        subdivision_evenness=_clamp(\n            _sample_param(params[\"timing\"][\"subdivision_evenness\"], rng), 0, 1\n        ),\n    )\n\n    # Sample dynamics dimensions\n    dynamics = DynamicsDimensions(\n        velocity_mean=_clamp(_sample_param(params[\"dynamics\"][\"velocity_mean\"], rng), 30, 127),\n        velocity_variance=_clamp(\n            _sample_param(params[\"dynamics\"][\"velocity_variance\"], rng), 0, 0.5\n        ),\n        accent_differentiation=_clamp(\n            _sample_param(params[\"dynamics\"][\"accent_differentiation\"], rng), 0, 20\n        ),\n        accent_accuracy=_clamp(_sample_param(params[\"dynamics\"][\"accent_accuracy\"], rng), 0, 1),\n    )\n\n    # Sample hand balance dimensions\n    hand_balance = HandBalanceDimensions(\n        lr_velocity_ratio=_clamp(\n            _sample_param(params[\"hand_balance\"][\"lr_velocity_ratio\"], rng), 0.5, 1.0\n        ),\n        lr_timing_bias=_clamp(\n            _sample_param(params[\"hand_balance\"][\"lr_timing_bias\"], rng), -20, 20\n        ),\n        lr_consistency_delta=_clamp(\n            _sample_param(params[\"hand_balance\"][\"lr_consistency_delta\"], rng), 0, 0.3\n        ),\n    )\n\n    # Sample rudiment-specific dimensions\n    rudiment_specific = RudimentSpecificDimensions(\n        flam_spacing=_clamp(\n            _sample_param(params[\"rudiment_specific\"][\"flam_spacing\"], rng), 10, 80\n        ),\n        flam_spacing_variance=_clamp(\n            _sample_param(params[\"rudiment_specific\"][\"flam_spacing_variance\"], rng), 0, 0.5\n        ),\n        diddle_evenness=_clamp(\n            _sample_param(params[\"rudiment_specific\"][\"diddle_evenness\"], rng), 0.6, 1.4\n        ),\n        diddle_variance=_clamp(\n            _sample_param(params[\"rudiment_specific\"][\"diddle_variance\"], rng), 0, 0.3\n        ),\n        roll_sustain=_clamp(\n            _sample_param(params[\"rudiment_specific\"][\"roll_sustain\"], rng), 0, 0.5\n        ),\n        buzz_density_consistency=_clamp(\n            _sample_param(params[\"rudiment_specific\"][\"buzz_density_consistency\"], rng), 0, 1\n        ),\n    )\n\n    # Sample meta parameters\n    fatigue = _clamp(_sample_param(params[\"meta\"][\"fatigue_coefficient\"], rng), 0, 0.3)\n    tempo_range = params[\"meta\"][\"tempo_comfort_range\"][0]\n\n    # Determine dominant hand (90% right-handed)\n    dominant_hand = \"left\" if rng.random() &lt; 0.1 else \"right\"\n\n    return PlayerProfile(\n        id=profile_id or str(uuid4()),\n        skill_tier=skill_tier,\n        dimensions=ExecutionDimensions(\n            timing=timing,\n            dynamics=dynamics,\n            hand_balance=hand_balance,\n            rudiment_specific=rudiment_specific,\n        ),\n        dominant_hand=dominant_hand,\n        fatigue_coefficient=fatigue,\n        tempo_comfort_range=tempo_range,\n    )\n</code></pre>"},{"location":"api/profiles/#dataset_gen.profiles.archetypes.generate_profiles_batch","title":"generate_profiles_batch","text":"<pre><code>generate_profiles_batch(n_profiles: int, skill_distribution: dict[SkillTier, float] | None = None, seed: int | None = None) -&gt; list[PlayerProfile]\n</code></pre> <p>Generate a batch of player profiles with specified skill distribution.</p> <p>Parameters:</p> Name Type Description Default <code>n_profiles</code> <code>int</code> <p>Number of profiles to generate</p> required <code>skill_distribution</code> <code>dict[SkillTier, float] | None</code> <p>Dict mapping skill tiers to proportions (must sum to 1).                Defaults to uniform distribution.</p> <code>None</code> <code>seed</code> <code>int | None</code> <p>Random seed for reproducibility</p> <code>None</code> <p>Returns:</p> Type Description <code>list[PlayerProfile]</code> <p>List of PlayerProfile objects</p> Source code in <code>dataset_gen/profiles/archetypes.py</code> <pre><code>def generate_profiles_batch(\n    n_profiles: int,\n    skill_distribution: dict[SkillTier, float] | None = None,\n    seed: int | None = None,\n) -&gt; list[PlayerProfile]:\n    \"\"\"\n    Generate a batch of player profiles with specified skill distribution.\n\n    Args:\n        n_profiles: Number of profiles to generate\n        skill_distribution: Dict mapping skill tiers to proportions (must sum to 1).\n                           Defaults to uniform distribution.\n        seed: Random seed for reproducibility\n\n    Returns:\n        List of PlayerProfile objects\n    \"\"\"\n    rng = np.random.default_rng(seed)\n\n    if skill_distribution is None:\n        skill_distribution = {tier: 0.25 for tier in SkillTier}\n\n    # Normalize distribution\n    total = sum(skill_distribution.values())\n    skill_distribution = {k: v / total for k, v in skill_distribution.items()}\n\n    # Calculate counts per tier\n    tiers = list(skill_distribution.keys())\n    probs = [skill_distribution[t] for t in tiers]\n\n    # Use indices to avoid numpy string conversion of enum values\n    tier_indices = rng.choice(len(tiers), size=n_profiles, p=probs)\n\n    profiles = []\n    for idx in tier_indices:\n        profiles.append(generate_profile(tiers[idx], rng))\n\n    return profiles\n</code></pre>"},{"location":"api/rudiments/","title":"Rudiments Module","text":"<p>The rudiments module handles loading and parsing of PAS (Percussive Arts Society) drum rudiment definitions. It provides Pydantic models for representing the 40 standard drum rudiments with their sticking patterns, accent patterns, and articulation-specific parameters.</p> <p>Rudiment definitions are stored as YAML files in <code>dataset_gen/rudiments/definitions/</code> and specify:</p> <ul> <li>Stroke patterns with hand assignments (R/L)</li> <li>Articulation types (tap, accent, grace note, diddle, buzz)</li> <li>Tempo ranges and subdivisions</li> <li>Category classification (roll, diddle, flam, drag)</li> </ul>"},{"location":"api/rudiments/#schema-classes","title":"Schema Classes","text":"<p>Pydantic models for rudiment definitions.</p> <p>The schema captures all 40 PAS rudiments with their sticking patterns, accent patterns, and articulation-specific parameters.</p>"},{"location":"api/rudiments/#dataset_gen.rudiments.schema.Hand","title":"Hand","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Which hand plays the stroke.</p>"},{"location":"api/rudiments/#dataset_gen.rudiments.schema.StrokeType","title":"StrokeType","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Type of drum stroke.</p>"},{"location":"api/rudiments/#dataset_gen.rudiments.schema.Stroke","title":"Stroke","text":"<p>               Bases: <code>BaseModel</code></p> <p>A single stroke in a rudiment pattern.</p>"},{"location":"api/rudiments/#dataset_gen.rudiments.schema.Stroke.is_accented","title":"is_accented","text":"<pre><code>is_accented() -&gt; bool\n</code></pre> <p>Returns True if this is an accented stroke.</p> Source code in <code>dataset_gen/rudiments/schema.py</code> <pre><code>def is_accented(self) -&gt; bool:\n    \"\"\"Returns True if this is an accented stroke.\"\"\"\n    return self.stroke_type == StrokeType.ACCENT\n</code></pre>"},{"location":"api/rudiments/#dataset_gen.rudiments.schema.Stroke.is_grace_note","title":"is_grace_note","text":"<pre><code>is_grace_note() -&gt; bool\n</code></pre> <p>Returns True if this is a grace note.</p> Source code in <code>dataset_gen/rudiments/schema.py</code> <pre><code>def is_grace_note(self) -&gt; bool:\n    \"\"\"Returns True if this is a grace note.\"\"\"\n    return self.stroke_type == StrokeType.GRACE\n</code></pre>"},{"location":"api/rudiments/#dataset_gen.rudiments.schema.StickingPattern","title":"StickingPattern","text":"<p>               Bases: <code>BaseModel</code></p> <p>A complete sticking pattern for a rudiment.</p> <p>Patterns are defined per beat grouping (e.g., one paradiddle cycle). The pattern repeats for the duration of the exercise.</p>"},{"location":"api/rudiments/#dataset_gen.rudiments.schema.StickingPattern.stroke_count","title":"stroke_count","text":"<pre><code>stroke_count() -&gt; int\n</code></pre> <p>Total number of strokes in one cycle.</p> Source code in <code>dataset_gen/rudiments/schema.py</code> <pre><code>def stroke_count(self) -&gt; int:\n    \"\"\"Total number of strokes in one cycle.\"\"\"\n    return len(self.strokes)\n</code></pre>"},{"location":"api/rudiments/#dataset_gen.rudiments.schema.StickingPattern.accent_positions","title":"accent_positions","text":"<pre><code>accent_positions() -&gt; list[int]\n</code></pre> <p>Indices of accented strokes.</p> Source code in <code>dataset_gen/rudiments/schema.py</code> <pre><code>def accent_positions(self) -&gt; list[int]:\n    \"\"\"Indices of accented strokes.\"\"\"\n    return [i for i, s in enumerate(self.strokes) if s.is_accented()]\n</code></pre>"},{"location":"api/rudiments/#dataset_gen.rudiments.schema.StickingPattern.grace_note_positions","title":"grace_note_positions","text":"<pre><code>grace_note_positions() -&gt; list[int]\n</code></pre> <p>Indices of grace notes.</p> Source code in <code>dataset_gen/rudiments/schema.py</code> <pre><code>def grace_note_positions(self) -&gt; list[int]:\n    \"\"\"Indices of grace notes.\"\"\"\n    return [i for i, s in enumerate(self.strokes) if s.is_grace_note()]\n</code></pre>"},{"location":"api/rudiments/#dataset_gen.rudiments.schema.RudimentCategory","title":"RudimentCategory","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>PAS rudiment categories.</p>"},{"location":"api/rudiments/#dataset_gen.rudiments.schema.RudimentParams","title":"RudimentParams","text":"<p>               Bases: <code>BaseModel</code></p> <p>Rudiment-specific parameters that affect generation.</p> <p>Different rudiment types have different articulation requirements that affect how we generate timing and velocity.</p>"},{"location":"api/rudiments/#dataset_gen.rudiments.schema.Subdivision","title":"Subdivision","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Base subdivision for the rudiment.</p>"},{"location":"api/rudiments/#dataset_gen.rudiments.schema.Rudiment","title":"Rudiment","text":"<p>               Bases: <code>BaseModel</code></p> <p>Complete definition of a PAS drum rudiment.</p> <p>This captures everything needed to generate MIDI for the rudiment with appropriate timing, velocity, and articulation.</p>"},{"location":"api/rudiments/#dataset_gen.rudiments.schema.Rudiment.validate_slug","title":"validate_slug  <code>classmethod</code>","text":"<pre><code>validate_slug(v: str) -&gt; str\n</code></pre> <p>Ensure slug is URL-safe.</p> Source code in <code>dataset_gen/rudiments/schema.py</code> <pre><code>@field_validator(\"slug\")\n@classmethod\ndef validate_slug(cls, v: str) -&gt; str:\n    \"\"\"Ensure slug is URL-safe.\"\"\"\n    import re\n\n    if not re.match(r\"^[a-z0-9_]+$\", v):\n        raise ValueError(\"Slug must be lowercase alphanumeric with underscores only\")\n    return v\n</code></pre>"},{"location":"api/rudiments/#dataset_gen.rudiments.schema.Rudiment.strokes_per_beat","title":"strokes_per_beat","text":"<pre><code>strokes_per_beat() -&gt; float\n</code></pre> <p>Calculate how many strokes occur per beat.</p> Source code in <code>dataset_gen/rudiments/schema.py</code> <pre><code>def strokes_per_beat(self) -&gt; float:\n    \"\"\"Calculate how many strokes occur per beat.\"\"\"\n    return self.pattern.stroke_count() / self.pattern.beats_per_cycle\n</code></pre>"},{"location":"api/rudiments/#dataset_gen.rudiments.schema.Rudiment.duration_at_tempo","title":"duration_at_tempo","text":"<pre><code>duration_at_tempo(tempo_bpm: int, num_cycles: int = 1) -&gt; float\n</code></pre> <p>Calculate duration in seconds for given number of cycles at tempo.</p> Source code in <code>dataset_gen/rudiments/schema.py</code> <pre><code>def duration_at_tempo(self, tempo_bpm: int, num_cycles: int = 1) -&gt; float:\n    \"\"\"Calculate duration in seconds for given number of cycles at tempo.\"\"\"\n    beats = self.pattern.beats_per_cycle * num_cycles\n    return beats * (60.0 / tempo_bpm)\n</code></pre>"},{"location":"api/rudiments/#dataset_gen.rudiments.schema.Rudiment.get_alternating_pattern","title":"get_alternating_pattern","text":"<pre><code>get_alternating_pattern() -&gt; StickingPattern\n</code></pre> <p>Return the pattern with hands swapped (for practicing alternating starts).</p> Source code in <code>dataset_gen/rudiments/schema.py</code> <pre><code>def get_alternating_pattern(self) -&gt; StickingPattern:\n    \"\"\"\n    Return the pattern with hands swapped (for practicing alternating starts).\n    \"\"\"\n    swapped_strokes = []\n    for stroke in self.pattern.strokes:\n        new_hand = Hand.LEFT if stroke.hand == Hand.RIGHT else Hand.RIGHT\n        swapped_strokes.append(stroke.model_copy(update={\"hand\": new_hand}))\n\n    return StickingPattern(\n        strokes=swapped_strokes, beats_per_cycle=self.pattern.beats_per_cycle\n    )\n</code></pre>"},{"location":"api/rudiments/#loader","title":"Loader","text":"<p>The loader module provides functions for parsing YAML rudiment definitions into Pydantic models.</p> <p>Load rudiment definitions from YAML files.</p>"},{"location":"api/rudiments/#dataset_gen.rudiments.loader.load_rudiment","title":"load_rudiment","text":"<pre><code>load_rudiment(path: Path | str) -&gt; Rudiment\n</code></pre> <p>Load a single rudiment definition from a YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path | str</code> <p>Path to the YAML file</p> required <p>Returns:</p> Type Description <code>Rudiment</code> <p>Parsed Rudiment object</p> Source code in <code>dataset_gen/rudiments/loader.py</code> <pre><code>def load_rudiment(path: Path | str) -&gt; Rudiment:\n    \"\"\"\n    Load a single rudiment definition from a YAML file.\n\n    Args:\n        path: Path to the YAML file\n\n    Returns:\n        Parsed Rudiment object\n    \"\"\"\n    path = Path(path)\n    with open(path) as f:\n        data = yaml.safe_load(f)\n\n    pattern = _parse_pattern_from_yaml(data[\"pattern\"])\n    params = _parse_rudiment_params(data.get(\"params\"))\n\n    return Rudiment(\n        name=data[\"name\"],\n        slug=data[\"slug\"],\n        category=RudimentCategory(data[\"category\"]),\n        pattern=pattern,\n        subdivision=Subdivision(data.get(\"subdivision\", \"sixteenth\")),\n        tempo_range=tuple(data.get(\"tempo_range\", [60, 180])),\n        params=params,\n        pas_number=data.get(\"pas_number\"),\n        description=data.get(\"description\"),\n        starts_on_left=data.get(\"starts_on_left\", False),\n    )\n</code></pre>"},{"location":"api/rudiments/#dataset_gen.rudiments.loader.load_all_rudiments","title":"load_all_rudiments","text":"<pre><code>load_all_rudiments(directory: Path | str | None = None) -&gt; dict[str, Rudiment]\n</code></pre> <p>Load all rudiment definitions from a directory.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>Path | str | None</code> <p>Path to directory containing YAML files.        Defaults to the built-in definitions directory.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Rudiment]</code> <p>Dict mapping slug to Rudiment object</p> Source code in <code>dataset_gen/rudiments/loader.py</code> <pre><code>def load_all_rudiments(directory: Path | str | None = None) -&gt; dict[str, Rudiment]:\n    \"\"\"\n    Load all rudiment definitions from a directory.\n\n    Args:\n        directory: Path to directory containing YAML files.\n                   Defaults to the built-in definitions directory.\n\n    Returns:\n        Dict mapping slug to Rudiment object\n    \"\"\"\n    if directory is None:\n        directory = DEFINITIONS_DIR\n    directory = Path(directory)\n\n    if not directory.exists():\n        return {}\n\n    rudiments = {}\n    for yaml_file in directory.glob(\"*.yaml\"):\n        try:\n            rudiment = load_rudiment(yaml_file)\n            rudiments[rudiment.slug] = rudiment\n        except Exception as e:\n            raise ValueError(f\"Failed to load {yaml_file}: {e}\") from e\n\n    return rudiments\n</code></pre>"},{"location":"api/rudiments/#dataset_gen.rudiments.loader.get_rudiments_by_category","title":"get_rudiments_by_category","text":"<pre><code>get_rudiments_by_category(rudiments: dict[str, Rudiment] | None = None) -&gt; dict[RudimentCategory, list[Rudiment]]\n</code></pre> <p>Group rudiments by category.</p> <p>Parameters:</p> Name Type Description Default <code>rudiments</code> <code>dict[str, Rudiment] | None</code> <p>Dict of rudiments. If None, loads all definitions.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[RudimentCategory, list[Rudiment]]</code> <p>Dict mapping category to list of rudiments</p> Source code in <code>dataset_gen/rudiments/loader.py</code> <pre><code>def get_rudiments_by_category(\n    rudiments: dict[str, Rudiment] | None = None,\n) -&gt; dict[RudimentCategory, list[Rudiment]]:\n    \"\"\"\n    Group rudiments by category.\n\n    Args:\n        rudiments: Dict of rudiments. If None, loads all definitions.\n\n    Returns:\n        Dict mapping category to list of rudiments\n    \"\"\"\n    if rudiments is None:\n        rudiments = load_all_rudiments()\n\n    by_category: dict[RudimentCategory, list[Rudiment]] = {cat: [] for cat in RudimentCategory}\n\n    for rudiment in rudiments.values():\n        by_category[rudiment.category].append(rudiment)\n\n    return by_category\n</code></pre>"},{"location":"api/validation/","title":"Validation Module","text":"<p>The validation module provides tools for verifying dataset correctness and analyzing statistical properties. It includes:</p> <ul> <li>Verification: Data integrity checks, range validation, skill tier ordering</li> <li>Analysis: Distribution statistics across all hierarchical levels</li> <li>Reporting: Comprehensive validation reports with pass/fail summaries</li> </ul>"},{"location":"api/validation/#validation-report","title":"Validation Report","text":"<p>Generates comprehensive validation reports combining statistical analysis and verification results.</p> <p>Validation report generation.</p> <p>This module generates comprehensive validation reports combining statistical analysis and verification results.</p>"},{"location":"api/validation/#dataset_gen.validation.report.ValidationReport","title":"ValidationReport  <code>dataclass</code>","text":"<pre><code>ValidationReport(dataset_path: str, generated_at: str, stats: DatasetStats, verification: VerificationResult, skill_tier_ordering: dict[str, Any], realism: RealismReport | None = None)\n</code></pre> <p>Complete validation report.</p>"},{"location":"api/validation/#dataset_gen.validation.report.ValidationReport.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; dict\n</code></pre> <p>Convert to dictionary.</p> Source code in <code>dataset_gen/validation/report.py</code> <pre><code>def to_dict(self) -&gt; dict:\n    \"\"\"Convert to dictionary.\"\"\"\n    result = {\n        \"dataset_path\": self.dataset_path,\n        \"generated_at\": self.generated_at,\n        \"stats\": self.stats.to_dict(),\n        \"verification\": self.verification.to_dict(),\n        \"skill_tier_ordering\": self.skill_tier_ordering,\n    }\n    if self.realism:\n        result[\"realism\"] = self.realism.to_dict()\n    return result\n</code></pre>"},{"location":"api/validation/#dataset_gen.validation.report.ValidationReport.save","title":"save","text":"<pre><code>save(output_path: Path | str) -&gt; None\n</code></pre> <p>Save report to JSON file.</p> Source code in <code>dataset_gen/validation/report.py</code> <pre><code>def save(self, output_path: Path | str) -&gt; None:\n    \"\"\"Save report to JSON file.\"\"\"\n    output_path = Path(output_path)\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n\n    with open(output_path, \"w\") as f:\n        json.dump(self.to_dict(), f, indent=2, default=str)\n\n    logger.info(f\"Report saved to {output_path}\")\n</code></pre>"},{"location":"api/validation/#dataset_gen.validation.report.ValidationReport.summary","title":"summary","text":"<pre><code>summary() -&gt; str\n</code></pre> <p>Generate human-readable summary.</p> Source code in <code>dataset_gen/validation/report.py</code> <pre><code>def summary(self) -&gt; str:\n    \"\"\"Generate human-readable summary.\"\"\"\n    lines = [\n        \"=\" * 60,\n        \"DATASET VALIDATION REPORT\",\n        \"=\" * 60,\n        f\"Dataset: {self.dataset_path}\",\n        f\"Generated: {self.generated_at}\",\n        \"\",\n        \"--- Dataset Overview ---\",\n        f\"Samples:    {self.stats.num_samples:,}\",\n        f\"Profiles:   {self.stats.num_profiles:,}\",\n        f\"Rudiments:  {self.stats.num_rudiments:,}\",\n        \"\",\n        \"--- Skill Tier Distribution ---\",\n    ]\n\n    for tier, count in sorted(self.stats.skill_tier_counts.items()):\n        pct = 100 * count / max(self.stats.num_samples, 1)\n        lines.append(f\"  {tier}: {count:,} ({pct:.1f}%)\")\n\n    lines.extend(\n        [\n            \"\",\n            \"--- Split Distribution ---\",\n        ]\n    )\n\n    for split, count in sorted(self.stats.split_counts.items()):\n        pct = 100 * count / max(self.stats.num_samples, 1)\n        lines.append(f\"  {split}: {count:,} ({pct:.1f}%)\")\n\n    lines.extend(\n        [\n            \"\",\n            \"--- Key Statistics ---\",\n            f\"Tempo:           {self.stats.tempo_stats.mean:.1f} \u00b1 {self.stats.tempo_stats.std:.1f} BPM\",\n            f\"Duration:        {self.stats.duration_stats.mean:.2f} \u00b1 {self.stats.duration_stats.std:.2f} sec\",\n            f\"Strokes/sample:  {self.stats.num_strokes_stats.mean:.1f} \u00b1 {self.stats.num_strokes_stats.std:.1f}\",\n            \"\",\n            \"--- Timing Error by Skill Tier ---\",\n        ]\n    )\n\n    for tier, tier_stats in sorted(\n        self.stats.timing_error_stats.by_group.items(),\n        key=lambda x: x[0],\n    ):\n        lines.append(f\"  {tier}: {tier_stats.mean:.2f} \u00b1 {tier_stats.std:.2f} ms\")\n\n    lines.extend(\n        [\n            \"\",\n            \"--- Exercise Scores by Skill Tier ---\",\n            \"Timing Accuracy:\",\n        ]\n    )\n\n    for tier, tier_stats in sorted(\n        self.stats.exercise_timing_accuracy.by_group.items(),\n        key=lambda x: x[0],\n    ):\n        lines.append(f\"  {tier}: {tier_stats.mean:.1f} \u00b1 {tier_stats.std:.1f}\")\n\n    lines.append(\"Hand Balance:\")\n    for tier, tier_stats in sorted(\n        self.stats.exercise_hand_balance.by_group.items(),\n        key=lambda x: x[0],\n    ):\n        lines.append(f\"  {tier}: {tier_stats.mean:.1f} \u00b1 {tier_stats.std:.1f}\")\n\n    lines.extend(\n        [\n            \"\",\n            \"--- Skill Tier Ordering ---\",\n        ]\n    )\n\n    for key, value in self.skill_tier_ordering.items():\n        if isinstance(value, bool):\n            status = \"\u2713\" if value else \"\u2717\"\n            lines.append(f\"  {status} {key}\")\n\n    lines.extend(\n        [\n            \"\",\n            \"--- Verification Results ---\",\n            f\"Passed: {self.verification.num_passed}/{len(self.verification.checks)}\",\n            \"\",\n        ]\n    )\n\n    for check in self.verification.checks:\n        status = \"\u2713\" if check.passed else \"\u2717\"\n        lines.append(f\"  {status} {check.name}: {check.message}\")\n\n    # Realism validation results\n    if self.realism:\n        lines.extend(\n            [\n                \"\",\n                \"--- Realism Validation ---\",\n                f\"Literature comparison: {self.realism.literature_pass_rate:.1f}% pass\",\n                f\"Correlation structure: {self.realism.correlation_pass_rate:.1f}% pass\",\n                \"\",\n            ]\n        )\n\n        # Literature comparisons by tier\n        for comp in self.realism.literature_comparisons:\n            status = \"\u2713\" if comp.within_range else \"\u2717\"\n            lines.append(\n                f\"  {status} {comp.skill_tier} {comp.metric}: \"\n                f\"{comp.dataset_value:.2f} (expected {comp.expected_range})\"\n            )\n\n    lines.extend(\n        [\n            \"\",\n            \"=\" * 60,\n            f\"Overall: {'PASSED' if self.verification.all_passed else 'FAILED'}\",\n            \"=\" * 60,\n        ]\n    )\n\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"api/validation/#dataset_gen.validation.report.generate_report","title":"generate_report","text":"<pre><code>generate_report(dataset_dir: Path | str, output_path: Path | str | None = None, include_realism: bool = True, include_midi_checks: bool = True) -&gt; ValidationReport\n</code></pre> <p>Generate comprehensive validation report for a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_dir</code> <code>Path | str</code> <p>Path to dataset directory</p> required <code>output_path</code> <code>Path | str | None</code> <p>Optional path to save report JSON</p> <code>None</code> <code>include_realism</code> <code>bool</code> <p>Whether to run realism validation (literature comparison)</p> <code>True</code> <code>include_midi_checks</code> <code>bool</code> <p>Whether to run MIDI alignment checks</p> <code>True</code> <p>Returns:</p> Type Description <code>ValidationReport</code> <p>ValidationReport with stats and verification results</p> Source code in <code>dataset_gen/validation/report.py</code> <pre><code>def generate_report(\n    dataset_dir: Path | str,\n    output_path: Path | str | None = None,\n    include_realism: bool = True,\n    include_midi_checks: bool = True,\n) -&gt; ValidationReport:\n    \"\"\"\n    Generate comprehensive validation report for a dataset.\n\n    Args:\n        dataset_dir: Path to dataset directory\n        output_path: Optional path to save report JSON\n        include_realism: Whether to run realism validation (literature comparison)\n        include_midi_checks: Whether to run MIDI alignment checks\n\n    Returns:\n        ValidationReport with stats and verification results\n    \"\"\"\n    dataset_dir = Path(dataset_dir)\n    logger.info(f\"Generating validation report for {dataset_dir}\")\n\n    # Run analysis\n    analyzer = DatasetAnalyzer(dataset_dir)\n    stats = analyzer.compute_stats()\n    skill_ordering = analyzer.check_skill_tier_ordering()\n\n    # Run verification\n    verifier = LabelVerifier(dataset_dir)\n    verification = verifier.verify_all(include_midi_checks=include_midi_checks)\n\n    # Run realism validation\n    realism = None\n    if include_realism:\n        logger.info(\"Running realism validation...\")\n        realism_validator = RealismValidator(dataset_dir)\n        realism = realism_validator.validate_all()\n\n    # Create report\n    report = ValidationReport(\n        dataset_path=str(dataset_dir),\n        generated_at=datetime.now().isoformat(),\n        stats=stats,\n        verification=verification,\n        skill_tier_ordering=skill_ordering,\n        realism=realism,\n    )\n\n    # Save if requested\n    if output_path:\n        report.save(output_path)\n\n    return report\n</code></pre>"},{"location":"api/validation/#dataset_gen.validation.report.quick_validate","title":"quick_validate","text":"<pre><code>quick_validate(dataset_dir: Path | str) -&gt; bool\n</code></pre> <p>Quick validation check - returns True if dataset is valid.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_dir</code> <code>Path | str</code> <p>Path to dataset directory</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if all verification checks pass</p> Source code in <code>dataset_gen/validation/report.py</code> <pre><code>def quick_validate(dataset_dir: Path | str) -&gt; bool:\n    \"\"\"\n    Quick validation check - returns True if dataset is valid.\n\n    Args:\n        dataset_dir: Path to dataset directory\n\n    Returns:\n        True if all verification checks pass\n    \"\"\"\n    verifier = LabelVerifier(dataset_dir)\n    result = verifier.verify_all()\n    return result.all_passed\n</code></pre>"},{"location":"api/validation/#dataset_gen.validation.report.generate_html_report","title":"generate_html_report","text":"<pre><code>generate_html_report(report: ValidationReport, output_path: Path | str, template_path: Path | str | None = None) -&gt; None\n</code></pre> <p>Generate an HTML validation report with visualizations.</p> <p>Parameters:</p> Name Type Description Default <code>report</code> <code>ValidationReport</code> <p>ValidationReport to render</p> required <code>output_path</code> <code>Path | str</code> <p>Path to save HTML file</p> required <code>template_path</code> <code>Path | str | None</code> <p>Optional path to custom template</p> <code>None</code> Source code in <code>dataset_gen/validation/report.py</code> <pre><code>def generate_html_report(\n    report: ValidationReport,\n    output_path: Path | str,\n    template_path: Path | str | None = None,\n) -&gt; None:\n    \"\"\"\n    Generate an HTML validation report with visualizations.\n\n    Args:\n        report: ValidationReport to render\n        output_path: Path to save HTML file\n        template_path: Optional path to custom template\n    \"\"\"\n    import re\n\n    # Find template\n    if template_path is None:\n        # Look in scripts/templates relative to this file\n        module_dir = Path(__file__).parent.parent.parent\n        template_path = module_dir / \"scripts\" / \"templates\" / \"validation_report.html\"\n\n    template_path = Path(template_path)\n    if not template_path.exists():\n        logger.warning(f\"HTML template not found at {template_path}\")\n        # Fall back to generating a simple HTML report\n        _generate_simple_html(report, output_path)\n        return\n\n    # Read template\n    with open(template_path) as f:\n        html = f.read()\n\n    # Embed report data as JSON\n    report_json = json.dumps(report.to_dict(), indent=2, default=str)\n\n    # Replace placeholder with actual data\n    html = re.sub(\n        r\"/\\* REPORT_DATA_PLACEHOLDER \\*/ \\{\\}\",\n        report_json,\n        html,\n    )\n\n    # Write output\n    output_path = Path(output_path)\n    output_path.parent.mkdir(parents=True, exist_ok=True)\n    with open(output_path, \"w\") as f:\n        f.write(html)\n\n    logger.info(f\"HTML report saved to {output_path}\")\n</code></pre>"},{"location":"api/validation/#label-verification","title":"Label Verification","text":"<p>Verifies label correctness and data integrity through a series of automated checks.</p> <p>Label verification for generated datasets.</p> <p>This module verifies that computed labels correctly reflect the generation parameters and that data integrity is maintained.</p>"},{"location":"api/validation/#dataset_gen.validation.verify.VerificationCheck","title":"VerificationCheck  <code>dataclass</code>","text":"<pre><code>VerificationCheck(name: str, passed: bool, message: str, details: dict = dict())\n</code></pre> <p>Result of a single verification check.</p>"},{"location":"api/validation/#dataset_gen.validation.verify.VerificationResult","title":"VerificationResult  <code>dataclass</code>","text":"<pre><code>VerificationResult(checks: list[VerificationCheck] = list())\n</code></pre> <p>Complete verification results.</p>"},{"location":"api/validation/#dataset_gen.validation.verify.VerificationResult.summary","title":"summary","text":"<pre><code>summary() -&gt; str\n</code></pre> <p>Get summary string.</p> Source code in <code>dataset_gen/validation/verify.py</code> <pre><code>def summary(self) -&gt; str:\n    \"\"\"Get summary string.\"\"\"\n    lines = [\n        f\"Verification: {self.num_passed}/{len(self.checks)} checks passed\",\n        \"\",\n    ]\n    for check in self.checks:\n        status = \"\u2713\" if check.passed else \"\u2717\"\n        lines.append(f\"  {status} {check.name}: {check.message}\")\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"api/validation/#dataset_gen.validation.verify.LabelVerifier","title":"LabelVerifier","text":"<pre><code>LabelVerifier(dataset_dir: Path | str, midi_dir: Path | str | None = None)\n</code></pre> <p>Verify label correctness and data integrity.</p> <p>Runs a series of checks to ensure: - Labels are within expected ranges - Skill tier differences are reflected in metrics - Data relationships are consistent - No data corruption occurred</p> <p>Initialize verifier.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_dir</code> <code>Path | str</code> <p>Path to dataset directory</p> required <code>midi_dir</code> <code>Path | str | None</code> <p>Path to MIDI files (defaults to dataset_dir/midi)</p> <code>None</code> Source code in <code>dataset_gen/validation/verify.py</code> <pre><code>def __init__(self, dataset_dir: Path | str, midi_dir: Path | str | None = None):\n    \"\"\"\n    Initialize verifier.\n\n    Args:\n        dataset_dir: Path to dataset directory\n        midi_dir: Path to MIDI files (defaults to dataset_dir/midi)\n    \"\"\"\n    self.dataset_dir = Path(dataset_dir)\n    self.reader = ParquetReader(dataset_dir)\n    self.midi_dir = Path(midi_dir) if midi_dir else self.dataset_dir / \"midi\"\n\n    self._samples_df: pd.DataFrame | None = None\n    self._strokes_df: pd.DataFrame | None = None\n    self._measures_df: pd.DataFrame | None = None\n    self._exercises_df: pd.DataFrame | None = None\n    self._rudiments: dict | None = None\n</code></pre>"},{"location":"api/validation/#dataset_gen.validation.verify.LabelVerifier.rudiments","title":"rudiments  <code>property</code>","text":"<pre><code>rudiments: dict\n</code></pre> <p>Lazily load rudiment definitions.</p>"},{"location":"api/validation/#dataset_gen.validation.verify.LabelVerifier.load_data","title":"load_data","text":"<pre><code>load_data() -&gt; None\n</code></pre> <p>Load all parquet data.</p> Source code in <code>dataset_gen/validation/verify.py</code> <pre><code>def load_data(self) -&gt; None:\n    \"\"\"Load all parquet data.\"\"\"\n    self._samples_df = self.reader.load_samples()\n    self._strokes_df = self.reader.load_strokes()\n    self._measures_df = self.reader.load_measures()\n    self._exercises_df = self.reader.load_exercises()\n</code></pre>"},{"location":"api/validation/#dataset_gen.validation.verify.LabelVerifier.verify_all","title":"verify_all","text":"<pre><code>verify_all(include_midi_checks: bool = True) -&gt; VerificationResult\n</code></pre> <p>Run all verification checks.</p> <p>Parameters:</p> Name Type Description Default <code>include_midi_checks</code> <code>bool</code> <p>Whether to run MIDI alignment checks (slower)</p> <code>True</code> Source code in <code>dataset_gen/validation/verify.py</code> <pre><code>def verify_all(self, include_midi_checks: bool = True) -&gt; VerificationResult:\n    \"\"\"\n    Run all verification checks.\n\n    Args:\n        include_midi_checks: Whether to run MIDI alignment checks (slower)\n    \"\"\"\n    result = VerificationResult()\n\n    # Parquet file integrity (run first - if this fails, other checks can't run)\n    result.checks.append(self._check_parquet_integrity())\n\n    # Data integrity checks\n    result.checks.append(self._check_sample_ids_unique())\n    result.checks.append(self._check_stroke_sample_refs())\n    result.checks.append(self._check_measure_sample_refs())\n    result.checks.append(self._check_exercise_sample_refs())\n\n    # Range checks\n    result.checks.append(self._check_velocity_range())\n    result.checks.append(self._check_timing_range())\n    result.checks.append(self._check_score_ranges())\n\n    # Consistency checks\n    result.checks.append(self._check_stroke_counts_match())\n    result.checks.append(self._check_measure_counts_match())\n\n    # Skill tier checks\n    result.checks.append(self._check_skill_tier_timing())\n    result.checks.append(self._check_skill_tier_hand_balance())\n\n    # Rudiment pattern correctness\n    result.checks.append(self._check_rudiment_pattern_correctness())\n\n    # Label-to-MIDI alignment (optional, slower)\n    if include_midi_checks and self.midi_dir.exists():\n        result.checks.append(self._check_label_midi_alignment())\n\n    return result\n</code></pre>"},{"location":"api/validation/#dataset_gen.validation.verify.verify_labels","title":"verify_labels","text":"<pre><code>verify_labels(dataset_dir: Path | str) -&gt; VerificationResult\n</code></pre> <p>Convenience function to verify a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_dir</code> <code>Path | str</code> <p>Path to dataset directory</p> required <p>Returns:</p> Type Description <code>VerificationResult</code> <p>VerificationResult with all checks</p> Source code in <code>dataset_gen/validation/verify.py</code> <pre><code>def verify_labels(dataset_dir: Path | str) -&gt; VerificationResult:\n    \"\"\"\n    Convenience function to verify a dataset.\n\n    Args:\n        dataset_dir: Path to dataset directory\n\n    Returns:\n        VerificationResult with all checks\n    \"\"\"\n    verifier = LabelVerifier(dataset_dir)\n    return verifier.verify_all()\n</code></pre>"},{"location":"api/validation/#statistical-analysis","title":"Statistical Analysis","text":"<p>Analyzes generated dataset distributions across samples, strokes, measures, and exercises.</p> <p>Statistical analysis of generated datasets.</p> <p>This module provides tools for analyzing the distribution of generated samples across various dimensions.</p>"},{"location":"api/validation/#dataset_gen.validation.analysis.DistributionStats","title":"DistributionStats  <code>dataclass</code>","text":"<pre><code>DistributionStats(name: str, count: int, mean: float, std: float, min: float, max: float, median: float, q25: float, q75: float, skewness: float, kurtosis: float, by_group: dict[str, 'DistributionStats'] = dict())\n</code></pre> <p>Statistics for a single distribution.</p>"},{"location":"api/validation/#dataset_gen.validation.analysis.DistributionStats.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; dict\n</code></pre> <p>Convert to dictionary.</p> Source code in <code>dataset_gen/validation/analysis.py</code> <pre><code>def to_dict(self) -&gt; dict:\n    \"\"\"Convert to dictionary.\"\"\"\n    result = {\n        \"name\": self.name,\n        \"count\": self.count,\n        \"mean\": self.mean,\n        \"std\": self.std,\n        \"min\": self.min,\n        \"max\": self.max,\n        \"median\": self.median,\n        \"q25\": self.q25,\n        \"q75\": self.q75,\n        \"skewness\": self.skewness,\n        \"kurtosis\": self.kurtosis,\n    }\n    if self.by_group:\n        result[\"by_group\"] = {k: v.to_dict() for k, v in self.by_group.items()}\n    return result\n</code></pre>"},{"location":"api/validation/#dataset_gen.validation.analysis.DistributionStats.from_array","title":"from_array  <code>classmethod</code>","text":"<pre><code>from_array(name: str, data: ndarray) -&gt; 'DistributionStats'\n</code></pre> <p>Compute statistics from numpy array.</p> Source code in <code>dataset_gen/validation/analysis.py</code> <pre><code>@classmethod\ndef from_array(cls, name: str, data: np.ndarray) -&gt; \"DistributionStats\":\n    \"\"\"Compute statistics from numpy array.\"\"\"\n    if len(data) == 0:\n        return cls(\n            name=name,\n            count=0,\n            mean=0.0,\n            std=0.0,\n            min=0.0,\n            max=0.0,\n            median=0.0,\n            q25=0.0,\n            q75=0.0,\n            skewness=0.0,\n            kurtosis=0.0,\n        )\n\n    return cls(\n        name=name,\n        count=len(data),\n        mean=float(np.mean(data)),\n        std=float(np.std(data)),\n        min=float(np.min(data)),\n        max=float(np.max(data)),\n        median=float(np.median(data)),\n        q25=float(np.percentile(data, 25)),\n        q75=float(np.percentile(data, 75)),\n        skewness=float(stats.skew(data)) if len(data) &gt; 2 else 0.0,\n        kurtosis=float(stats.kurtosis(data)) if len(data) &gt; 3 else 0.0,\n    )\n</code></pre>"},{"location":"api/validation/#dataset_gen.validation.analysis.DatasetStats","title":"DatasetStats  <code>dataclass</code>","text":"<pre><code>DatasetStats(num_samples: int, num_profiles: int, num_rudiments: int, tempo_stats: DistributionStats, duration_stats: DistributionStats, num_strokes_stats: DistributionStats, timing_error_stats: DistributionStats, velocity_stats: DistributionStats, measure_timing_stats: DistributionStats, measure_velocity_stats: DistributionStats, exercise_timing_accuracy: DistributionStats, exercise_hand_balance: DistributionStats, skill_tier_counts: dict[str, int], rudiment_counts: dict[str, int], split_counts: dict[str, int])\n</code></pre> <p>Complete statistics for a dataset.</p>"},{"location":"api/validation/#dataset_gen.validation.analysis.DatasetStats.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; dict\n</code></pre> <p>Convert to dictionary.</p> Source code in <code>dataset_gen/validation/analysis.py</code> <pre><code>def to_dict(self) -&gt; dict:\n    \"\"\"Convert to dictionary.\"\"\"\n    return {\n        \"num_samples\": self.num_samples,\n        \"num_profiles\": self.num_profiles,\n        \"num_rudiments\": self.num_rudiments,\n        \"tempo\": self.tempo_stats.to_dict(),\n        \"duration\": self.duration_stats.to_dict(),\n        \"num_strokes\": self.num_strokes_stats.to_dict(),\n        \"timing_error\": self.timing_error_stats.to_dict(),\n        \"velocity\": self.velocity_stats.to_dict(),\n        \"measure_timing\": self.measure_timing_stats.to_dict(),\n        \"measure_velocity\": self.measure_velocity_stats.to_dict(),\n        \"exercise_timing_accuracy\": self.exercise_timing_accuracy.to_dict(),\n        \"exercise_hand_balance\": self.exercise_hand_balance.to_dict(),\n        \"skill_tier_counts\": self.skill_tier_counts,\n        \"rudiment_counts\": self.rudiment_counts,\n        \"split_counts\": self.split_counts,\n    }\n</code></pre>"},{"location":"api/validation/#dataset_gen.validation.analysis.DatasetAnalyzer","title":"DatasetAnalyzer","text":"<pre><code>DatasetAnalyzer(dataset_dir: Path | str)\n</code></pre> <p>Analyze generated dataset statistics.</p> <p>Provides comprehensive analysis of distributions across all hierarchical levels (stroke, measure, exercise, sample).</p> <p>Initialize analyzer.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_dir</code> <code>Path | str</code> <p>Path to dataset directory</p> required Source code in <code>dataset_gen/validation/analysis.py</code> <pre><code>def __init__(self, dataset_dir: Path | str):\n    \"\"\"\n    Initialize analyzer.\n\n    Args:\n        dataset_dir: Path to dataset directory\n    \"\"\"\n    self.dataset_dir = Path(dataset_dir)\n    self.reader = ParquetReader(dataset_dir)\n\n    # Cached data\n    self._samples_df: pd.DataFrame | None = None\n    self._strokes_df: pd.DataFrame | None = None\n    self._measures_df: pd.DataFrame | None = None\n    self._exercises_df: pd.DataFrame | None = None\n</code></pre>"},{"location":"api/validation/#dataset_gen.validation.analysis.DatasetAnalyzer.load_data","title":"load_data","text":"<pre><code>load_data() -&gt; None\n</code></pre> <p>Load all parquet data into memory.</p> Source code in <code>dataset_gen/validation/analysis.py</code> <pre><code>def load_data(self) -&gt; None:\n    \"\"\"Load all parquet data into memory.\"\"\"\n    logger.info(\"Loading dataset...\")\n    self._samples_df = self.reader.load_samples()\n    self._strokes_df = self.reader.load_strokes()\n    self._measures_df = self.reader.load_measures()\n    self._exercises_df = self.reader.load_exercises()\n    logger.info(f\"Loaded {len(self._samples_df)} samples\")\n</code></pre>"},{"location":"api/validation/#dataset_gen.validation.analysis.DatasetAnalyzer.compute_stats","title":"compute_stats","text":"<pre><code>compute_stats() -&gt; DatasetStats\n</code></pre> <p>Compute comprehensive dataset statistics.</p> Source code in <code>dataset_gen/validation/analysis.py</code> <pre><code>def compute_stats(self) -&gt; DatasetStats:\n    \"\"\"Compute comprehensive dataset statistics.\"\"\"\n    samples = self.samples\n    strokes = self.strokes\n    measures = self.measures\n    exercises = self.exercises\n\n    # Basic counts\n    num_samples = len(samples)\n    num_profiles = samples[\"profile_id\"].nunique()\n    num_rudiments = samples[\"rudiment_slug\"].nunique()\n\n    # Sample-level stats\n    tempo_stats = DistributionStats.from_array(\n        \"tempo_bpm\",\n        samples[\"tempo_bpm\"].values,\n    )\n    duration_stats = DistributionStats.from_array(\n        \"duration_sec\",\n        samples[\"duration_sec\"].values,\n    )\n    num_strokes_stats = DistributionStats.from_array(\n        \"num_strokes\",\n        samples[\"num_strokes\"].values,\n    )\n\n    # Stroke-level stats\n    timing_error_stats = DistributionStats.from_array(\n        \"timing_error_ms\",\n        strokes[\"timing_error_ms\"].values,\n    )\n    velocity_stats = DistributionStats.from_array(\n        \"actual_velocity\",\n        strokes[\"actual_velocity\"].values,\n    )\n\n    # Add by-skill-tier breakdown for timing errors\n    timing_by_tier = self._compute_stroke_stats_by_tier(strokes, samples, \"timing_error_ms\")\n    timing_error_stats.by_group = timing_by_tier\n\n    # Measure-level stats\n    measure_timing_stats = DistributionStats.from_array(\n        \"timing_mean_error_ms\",\n        measures[\"timing_mean_error_ms\"].values,\n    )\n    measure_velocity_stats = DistributionStats.from_array(\n        \"velocity_consistency\",\n        measures[\"velocity_consistency\"].values,\n    )\n\n    # Exercise-level stats\n    exercise_timing_accuracy = DistributionStats.from_array(\n        \"timing_accuracy\",\n        exercises[\"timing_accuracy\"].values,\n    )\n    exercise_hand_balance = DistributionStats.from_array(\n        \"hand_balance\",\n        exercises[\"hand_balance\"].values,\n    )\n\n    # Add by-skill-tier breakdown\n    exercise_timing_by_tier = self._compute_exercise_stats_by_tier(\n        exercises, samples, \"timing_accuracy\"\n    )\n    exercise_timing_accuracy.by_group = exercise_timing_by_tier\n\n    exercise_balance_by_tier = self._compute_exercise_stats_by_tier(\n        exercises, samples, \"hand_balance\"\n    )\n    exercise_hand_balance.by_group = exercise_balance_by_tier\n\n    # Categorical counts\n    skill_tier_counts = samples[\"skill_tier\"].value_counts().to_dict()\n    rudiment_counts = samples[\"rudiment_slug\"].value_counts().to_dict()\n\n    # Split counts (if available from splits.json)\n    split_counts = self._get_split_counts()\n\n    return DatasetStats(\n        num_samples=num_samples,\n        num_profiles=num_profiles,\n        num_rudiments=num_rudiments,\n        tempo_stats=tempo_stats,\n        duration_stats=duration_stats,\n        num_strokes_stats=num_strokes_stats,\n        timing_error_stats=timing_error_stats,\n        velocity_stats=velocity_stats,\n        measure_timing_stats=measure_timing_stats,\n        measure_velocity_stats=measure_velocity_stats,\n        exercise_timing_accuracy=exercise_timing_accuracy,\n        exercise_hand_balance=exercise_hand_balance,\n        skill_tier_counts=skill_tier_counts,\n        rudiment_counts=rudiment_counts,\n        split_counts=split_counts,\n    )\n</code></pre>"},{"location":"api/validation/#dataset_gen.validation.analysis.DatasetAnalyzer.check_skill_tier_ordering","title":"check_skill_tier_ordering","text":"<pre><code>check_skill_tier_ordering() -&gt; dict[str, bool]\n</code></pre> <p>Verify that skill tiers show expected ordering.</p> <p>Higher skill should correlate with: - Lower timing errors - Higher timing accuracy scores - Better hand balance</p> Source code in <code>dataset_gen/validation/analysis.py</code> <pre><code>def check_skill_tier_ordering(self) -&gt; dict[str, bool]:\n    \"\"\"\n    Verify that skill tiers show expected ordering.\n\n    Higher skill should correlate with:\n    - Lower timing errors\n    - Higher timing accuracy scores\n    - Better hand balance\n    \"\"\"\n    exercises = self.exercises\n    samples = self.samples\n\n    merged = exercises.merge(\n        samples[[\"sample_id\", \"skill_tier\"]],\n        on=\"sample_id\",\n    )\n\n    # Expected order (best to worst for timing accuracy)\n    tier_order = [\"professional\", \"advanced\", \"intermediate\", \"beginner\"]\n\n    results = {}\n\n    # Check timing accuracy ordering\n    tier_means = {}\n    for tier in tier_order:\n        tier_data = merged[merged[\"skill_tier\"] == tier][\"timing_accuracy\"]\n        if len(tier_data) &gt; 0:\n            tier_means[tier] = tier_data.mean()\n\n    if len(tier_means) &gt;= 2:\n        # Check if professional &gt; advanced &gt; intermediate &gt; beginner\n        available_tiers = [t for t in tier_order if t in tier_means]\n        is_ordered = all(\n            tier_means[available_tiers[i]] &gt;= tier_means[available_tiers[i + 1]]\n            for i in range(len(available_tiers) - 1)\n        )\n        results[\"timing_accuracy_ordered\"] = is_ordered\n        results[\"timing_accuracy_means\"] = tier_means\n\n    # Check hand balance ordering\n    tier_means = {}\n    for tier in tier_order:\n        tier_data = merged[merged[\"skill_tier\"] == tier][\"hand_balance\"]\n        if len(tier_data) &gt; 0:\n            tier_means[tier] = tier_data.mean()\n\n    if len(tier_means) &gt;= 2:\n        available_tiers = [t for t in tier_order if t in tier_means]\n        is_ordered = all(\n            tier_means[available_tiers[i]] &gt;= tier_means[available_tiers[i + 1]]\n            for i in range(len(available_tiers) - 1)\n        )\n        results[\"hand_balance_ordered\"] = is_ordered\n        results[\"hand_balance_means\"] = tier_means\n\n    return results\n</code></pre>"},{"location":"api/validation/#dataset_gen.validation.analysis.DatasetAnalyzer.compute_correlation_matrix","title":"compute_correlation_matrix","text":"<pre><code>compute_correlation_matrix(columns: list[str] | None = None) -&gt; DataFrame\n</code></pre> <p>Compute correlation matrix for exercise scores.</p> <p>Parameters:</p> Name Type Description Default <code>columns</code> <code>list[str] | None</code> <p>Columns to include (default: all numeric)</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Correlation matrix as DataFrame</p> Source code in <code>dataset_gen/validation/analysis.py</code> <pre><code>def compute_correlation_matrix(\n    self,\n    columns: list[str] | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Compute correlation matrix for exercise scores.\n\n    Args:\n        columns: Columns to include (default: all numeric)\n\n    Returns:\n        Correlation matrix as DataFrame\n    \"\"\"\n    exercises = self.exercises\n\n    if columns is None:\n        # Use all numeric columns except sample_id\n        numeric_cols = exercises.select_dtypes(include=[np.number]).columns\n        columns = [c for c in numeric_cols if c != \"sample_id\"]\n\n    return exercises[columns].corr()\n</code></pre>"},{"location":"api/validation/#dataset_gen.validation.analysis.analyze_dataset","title":"analyze_dataset","text":"<pre><code>analyze_dataset(dataset_dir: Path | str) -&gt; DatasetStats\n</code></pre> <p>Convenience function to analyze a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_dir</code> <code>Path | str</code> <p>Path to dataset directory</p> required <p>Returns:</p> Type Description <code>DatasetStats</code> <p>DatasetStats with comprehensive statistics</p> Source code in <code>dataset_gen/validation/analysis.py</code> <pre><code>def analyze_dataset(dataset_dir: Path | str) -&gt; DatasetStats:\n    \"\"\"\n    Convenience function to analyze a dataset.\n\n    Args:\n        dataset_dir: Path to dataset directory\n\n    Returns:\n        DatasetStats with comprehensive statistics\n    \"\"\"\n    analyzer = DatasetAnalyzer(dataset_dir)\n    return analyzer.compute_stats()\n</code></pre>"},{"location":"contributing/","title":"Contributing to SOUSA","text":"<p>Thank you for your interest in contributing to SOUSA (Synthetic Open Unified Snare Assessment)! This guide covers everything you need to get started.</p>"},{"location":"contributing/#ways-to-contribute","title":"Ways to Contribute","text":"<ul> <li>Report bugs - Found an issue? Open an issue</li> <li>Suggest features - Have an idea? We'd love to hear it</li> <li>Submit pull requests - Fix bugs, add features, improve docs</li> <li>Add rudiments - Expand the rudiment library</li> <li>Share examples - Show how you're using SOUSA</li> </ul>"},{"location":"contributing/#reporting-issues","title":"Reporting Issues","text":"<p>When reporting bugs, please include:</p> <ul> <li>A clear description of the problem</li> <li>Steps to reproduce the issue</li> <li>Your environment (Python version, OS, package versions)</li> <li>Any relevant error messages or logs</li> </ul> <p><pre><code>**Environment:**\n- OS: macOS 14.0\n- Python: 3.11.5\n- SOUSA version: 0.2.0\n- FluidSynth version: 2.3.4\n\n**Steps to reproduce:**\n1. Run `python scripts/generate_dataset.py --preset small`\n2. ...\n\n**Error message:**\n</code></pre> <pre><code>\n</code></pre>"},{"location":"contributing/#development-setup","title":"Development Setup","text":""},{"location":"contributing/#1-clone-the-repository","title":"1. Clone the Repository","text":"<pre><code>git clone https://github.com/zakkeown/SOUSA.git\ncd SOUSA\n</code></pre>"},{"location":"contributing/#2-create-a-virtual-environment","title":"2. Create a Virtual Environment","text":"macOS/LinuxWindows <pre><code>python -m venv .venv\nsource .venv/bin/activate\n</code></pre> <pre><code>python -m venv .venv\n.venv\\Scripts\\activate\n</code></pre>"},{"location":"contributing/#3-install-with-development-dependencies","title":"3. Install with Development Dependencies","text":"<pre><code>pip install -e \".[dev]\"\n</code></pre> <p>This installs:</p> <ul> <li>Core SOUSA package in editable mode</li> <li><code>pytest</code> for testing</li> <li><code>black</code> for code formatting</li> <li><code>ruff</code> for linting</li> <li><code>pre-commit</code> for git hooks</li> </ul>"},{"location":"contributing/#4-install-pre-commit-hooks","title":"4. Install Pre-commit Hooks","text":"<p>Required</p> <p>Pre-commit hooks are required. CI will fail if code doesn't pass the hooks.</p> <pre><code>pip install pre-commit\npre-commit install\n</code></pre> <p>Or use the Makefile:</p> <pre><code>make setup-hooks\n</code></pre>"},{"location":"contributing/#5-install-fluidsynth","title":"5. Install FluidSynth","text":"<p>FluidSynth is required for audio synthesis.</p> macOSUbuntu/DebianWindows <pre><code>brew install fluid-synth\n</code></pre> <pre><code>sudo apt-get update\nsudo apt-get install fluidsynth libfluidsynth-dev\n</code></pre> <p>Download from FluidSynth releases and add to PATH.</p> <p>Verify installation:</p> <pre><code>fluidsynth --version\n</code></pre>"},{"location":"contributing/#6-download-soundfonts","title":"6. Download Soundfonts","text":"<pre><code>python scripts/setup_soundfonts.py\n</code></pre> <p>This downloads the required SF2 soundfont files to <code>data/soundfonts/</code>.</p>"},{"location":"contributing/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests\npytest\n\n# Run specific test file\npytest tests/test_profiles.py\n\n# Run specific test\npytest tests/test_profiles.py::TestProfileGeneration::test_generate_profile_beginner\n\n# Run with verbose output\npytest -v\n\n# Run with coverage\npytest --cov=dataset_gen\n\n# Run tests matching a pattern\npytest -k \"profile\"\n</code></pre>"},{"location":"contributing/#pre-commit-hooks","title":"Pre-commit Hooks","text":"<p>The project uses pre-commit to run automated checks before each commit.</p>"},{"location":"contributing/#configured-hooks","title":"Configured Hooks","text":"Hook Purpose <code>trailing-whitespace</code> Removes trailing whitespace <code>end-of-file-fixer</code> Ensures files end with newline <code>check-yaml</code> Validates YAML syntax <code>check-toml</code> Validates TOML syntax <code>black</code> Formats Python code <code>ruff</code> Lints Python code with auto-fix"},{"location":"contributing/#running-hooks-manually","title":"Running Hooks Manually","text":"<pre><code># Run on all files\npre-commit run --all-files\n\n# Or use the Makefile\nmake pre-commit\n\n# Run specific hook\npre-commit run black --all-files\n</code></pre>"},{"location":"contributing/#skipping-hooks-emergency-only","title":"Skipping Hooks (Emergency Only)","text":"<pre><code>git commit --no-verify -m \"Emergency fix\"\n</code></pre> <p>Use Sparingly</p> <p>CI will still run the checks. Only skip hooks in genuine emergencies.</p>"},{"location":"contributing/#code-style","title":"Code Style","text":""},{"location":"contributing/#formatting-with-black","title":"Formatting with Black","text":"<p>We use Black for consistent formatting:</p> <pre><code># Format all code\nblack dataset_gen/ tests/ scripts/\n\n# Check without modifying\nblack --check dataset_gen/ tests/ scripts/\n\n# Or use Makefile\nmake format\n</code></pre> <p>Configuration in <code>pyproject.toml</code>:</p> <ul> <li>Line length: 100 characters</li> <li>Python target: 3.10+</li> </ul>"},{"location":"contributing/#linting-with-ruff","title":"Linting with Ruff","text":"<p>We use Ruff for fast linting:</p> <pre><code># Check for issues\nruff check dataset_gen/ tests/ scripts/\n\n# Auto-fix issues\nruff check --fix dataset_gen/ tests/ scripts/\n\n# Or use Makefile\nmake lint\n</code></pre>"},{"location":"contributing/#type-hints","title":"Type Hints","text":"<p>We encourage type hints for public APIs:</p> <pre><code>def generate_profile(\n    skill_tier: str,\n    seed: int | None = None,\n) -&gt; PlayerProfile:\n    \"\"\"Generate a player profile.\n\n    Args:\n        skill_tier: One of \"beginner\", \"intermediate\", \"advanced\", \"professional\"\n        seed: Random seed for reproducibility\n\n    Returns:\n        Generated PlayerProfile instance\n    \"\"\"\n    ...\n</code></pre>"},{"location":"contributing/#docstrings","title":"Docstrings","text":"<p>Use Google-style docstrings:</p> <pre><code>def compute_timing_score(\n    errors_ms: list[float],\n    threshold_ms: float = 25.0,\n) -&gt; float:\n    \"\"\"Compute timing accuracy score from error values.\n\n    Uses sigmoid scaling to map errors to a 0-100 score range.\n\n    Args:\n        errors_ms: List of timing errors in milliseconds\n        threshold_ms: Error value mapping to score of 50\n\n    Returns:\n        Timing accuracy score (0-100)\n\n    Raises:\n        ValueError: If errors_ms is empty\n\n    Example:\n        &gt;&gt;&gt; compute_timing_score([5, 10, 15])\n        85.7\n    \"\"\"\n</code></pre>"},{"location":"contributing/#pull-request-process","title":"Pull Request Process","text":""},{"location":"contributing/#1-create-a-branch","title":"1. Create a Branch","text":"<pre><code>git checkout -b feature/your-feature-name\n# or\ngit checkout -b fix/your-bug-fix\n</code></pre>"},{"location":"contributing/#2-make-your-changes","title":"2. Make Your Changes","text":"<ul> <li>Write clear, descriptive commits</li> <li>Add tests for new functionality</li> <li>Update documentation if needed</li> </ul>"},{"location":"contributing/#3-ensure-quality","title":"3. Ensure Quality","text":"<pre><code># Format code\nblack dataset_gen/ tests/ scripts/\n\n# Run linter\nruff check dataset_gen/ tests/ scripts/\n\n# Run tests\npytest\n\n# Run pre-commit\npre-commit run --all-files\n</code></pre>"},{"location":"contributing/#4-submit-pull-request","title":"4. Submit Pull Request","text":"<ul> <li>Provide a clear description of changes</li> <li>Reference any related issues</li> <li>Include test results if relevant</li> </ul>"},{"location":"contributing/#pr-title-format","title":"PR Title Format","text":"<pre><code>feat: Add new flam quality metric\nfix: Handle empty stroke lists in scoring\ndocs: Update PyTorch DataLoader example\nrefactor: Simplify MIDI generation logic\ntest: Add tests for edge cases in profiles\n</code></pre>"},{"location":"contributing/#adding-new-rudiments","title":"Adding New Rudiments","text":"<p>Rudiment definitions are YAML files in <code>dataset_gen/rudiments/definitions/</code>.</p>"},{"location":"contributing/#1-create-yaml-definition","title":"1. Create YAML Definition","text":"<pre><code># dataset_gen/rudiments/definitions/your_rudiment.yaml\nname: Your Rudiment Name\nslug: your_rudiment_slug\ncategory: single-stroke  # or double-stroke, diddle, flam, drag, roll\ntempo_range:\n  min: 60\n  max: 180\ntime_signature: \"4/4\"\nsubdivision: 16  # 16th notes\n\npatterns:\n  - name: basic\n    sticking: \"RLRL RLRL\"\n    articulations: \"taaa taaa\"\n    measures: 2\n\n# Optional articulation map\narticulation_map:\n  t: tap\n  a: accent\n  g: grace\n  d: diddle\n</code></pre>"},{"location":"contributing/#2-add-tests","title":"2. Add Tests","text":"<pre><code># tests/test_rudiments.py\n\ndef test_your_rudiment_loads():\n    rudiment = load_rudiment(\"your_rudiment_slug\")\n    assert rudiment is not None\n    assert rudiment.slug == \"your_rudiment_slug\"\n\n\ndef test_your_rudiment_generation():\n    rudiment = load_rudiment(\"your_rudiment_slug\")\n    midi = generate_midi(rudiment, tempo=120)\n    assert len(midi.notes) &gt; 0\n</code></pre>"},{"location":"contributing/#3-validate","title":"3. Validate","text":"<pre><code># Run rudiment validation\npython -m dataset_gen.validation.rudiments your_rudiment_slug\n\n# Run tests\npytest tests/test_rudiments.py -v\n</code></pre>"},{"location":"contributing/#project-structure","title":"Project Structure","text":"<pre><code>SOUSA/\n\u251c\u2500\u2500 dataset_gen/           # Core generation modules\n\u2502   \u251c\u2500\u2500 rudiments/         # Rudiment definitions (YAML)\n\u2502   \u2502   \u2514\u2500\u2500 definitions/   # 40 rudiment YAML files\n\u2502   \u251c\u2500\u2500 profiles/          # Player skill modeling\n\u2502   \u251c\u2500\u2500 midi_gen/          # MIDI generation engine\n\u2502   \u251c\u2500\u2500 audio_synth/       # FluidSynth wrapper\n\u2502   \u251c\u2500\u2500 audio_aug/         # Augmentation pipeline\n\u2502   \u251c\u2500\u2500 labels/            # Label computation\n\u2502   \u251c\u2500\u2500 pipeline/          # Orchestration\n\u2502   \u2514\u2500\u2500 validation/        # Dataset validation\n\u251c\u2500\u2500 tests/                 # Test suite\n\u251c\u2500\u2500 scripts/               # CLI scripts\n\u251c\u2500\u2500 examples/              # Usage examples\n\u251c\u2500\u2500 docs/                  # Documentation source\n\u251c\u2500\u2500 data/                  # Soundfonts, IRs, noise profiles\n\u2514\u2500\u2500 output/                # Generated datasets\n</code></pre>"},{"location":"contributing/#getting-help","title":"Getting Help","text":"<ul> <li>Questions: Open an issue with the \"question\" label</li> <li>Discussions: Use GitHub Discussions for general topics</li> <li>Documentation: Check the docs</li> </ul>"},{"location":"contributing/#code-of-conduct","title":"Code of Conduct","text":"<p>Please be respectful and constructive in all interactions. We're building this together!</p>"},{"location":"contributing/#license","title":"License","text":"<p>By contributing to SOUSA, you agree that your contributions will be licensed under the same license as the project (MIT).</p>"},{"location":"examples/","title":"Examples","text":"<p>This section provides practical examples for working with the SOUSA dataset in machine learning workflows. Each example includes complete, runnable code with detailed explanations.</p>"},{"location":"examples/#overview","title":"Overview","text":"Example Description Key Concepts PyTorch DataLoader Load SOUSA into PyTorch training pipelines <code>Dataset</code>, <code>DataLoader</code>, collate functions, batching Feature Extraction Extract ML features from audio Mel spectrograms, MFCCs, Wav2Vec2, caching Filtering Samples Filter dataset by various criteria Skill tiers, rudiments, scores, augmentation Hierarchical Labels Work with stroke/measure/exercise labels Multi-level targets, sequence modeling"},{"location":"examples/#prerequisites","title":"Prerequisites","text":"<p>All examples assume you have:</p> <ol> <li> <p>Installed SOUSA with development dependencies:</p> <pre><code>pip install -e \".[dev]\"\n</code></pre> </li> <li> <p>Generated a dataset (at minimum, the small preset):</p> <pre><code>python scripts/generate_dataset.py --preset small --with-audio\n</code></pre> </li> <li> <p>Optional ML dependencies for specific examples:</p> PyTorchFeature Extraction <pre><code>pip install torch torchaudio\n</code></pre> <pre><code>pip install librosa transformers\n</code></pre> </li> </ol>"},{"location":"examples/#running-the-examples","title":"Running the Examples","text":"<p>Each example can be run directly from the command line:</p> <pre><code># PyTorch DataLoader example\npython -m examples.pytorch_dataloader output/dataset\n\n# Feature extraction example\npython -m examples.feature_extraction\n\n# Filtering example\npython -m examples.filtering output/dataset\n\n# Hierarchical labels example\npython -m examples.hierarchical_labels output/dataset\n</code></pre>"},{"location":"examples/#quick-start","title":"Quick Start","text":"<p>Here's a minimal example to get you started:</p> <pre><code>import pandas as pd\nfrom pathlib import Path\n\n# Load dataset metadata\ndata_dir = Path(\"output/dataset\")\nsamples = pd.read_parquet(data_dir / \"labels\" / \"samples.parquet\")\nexercises = pd.read_parquet(data_dir / \"labels\" / \"exercises.parquet\")\n\n# Merge for easy access\ndata = samples.merge(exercises, on=\"sample_id\")\n\n# Filter to advanced players playing paradiddles\nsubset = data[\n    (data[\"skill_tier\"] == \"advanced\") &amp;\n    (data[\"rudiment_slug\"].str.contains(\"paradiddle\"))\n]\n\nprint(f\"Found {len(subset)} samples\")\nprint(f\"Mean overall score: {subset['overall_score'].mean():.1f}\")\n</code></pre>"},{"location":"examples/#source-code","title":"Source Code","text":"<p>All example source files are available in the <code>examples/</code> directory:</p> <ul> <li><code>pytorch_dataloader.py</code> - Complete PyTorch integration</li> <li><code>feature_extraction.py</code> - Feature extraction utilities</li> <li><code>filtering.py</code> - Filtering functions and examples</li> <li><code>hierarchical_labels.py</code> - Hierarchical label access</li> </ul>"},{"location":"examples/#contributing-examples","title":"Contributing Examples","text":"<p>Have an interesting use case? We welcome example contributions! See the Contributing Guide for how to submit new examples.</p> <p>Example Requirements</p> <p>Good examples should:</p> <ul> <li>Be self-contained and runnable</li> <li>Include docstrings and comments</li> <li>Handle missing optional dependencies gracefully</li> <li>Work with the <code>--preset small</code> dataset for quick testing</li> </ul>"},{"location":"examples/feature-extraction/","title":"Feature Extraction","text":"<p>This guide covers extracting machine learning features from SOUSA audio files. We provide utilities for mel spectrograms, onset detection, RMS energy, and pretrained model features (Wav2Vec2, HuBERT).</p> <p>Source Code</p> <p>Complete implementation: <code>examples/feature_extraction.py</code></p>"},{"location":"examples/feature-extraction/#prerequisites","title":"Prerequisites","text":"Basic (Mel/RMS)Onset DetectionPretrained Models <pre><code>pip install torch torchaudio\n# or\npip install librosa\n</code></pre> <pre><code>pip install librosa\n</code></pre> <pre><code>pip install torch transformers\n</code></pre>"},{"location":"examples/feature-extraction/#quick-start","title":"Quick Start","text":"<pre><code>from examples.feature_extraction import FeatureExtractor\n\n# Create unified extractor\nextractor = FeatureExtractor(\n    sample_rate=16000,\n    feature_type=\"mel_spectrogram\",\n)\n\n# Extract from file\nfeatures = extractor.extract(\"output/dataset/audio/sample.flac\")\n\n# Or from array\nimport numpy as np\naudio = np.random.randn(16000 * 3)  # 3 seconds at 16kHz\nfeatures = extractor.extract_from_array(audio)\n\nprint(features[\"mel_spectrogram\"].shape)  # (128, 188) - (n_mels, time_frames)\n</code></pre>"},{"location":"examples/feature-extraction/#mel-spectrogram-extraction","title":"Mel Spectrogram Extraction","text":"<p>Mel spectrograms are the most common input representation for audio neural networks.</p>"},{"location":"examples/feature-extraction/#basic-usage","title":"Basic Usage","text":"<pre><code>from examples.feature_extraction import MelSpectrogramExtractor\n\nextractor = MelSpectrogramExtractor(\n    sample_rate=16000,\n    n_mels=128,\n    n_fft=1024,\n    hop_length=256,\n    f_min=20.0,\n    f_max=8000.0,\n    normalized=True,\n)\n\n# Extract from audio array\nmel = extractor(audio_array)\nprint(mel.shape)  # (128, time_frames)\n</code></pre>"},{"location":"examples/feature-extraction/#parameters","title":"Parameters","text":"Parameter Default Description <code>sample_rate</code> <code>16000</code> Audio sample rate <code>n_mels</code> <code>128</code> Number of mel frequency bands <code>n_fft</code> <code>1024</code> FFT window size <code>hop_length</code> <code>256</code> Hop length between frames <code>f_min</code> <code>20.0</code> Minimum frequency (Hz) <code>f_max</code> <code>8000.0</code> Maximum frequency (Hz) <code>power</code> <code>2.0</code> Spectrogram power (2=power, 1=magnitude) <code>normalized</code> <code>True</code> Per-channel normalization <code>use_torchaudio</code> <code>True</code> Use torchaudio (faster) or librosa"},{"location":"examples/feature-extraction/#recommended-settings-for-drums","title":"Recommended Settings for Drums","text":"<pre><code># Settings optimized for drum/percussion audio\nextractor = MelSpectrogramExtractor(\n    sample_rate=16000,\n    n_mels=128,\n    n_fft=1024,       # ~64ms window at 16kHz\n    hop_length=256,   # ~16ms hop = 62.5 fps\n    f_min=20.0,       # Capture low-frequency body\n    f_max=8000.0,     # Drums don't need higher frequencies\n)\n</code></pre> <p>Frame Rate Calculation</p> <p>Frame rate = sample_rate / hop_length</p> <p>With default settings: 16000 / 256 = 62.5 frames per second</p>"},{"location":"examples/feature-extraction/#mfcc-features","title":"MFCC Features","text":"<p>For traditional ML models, MFCCs provide compact representations:</p> <pre><code>import librosa\n\ndef extract_mfcc(audio: np.ndarray, sample_rate: int = 16000, n_mfcc: int = 13):\n    \"\"\"Extract MFCC features.\"\"\"\n    mfcc = librosa.feature.mfcc(\n        y=audio,\n        sr=sample_rate,\n        n_mfcc=n_mfcc,\n        n_fft=1024,\n        hop_length=256,\n    )\n\n    # Add delta and delta-delta features\n    mfcc_delta = librosa.feature.delta(mfcc)\n    mfcc_delta2 = librosa.feature.delta(mfcc, order=2)\n\n    # Stack: (39, time_frames) = 13 MFCCs + 13 deltas + 13 delta-deltas\n    return np.vstack([mfcc, mfcc_delta, mfcc_delta2])\n</code></pre>"},{"location":"examples/feature-extraction/#onset-detection-features","title":"Onset Detection Features","text":"<p>Onset detection is particularly relevant for drum performance analysis:</p> <pre><code>from examples.feature_extraction import OnsetDetector\n\ndetector = OnsetDetector(sample_rate=16000, hop_length=128)\n\n# Detect onsets\nresult = detector.detect_onsets(audio_array)\n\nprint(f\"Number of onsets: {result['num_onsets']}\")\nprint(f\"Onset times (seconds): {result['onset_times'][:5]}\")\nprint(f\"Onset strength shape: {result['onset_strength'].shape}\")\n\n# Compute inter-onset interval features\nioi_features = detector.compute_ioi_features(result[\"onset_times\"])\n\nprint(f\"Mean IOI: {ioi_features['ioi_mean']:.3f}s\")\nprint(f\"IOI std: {ioi_features['ioi_std']:.3f}s\")\nprint(f\"Estimated tempo: {ioi_features['tempo_estimate']:.1f} BPM\")\n</code></pre>"},{"location":"examples/feature-extraction/#ioi-features-for-timing-analysis","title":"IOI Features for Timing Analysis","text":"<p>Inter-onset intervals (IOIs) are useful for analyzing timing consistency:</p> Feature Description <code>ioi_mean</code> Mean time between strokes (seconds) <code>ioi_std</code> Standard deviation of IOIs <code>ioi_cv</code> Coefficient of variation (std/mean) - timing consistency <code>tempo_estimate</code> Estimated tempo from mean IOI"},{"location":"examples/feature-extraction/#rms-energy-features","title":"RMS Energy Features","text":"<p>RMS (Root Mean Square) energy captures dynamics:</p> <pre><code>from examples.feature_extraction import RMSExtractor\n\nextractor = RMSExtractor(frame_length=2048, hop_length=512)\n\n# Extract RMS curve\nrms = extractor(audio_array)\n\n# Compute dynamics statistics\ndynamics = extractor.compute_dynamics_features(rms)\n\nprint(f\"Mean RMS: {dynamics['rms_mean']:.4f}\")\nprint(f\"Dynamic range: {dynamics['dynamic_range']:.4f}\")\nprint(f\"RMS CV: {dynamics['rms_cv']:.4f}\")  # Dynamics consistency\n</code></pre>"},{"location":"examples/feature-extraction/#dynamics-features","title":"Dynamics Features","text":"Feature Description <code>rms_mean</code> Average loudness <code>rms_std</code> Loudness variation <code>rms_max</code> Peak loudness <code>rms_min</code> Minimum loudness <code>dynamic_range</code> Max - min RMS <code>rms_cv</code> Coefficient of variation"},{"location":"examples/feature-extraction/#pretrained-model-features","title":"Pretrained Model Features","text":""},{"location":"examples/feature-extraction/#wav2vec2","title":"Wav2Vec2","text":"<p>Wav2Vec2 provides powerful self-supervised features:</p> <pre><code>from examples.feature_extraction import Wav2Vec2Extractor\n\n# Load pretrained model\nextractor = Wav2Vec2Extractor(\n    model_name=\"facebook/wav2vec2-base\",\n    layer=-1,      # Last layer (-1) or specific layer (0-11)\n    device=\"cuda\",\n    freeze=True,\n)\n\n# Extract sequence features\nfeatures = extractor(audio_array)\nprint(features.shape)  # (time_frames, 768)\n\n# Extract pooled (single vector) features\npooled = extractor.extract_pooled(audio_array, pooling=\"mean\")\nprint(pooled.shape)  # (768,)\n</code></pre>"},{"location":"examples/feature-extraction/#hubert","title":"HuBERT","text":"<p>HuBERT works similarly (uses same API):</p> <pre><code>extractor = Wav2Vec2Extractor(\n    model_name=\"facebook/hubert-base-ls960\",\n    device=\"cuda\",\n)\n\nfeatures = extractor(audio_array)\n</code></pre>"},{"location":"examples/feature-extraction/#available-pooling-strategies","title":"Available Pooling Strategies","text":"<pre><code># Mean pooling (recommended for most tasks)\npooled = extractor.extract_pooled(audio, pooling=\"mean\")\n\n# Max pooling\npooled = extractor.extract_pooled(audio, pooling=\"max\")\n\n# First token (CLS-like)\npooled = extractor.extract_pooled(audio, pooling=\"first\")\n\n# Last token\npooled = extractor.extract_pooled(audio, pooling=\"last\")\n</code></pre>"},{"location":"examples/feature-extraction/#multi-layer-features","title":"Multi-Layer Features","text":"<p>For probing or concatenation:</p> <pre><code># Get features from all transformer layers\nall_layers = extractor(audio_array, return_all_layers=True)\n\nfor layer_name, features in all_layers.items():\n    print(f\"{layer_name}: {features.shape}\")\n\n# layer_0: (time_frames, 768)  - CNN output\n# layer_1: (time_frames, 768)  - Transformer layer 1\n# ...\n# layer_12: (time_frames, 768) - Last transformer layer\n</code></pre>"},{"location":"examples/feature-extraction/#unified-feature-extractor","title":"Unified Feature Extractor","text":"<p>The <code>FeatureExtractor</code> class combines multiple extractors:</p> <pre><code>from examples.feature_extraction import FeatureExtractor\n\n# Extract all available features\nextractor = FeatureExtractor(\n    sample_rate=16000,\n    feature_type=\"all\",  # \"mel_spectrogram\", \"onset\", \"rms\", \"wav2vec2\", or \"all\"\n    cache_dir=\"feature_cache\",  # Optional caching\n)\n\nfeatures = extractor.extract(\"audio.flac\")\n\n# Features dict contains:\n# - mel_spectrogram: (n_mels, time_frames)\n# - onset_times: (num_onsets,)\n# - onset_strength: (time_frames,)\n# - num_onsets: int\n# - ioi_mean, ioi_std, ioi_cv, tempo_estimate: floats\n# - rms: (time_frames,)\n# - rms_mean, rms_std, rms_max, rms_min, dynamic_range, rms_cv: floats\n# - wav2vec2: (time_frames, 768)\n# - wav2vec2_pooled: (768,)\n</code></pre>"},{"location":"examples/feature-extraction/#feature-caching","title":"Feature Caching","text":"<p>For large-scale experiments, enable caching:</p> <pre><code>extractor = FeatureExtractor(\n    sample_rate=16000,\n    feature_type=\"mel_spectrogram\",\n    cache_dir=\"./feature_cache\",\n)\n\n# First call: computes and caches\nfeatures1 = extractor.extract(\"audio.flac\")\n\n# Second call: loads from cache (instant)\nfeatures2 = extractor.extract(\"audio.flac\")\n</code></pre> <p>Cache Invalidation</p> <p>The cache key includes the audio filename, feature type, and sample rate. If you change extractor parameters (e.g., <code>n_mels</code>), clear the cache:</p> <pre><code>rm -rf ./feature_cache\n</code></pre>"},{"location":"examples/feature-extraction/#batch-processing","title":"Batch Processing","text":"<p>Process multiple files in parallel:</p> <pre><code>from examples.feature_extraction import extract_batch_features\n\naudio_paths = [\n    \"output/dataset/audio/sample1.flac\",\n    \"output/dataset/audio/sample2.flac\",\n    \"output/dataset/audio/sample3.flac\",\n]\n\nfeatures_list = extract_batch_features(\n    audio_paths,\n    feature_type=\"mel_spectrogram\",\n    sample_rate=16000,\n    num_workers=4,\n    cache_dir=\"./feature_cache\",\n)\n\nfor path, features in zip(audio_paths, features_list):\n    print(f\"{path}: {features['mel_spectrogram'].shape}\")\n</code></pre> <p>Wav2Vec2 and Multiprocessing</p> <p>Wav2Vec2 extraction may not work well with multiprocessing due to CUDA/model loading issues. Use <code>num_workers=1</code> for pretrained features, or extract them separately.</p>"},{"location":"examples/feature-extraction/#integration-with-pytorch","title":"Integration with PyTorch","text":""},{"location":"examples/feature-extraction/#as-a-transform","title":"As a Transform","text":"<pre><code>import torch\nfrom examples.feature_extraction import MelSpectrogramExtractor\nfrom examples.pytorch_dataloader import SOUSADataset\n\n\nclass MelTransform:\n    \"\"\"Transform waveform to mel spectrogram.\"\"\"\n\n    def __init__(self, sample_rate: int = 16000):\n        self.extractor = MelSpectrogramExtractor(sample_rate=sample_rate)\n\n    def __call__(self, waveform: torch.Tensor) -&gt; torch.Tensor:\n        mel = self.extractor(waveform.numpy())\n        return torch.from_numpy(mel)\n\n\n# Use with dataset\ndataset = SOUSADataset(\n    data_dir=\"output/dataset\",\n    split=\"train\",\n    target=\"skill_tier\",\n    transform=MelTransform(),\n)\n\nsample = dataset[0]\nprint(sample[\"waveform\"].shape)  # Now a mel spectrogram!\n</code></pre>"},{"location":"examples/feature-extraction/#pre-computed-features-dataset","title":"Pre-computed Features Dataset","text":"<p>For faster training, pre-compute features:</p> <pre><code>import torch\nfrom torch.utils.data import Dataset\nimport pandas as pd\nfrom pathlib import Path\n\n\nclass PrecomputedFeaturesDataset(Dataset):\n    \"\"\"Dataset with pre-computed features.\"\"\"\n\n    def __init__(\n        self,\n        data_dir: str,\n        features_dir: str,\n        split: str = \"train\",\n    ):\n        self.features_dir = Path(features_dir)\n\n        # Load metadata\n        data_dir = Path(data_dir)\n        samples = pd.read_parquet(data_dir / \"labels\" / \"samples.parquet\")\n        exercises = pd.read_parquet(data_dir / \"labels\" / \"exercises.parquet\")\n        self.data = samples.merge(exercises, on=\"sample_id\")\n\n        # Filter by split (implement your own split logic)\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        row = self.data.iloc[idx]\n        sample_id = row[\"sample_id\"]\n\n        # Load pre-computed features\n        features = torch.load(self.features_dir / f\"{sample_id}.pt\")\n\n        return {\n            \"features\": features,\n            \"label\": row[\"overall_score\"] / 100.0,\n            \"sample_id\": sample_id,\n        }\n</code></pre>"},{"location":"examples/feature-extraction/#example-complete-feature-extraction-pipeline","title":"Example: Complete Feature Extraction Pipeline","text":"<pre><code>\"\"\"Extract features for all samples in the dataset.\"\"\"\nfrom pathlib import Path\nimport pandas as pd\nimport torch\nfrom tqdm import tqdm\n\nfrom examples.feature_extraction import FeatureExtractor\n\n\ndef extract_all_features(\n    data_dir: str,\n    output_dir: str,\n    feature_type: str = \"mel_spectrogram\",\n):\n    data_dir = Path(data_dir)\n    output_dir = Path(output_dir)\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    # Load sample list\n    samples = pd.read_parquet(data_dir / \"labels\" / \"samples.parquet\")\n\n    # Create extractor\n    extractor = FeatureExtractor(\n        sample_rate=16000,\n        feature_type=feature_type,\n    )\n\n    # Process all samples\n    for _, row in tqdm(samples.iterrows(), total=len(samples)):\n        sample_id = row[\"sample_id\"]\n        audio_path = data_dir / row[\"audio_path\"]\n\n        if not audio_path.exists():\n            continue\n\n        # Extract features\n        features = extractor.extract(audio_path)\n\n        # Save as tensor\n        torch.save(\n            {k: torch.from_numpy(v) if hasattr(v, \"shape\") else v\n             for k, v in features.items()},\n            output_dir / f\"{sample_id}.pt\"\n        )\n\n\nif __name__ == \"__main__\":\n    extract_all_features(\n        data_dir=\"output/dataset\",\n        output_dir=\"output/features/mel\",\n        feature_type=\"mel_spectrogram\",\n    )\n</code></pre>"},{"location":"examples/feature-extraction/#next-steps","title":"Next Steps","text":"<ul> <li>PyTorch DataLoader - Combine features with DataLoader</li> <li>Filtering Samples - Filter dataset before feature extraction</li> <li>Hierarchical Labels - Align features with stroke-level labels</li> </ul>"},{"location":"examples/filtering/","title":"Filtering Samples","text":"<p>This guide shows how to filter the SOUSA dataset by skill tier, rudiment type, tempo, score ranges, soundfont, and augmentation settings. Filtering helps create focused subsets for specific experiments or training scenarios.</p> <p>Source Code</p> <p>Complete implementation: <code>examples/filtering.py</code></p>"},{"location":"examples/filtering/#prerequisites","title":"Prerequisites","text":"<pre><code>pip install pandas pyarrow\n</code></pre>"},{"location":"examples/filtering/#loading-dataset-metadata","title":"Loading Dataset Metadata","text":"<p>All filtering operates on the parquet metadata files:</p> <pre><code>import pandas as pd\nfrom pathlib import Path\n\n\ndef load_metadata(data_dir: str) -&gt; pd.DataFrame:\n    \"\"\"Load and merge sample metadata.\"\"\"\n    data_dir = Path(data_dir)\n\n    samples = pd.read_parquet(data_dir / \"labels\" / \"samples.parquet\")\n    exercises = pd.read_parquet(data_dir / \"labels\" / \"exercises.parquet\")\n\n    return samples.merge(exercises, on=\"sample_id\", how=\"left\")\n\n\n# Load all metadata\ndf = load_metadata(\"output/dataset\")\nprint(f\"Total samples: {len(df)}\")\nprint(f\"Columns: {list(df.columns)}\")\n</code></pre>"},{"location":"examples/filtering/#filter-by-skill-tier","title":"Filter by Skill Tier","text":"<pre><code>def filter_by_skill_tier(df: pd.DataFrame, tier: str) -&gt; pd.DataFrame:\n    \"\"\"Filter to specific skill tier.\"\"\"\n    return df[df[\"skill_tier\"] == tier].reset_index(drop=True)\n\n\n# Get only advanced players\nadvanced_df = filter_by_skill_tier(df, \"advanced\")\nprint(f\"Advanced samples: {len(advanced_df)}\")\n\n# Available tiers\nSKILL_TIERS = [\"beginner\", \"intermediate\", \"advanced\", \"professional\"]\n</code></pre>"},{"location":"examples/filtering/#multiple-skill-tiers","title":"Multiple Skill Tiers","text":"<pre><code># Get beginner and intermediate (novice group)\nnovice_df = df[df[\"skill_tier\"].isin([\"beginner\", \"intermediate\"])]\n\n# Get advanced and professional (skilled group)\nskilled_df = df[df[\"skill_tier\"].isin([\"advanced\", \"professional\"])]\n</code></pre>"},{"location":"examples/filtering/#filter-by-rudiment","title":"Filter by Rudiment","text":"<pre><code>def filter_by_rudiment(df: pd.DataFrame, rudiment_slug: str) -&gt; pd.DataFrame:\n    \"\"\"Filter to specific rudiment.\"\"\"\n    return df[df[\"rudiment_slug\"] == rudiment_slug].reset_index(drop=True)\n\n\n# Get single paradiddle samples\nparadiddle_df = filter_by_rudiment(df, \"single_paradiddle\")\n\n# List all available rudiments\nrudiments = sorted(df[\"rudiment_slug\"].unique())\nprint(f\"Available rudiments ({len(rudiments)}):\")\nfor r in rudiments[:10]:\n    print(f\"  - {r}\")\n</code></pre>"},{"location":"examples/filtering/#filter-by-rudiment-category","title":"Filter by Rudiment Category","text":"<pre><code># Get all paradiddle variants\nparadiddle_variants = df[df[\"rudiment_slug\"].str.contains(\"paradiddle\")]\n\n# Get all roll types\nrolls = df[df[\"rudiment_slug\"].str.contains(\"roll\")]\n\n# Get all flam rudiments\nflams = df[df[\"rudiment_slug\"].str.contains(\"flam\")]\n</code></pre>"},{"location":"examples/filtering/#filter-by-tempo","title":"Filter by Tempo","text":"<pre><code>def filter_by_tempo_range(\n    df: pd.DataFrame,\n    min_tempo: int = 60,\n    max_tempo: int = 200,\n) -&gt; pd.DataFrame:\n    \"\"\"Filter to tempo range (BPM).\"\"\"\n    mask = (df[\"tempo_bpm\"] &gt;= min_tempo) &amp; (df[\"tempo_bpm\"] &lt;= max_tempo)\n    return df[mask].reset_index(drop=True)\n\n\n# Get slow tempos only (60-100 BPM)\nslow_df = filter_by_tempo_range(df, 60, 100)\n\n# Get fast tempos only (140-200 BPM)\nfast_df = filter_by_tempo_range(df, 140, 200)\n</code></pre>"},{"location":"examples/filtering/#filter-by-score","title":"Filter by Score","text":"<pre><code>def filter_by_score_range(\n    df: pd.DataFrame,\n    min_score: float = 0,\n    max_score: float = 100,\n    score_column: str = \"overall_score\",\n) -&gt; pd.DataFrame:\n    \"\"\"Filter to samples within score range.\"\"\"\n    mask = (df[score_column] &gt;= min_score) &amp; (df[score_column] &lt;= max_score)\n    return df[mask].reset_index(drop=True)\n\n\n# High-quality performances only\nhigh_quality_df = filter_by_score_range(df, min_score=80)\n\n# Low-quality performances (for contrastive learning)\nlow_quality_df = filter_by_score_range(df, max_score=40)\n\n# Filter by timing accuracy specifically\naccurate_df = filter_by_score_range(df, min_score=75, score_column=\"timing_accuracy\")\n</code></pre>"},{"location":"examples/filtering/#available-score-columns","title":"Available Score Columns","text":"Column Description Range <code>overall_score</code> Composite quality score 0-100 <code>timing_accuracy</code> How close to intended timing 0-100 <code>timing_consistency</code> Consistency of timing errors 0-100 <code>velocity_accuracy</code> How close to intended velocity 0-100 <code>velocity_consistency</code> Consistency of velocity 0-100 <code>hand_balance</code> L/R hand consistency 0-100 <code>tempo_stability</code> Tempo drift throughout exercise 0-100 <code>tier_confidence</code> Confidence in skill tier label 0-1"},{"location":"examples/filtering/#filter-by-augmentation","title":"Filter by Augmentation","text":""},{"location":"examples/filtering/#clean-samples-only","title":"Clean Samples Only","text":"<pre><code>def filter_clean_samples(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Filter to clean (non-augmented) samples only.\"\"\"\n    if \"augmentation_preset\" not in df.columns:\n        return df\n\n    mask = df[\"augmentation_preset\"].isna() | (df[\"augmentation_preset\"] == \"none\")\n    return df[mask].reset_index(drop=True)\n\n\nclean_df = filter_clean_samples(df)\nprint(f\"Clean samples: {len(clean_df)}\")\n</code></pre>"},{"location":"examples/filtering/#augmented-samples-only","title":"Augmented Samples Only","text":"<pre><code>def filter_augmented_samples(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Filter to augmented samples only.\"\"\"\n    if \"augmentation_preset\" not in df.columns:\n        return df\n\n    mask = df[\"augmentation_preset\"].notna() &amp; (df[\"augmentation_preset\"] != \"none\")\n    return df[mask].reset_index(drop=True)\n\n\naugmented_df = filter_augmented_samples(df)\nprint(f\"Augmented samples: {len(augmented_df)}\")\n</code></pre>"},{"location":"examples/filtering/#specific-augmentation-preset","title":"Specific Augmentation Preset","text":"<pre><code>def filter_by_augmentation_preset(df: pd.DataFrame, preset: str) -&gt; pd.DataFrame:\n    \"\"\"Filter to specific augmentation preset.\"\"\"\n    return df[df[\"augmentation_preset\"] == preset].reset_index(drop=True)\n\n\n# Get moderately augmented samples\nmoderate_df = filter_by_augmentation_preset(df, \"moderate\")\n\n# Available presets (check your dataset)\nif \"augmentation_preset\" in df.columns:\n    presets = df[\"augmentation_preset\"].unique()\n    print(f\"Available presets: {presets}\")\n</code></pre>"},{"location":"examples/filtering/#filter-by-soundfont","title":"Filter by Soundfont","text":"<pre><code>def filter_by_soundfont(df: pd.DataFrame, soundfont: str) -&gt; pd.DataFrame:\n    \"\"\"Filter to specific soundfont.\"\"\"\n    return df[df[\"soundfont\"] == soundfont].reset_index(drop=True)\n\n\n# Get practice pad samples only\npractice_pad_df = filter_by_soundfont(df, \"practice_pad\")\n\n# Get marching snare samples\nmarching_df = filter_by_soundfont(df, \"marching_snare\")\n\n# List available soundfonts\nif \"soundfont\" in df.columns:\n    soundfonts = df[\"soundfont\"].unique()\n    print(f\"Available soundfonts: {soundfonts}\")\n</code></pre>"},{"location":"examples/filtering/#combined-filters","title":"Combined Filters","text":"<p>Chain multiple filters for specific subsets:</p> <pre><code># Advanced players, single paradiddle, fast tempo, high quality\nsubset = df[\n    (df[\"skill_tier\"] == \"advanced\") &amp;\n    (df[\"rudiment_slug\"] == \"single_paradiddle\") &amp;\n    (df[\"tempo_bpm\"] &gt;= 140) &amp;\n    (df[\"overall_score\"] &gt;= 75)\n]\n\nprint(f\"Subset size: {len(subset)}\")\n</code></pre>"},{"location":"examples/filtering/#using-filteredsousadataset","title":"Using FilteredSOUSADataset","text":"<p>The <code>FilteredSOUSADataset</code> class combines filtering with PyTorch DataLoader:</p> <pre><code>from examples.filtering import FilteredSOUSADataset\n\n# Create filtered dataset\ndataset = FilteredSOUSADataset(\n    data_dir=\"output/dataset\",\n    split=\"train\",\n    target=\"skill_tier\",\n    filters={\n        \"augmentation_preset\": None,    # None = clean/null\n        \"soundfont\": \"practice_pad\",\n        \"skill_tier\": \"advanced\",\n    }\n)\n\nprint(f\"Filtered dataset size: {len(dataset)}\")\n</code></pre>"},{"location":"examples/filtering/#filter-values","title":"Filter Values","text":"Filter Value Type Example Single value <code>str</code> <code>{\"skill_tier\": \"advanced\"}</code> Multiple values <code>list</code> <code>{\"skill_tier\": [\"advanced\", \"professional\"]}</code> Null/clean <code>None</code> <code>{\"augmentation_preset\": None}</code>"},{"location":"examples/filtering/#augmentation-groups","title":"Augmentation Groups","text":"<p>Link clean samples with their augmented variants:</p> <pre><code>def group_by_augmentation(df: pd.DataFrame) -&gt; dict[str, pd.DataFrame]:\n    \"\"\"Group samples by augmentation_group_id.\"\"\"\n    if \"augmentation_group_id\" not in df.columns:\n        return {}\n\n    return {\n        group_id: group_df.reset_index(drop=True)\n        for group_id, group_df in df.groupby(\"augmentation_group_id\")\n    }\n\n\ndef get_clean_augmented_pairs(df: pd.DataFrame) -&gt; list[tuple[str, list[str]]]:\n    \"\"\"Get pairs of (clean_sample_id, [augmented_sample_ids]).\"\"\"\n    if \"augmentation_group_id\" not in df.columns:\n        return []\n\n    pairs = []\n    for group_id, group_df in df.groupby(\"augmentation_group_id\"):\n        clean_mask = (\n            group_df[\"augmentation_preset\"].isna() |\n            (group_df[\"augmentation_preset\"] == \"none\")\n        )\n        clean_ids = group_df[clean_mask][\"sample_id\"].tolist()\n        augmented_ids = group_df[~clean_mask][\"sample_id\"].tolist()\n\n        if clean_ids and augmented_ids:\n            for clean_id in clean_ids:\n                pairs.append((clean_id, augmented_ids))\n\n    return pairs\n\n\n# Get pairs for contrastive learning\npairs = get_clean_augmented_pairs(df)\nprint(f\"Found {len(pairs)} clean-augmented pairs\")\n\nif pairs:\n    clean_id, aug_ids = pairs[0]\n    print(f\"Clean: {clean_id}\")\n    print(f\"Augmented: {aug_ids}\")\n</code></pre>"},{"location":"examples/filtering/#dataset-statistics","title":"Dataset Statistics","text":"<p>Print summary statistics for any filtered subset:</p> <pre><code>def print_dataset_summary(df: pd.DataFrame, name: str = \"Dataset\"):\n    \"\"\"Print summary statistics for a filtered dataset.\"\"\"\n    print(f\"\\n{name}\")\n    print(\"=\" * 50)\n    print(f\"Total samples: {len(df)}\")\n\n    if \"skill_tier\" in df.columns:\n        print(f\"\\nBy skill tier:\")\n        for tier, count in df[\"skill_tier\"].value_counts().items():\n            pct = count / len(df) * 100\n            print(f\"  {tier}: {count} ({pct:.1f}%)\")\n\n    if \"soundfont\" in df.columns:\n        print(f\"\\nBy soundfont:\")\n        for sf, count in df[\"soundfont\"].value_counts().items():\n            print(f\"  {sf}: {count}\")\n\n    if \"overall_score\" in df.columns:\n        print(f\"\\nScore statistics:\")\n        print(f\"  Mean: {df['overall_score'].mean():.1f}\")\n        print(f\"  Std:  {df['overall_score'].std():.1f}\")\n        print(f\"  Min:  {df['overall_score'].min():.1f}\")\n        print(f\"  Max:  {df['overall_score'].max():.1f}\")\n\n\n# Use it\nprint_dataset_summary(df, \"Full Dataset\")\nprint_dataset_summary(filter_clean_samples(df), \"Clean Samples Only\")\n</code></pre>"},{"location":"examples/filtering/#example-balanced-skill-tier-subset","title":"Example: Balanced Skill Tier Subset","text":"<p>Create a balanced subset for classification:</p> <pre><code>def create_balanced_subset(\n    df: pd.DataFrame,\n    samples_per_tier: int = 1000,\n    random_state: int = 42,\n) -&gt; pd.DataFrame:\n    \"\"\"Create balanced subset with equal samples per skill tier.\"\"\"\n    balanced_dfs = []\n\n    for tier in [\"beginner\", \"intermediate\", \"advanced\", \"professional\"]:\n        tier_df = df[df[\"skill_tier\"] == tier]\n\n        if len(tier_df) &gt;= samples_per_tier:\n            sampled = tier_df.sample(n=samples_per_tier, random_state=random_state)\n        else:\n            sampled = tier_df  # Take all if not enough\n\n        balanced_dfs.append(sampled)\n\n    return pd.concat(balanced_dfs, ignore_index=True)\n\n\nbalanced_df = create_balanced_subset(df, samples_per_tier=500)\nprint_dataset_summary(balanced_df, \"Balanced Dataset\")\n</code></pre>"},{"location":"examples/filtering/#example-filter-for-specific-experiment","title":"Example: Filter for Specific Experiment","text":"<pre><code>def create_experiment_subset(\n    data_dir: str,\n    output_path: str,\n):\n    \"\"\"Create a filtered subset for a timing accuracy experiment.\"\"\"\n    df = load_metadata(data_dir)\n\n    # Filter criteria:\n    # - Clean audio only (no augmentation)\n    # - Single paradiddle rudiment\n    # - Medium tempo range (100-140 BPM)\n    # - Exclude extreme scores (keep 20-90 range)\n\n    subset = df[\n        # Clean audio\n        (df[\"augmentation_preset\"].isna() | (df[\"augmentation_preset\"] == \"none\")) &amp;\n        # Single paradiddle\n        (df[\"rudiment_slug\"] == \"single_paradiddle\") &amp;\n        # Medium tempo\n        (df[\"tempo_bpm\"] &gt;= 100) &amp; (df[\"tempo_bpm\"] &lt;= 140) &amp;\n        # Exclude extremes\n        (df[\"overall_score\"] &gt;= 20) &amp; (df[\"overall_score\"] &lt;= 90)\n    ]\n\n    print(f\"Filtered to {len(subset)} samples\")\n\n    # Save sample IDs for reproducibility\n    subset[[\"sample_id\", \"skill_tier\", \"tempo_bpm\", \"overall_score\"]].to_csv(\n        output_path, index=False\n    )\n    print(f\"Saved to {output_path}\")\n\n    return subset\n\n\n# Run\nsubset = create_experiment_subset(\n    \"output/dataset\",\n    \"experiment_samples.csv\"\n)\n</code></pre>"},{"location":"examples/filtering/#using-filtered-indices-with-dataloader","title":"Using Filtered Indices with DataLoader","text":"<pre><code>from torch.utils.data import Subset\nfrom examples.pytorch_dataloader import SOUSADataset, create_dataloader\n\n\n# Load full dataset\nfull_dataset = SOUSADataset(\n    data_dir=\"output/dataset\",\n    split=\"train\",\n    target=\"skill_tier\",\n)\n\n# Get filtered indices\ndf = load_metadata(\"output/dataset\")\nfiltered_df = df[\n    (df[\"skill_tier\"].isin([\"advanced\", \"professional\"])) &amp;\n    (df[\"overall_score\"] &gt;= 70)\n]\n\n# Map sample_ids to dataset indices\nsample_id_to_idx = {\n    row[\"sample_id\"]: idx\n    for idx, row in full_dataset.data.iterrows()\n}\n\nfiltered_indices = [\n    sample_id_to_idx[sid]\n    for sid in filtered_df[\"sample_id\"]\n    if sid in sample_id_to_idx\n]\n\n# Create subset\nfiltered_dataset = Subset(full_dataset, filtered_indices)\nprint(f\"Filtered dataset size: {len(filtered_dataset)}\")\n\n# Create dataloader\ndataloader = create_dataloader(filtered_dataset, batch_size=16)\n</code></pre>"},{"location":"examples/filtering/#next-steps","title":"Next Steps","text":"<ul> <li>PyTorch DataLoader - Use filtered data in training</li> <li>Feature Extraction - Extract features from filtered subsets</li> <li>Hierarchical Labels - Access detailed stroke-level labels</li> </ul>"},{"location":"examples/hierarchical-labels/","title":"Hierarchical Labels","text":"<p>SOUSA provides labels at three hierarchical levels: exercise, measure, and stroke. This guide shows how to access and use these multi-level labels for various ML tasks.</p> <p>Source Code</p> <p>Complete implementation: <code>examples/hierarchical_labels.py</code></p>"},{"location":"examples/hierarchical-labels/#label-hierarchy","title":"Label Hierarchy","text":"<pre><code>Exercise Level (1 per sample)\n\u251c\u2500\u2500 Overall score, timing accuracy, etc.\n\u2502\n\u251c\u2500\u2500 Measure Level (N per sample)\n\u2502   \u251c\u2500\u2500 Per-measure timing statistics\n\u2502   \u251c\u2500\u2500 Per-measure velocity statistics\n\u2502   \u2514\u2500\u2500 Hand balance metrics\n\u2502\n\u2514\u2500\u2500 Stroke Level (M per sample)\n    \u251c\u2500\u2500 Intended vs actual timing\n    \u251c\u2500\u2500 Intended vs actual velocity\n    \u251c\u2500\u2500 Hand (L/R)\n    \u2514\u2500\u2500 Articulation type\n</code></pre>"},{"location":"examples/hierarchical-labels/#prerequisites","title":"Prerequisites","text":"<pre><code>pip install pandas numpy\n# Optional for visualization:\npip install matplotlib\n# Optional for PyTorch integration:\npip install torch\n</code></pre>"},{"location":"examples/hierarchical-labels/#loading-hierarchical-labels","title":"Loading Hierarchical Labels","text":"<pre><code>from examples.hierarchical_labels import HierarchicalLabels\n\n# Load all label levels\nlabels = HierarchicalLabels(\"output/dataset\")\n\n# Get sample IDs\nsample_ids = labels.get_sample_ids()\nprint(f\"Total samples: {len(sample_ids)}\")\n\nsample_id = sample_ids[0]\n</code></pre>"},{"location":"examples/hierarchical-labels/#exercise-level-labels","title":"Exercise-Level Labels","text":"<p>Exercise-level labels provide overall performance scores:</p> <pre><code># Get exercise scores\nexercise = labels.get_exercise(sample_id)\n\nprint(f\"Overall score: {exercise['overall_score']:.1f}\")\nprint(f\"Timing accuracy: {exercise['timing_accuracy']:.1f}\")\nprint(f\"Timing consistency: {exercise['timing_consistency']:.1f}\")\nprint(f\"Velocity accuracy: {exercise['velocity_accuracy']:.1f}\")\nprint(f\"Tempo stability: {exercise['tempo_stability']:.1f}\")\n</code></pre>"},{"location":"examples/hierarchical-labels/#exercise-score-columns","title":"Exercise Score Columns","text":"Column Description <code>overall_score</code> Composite quality score (0-100) <code>timing_accuracy</code> Mean timing error score (0-100) <code>timing_consistency</code> Timing error variance score (0-100) <code>velocity_accuracy</code> Mean velocity error score (0-100) <code>velocity_consistency</code> Velocity variance score (0-100) <code>hand_balance</code> L/R hand consistency (0-100) <code>tempo_stability</code> Tempo drift score (0-100) <code>flam_quality</code> Flam spacing quality (if applicable) <code>diddle_quality</code> Diddle evenness (if applicable)"},{"location":"examples/hierarchical-labels/#measure-level-labels","title":"Measure-Level Labels","text":"<p>Measure-level labels aggregate statistics per measure:</p> <pre><code># Get measures for a sample\nmeasures = labels.get_measures(sample_id)\n\nprint(f\"Number of measures: {len(measures)}\")\nprint(f\"Columns: {list(measures.columns)}\")\n\n# Examine first measure\nfirst_measure = measures.iloc[0]\nprint(f\"\\nMeasure 0:\")\nprint(f\"  Strokes {first_measure['stroke_start']} to {first_measure['stroke_end']}\")\nprint(f\"  Timing mean error: {first_measure['timing_mean_error_ms']:.2f} ms\")\nprint(f\"  Timing std: {first_measure['timing_std_ms']:.2f} ms\")\n</code></pre>"},{"location":"examples/hierarchical-labels/#measure-columns","title":"Measure Columns","text":"Column Description <code>index</code> Measure number (0-based) <code>stroke_start</code> First stroke index in measure <code>stroke_end</code> Last stroke index (exclusive) <code>timing_mean_error_ms</code> Mean timing error in ms <code>timing_std_ms</code> Timing error standard deviation <code>velocity_mean</code> Mean MIDI velocity <code>velocity_std</code> Velocity standard deviation <code>lr_velocity_ratio</code> Left/right hand velocity ratio"},{"location":"examples/hierarchical-labels/#stroke-level-labels","title":"Stroke-Level Labels","text":"<p>Stroke-level labels provide the finest granularity:</p> <pre><code># Get all strokes for a sample\nstrokes = labels.get_strokes(sample_id)\n\nprint(f\"Number of strokes: {len(strokes)}\")\nprint(f\"Columns: {list(strokes.columns)}\")\n\n# Examine first few strokes\nfor i, stroke in strokes.head().iterrows():\n    print(f\"Stroke {i}: {stroke['hand']} hand, \"\n          f\"intended={stroke['intended_time_ms']:.1f}ms, \"\n          f\"actual={stroke['actual_time_ms']:.1f}ms, \"\n          f\"error={stroke['timing_error_ms']:.1f}ms\")\n</code></pre>"},{"location":"examples/hierarchical-labels/#stroke-columns","title":"Stroke Columns","text":"Column Description <code>index</code> Stroke number (0-based) <code>hand</code> <code>\"L\"</code> or <code>\"R\"</code> <code>stroke_type</code> <code>\"tap\"</code>, <code>\"accent\"</code>, <code>\"grace\"</code>, <code>\"diddle\"</code> <code>intended_time_ms</code> Target timing (milliseconds) <code>actual_time_ms</code> Performed timing (milliseconds) <code>timing_error_ms</code> Actual - intended (positive = late) <code>intended_velocity</code> Target MIDI velocity (0-127) <code>actual_velocity</code> Performed MIDI velocity <code>is_grace_note</code> Boolean flag <code>is_accent</code> Boolean flag"},{"location":"examples/hierarchical-labels/#accessing-strokes-within-a-measure","title":"Accessing Strokes Within a Measure","text":"<pre><code># Get strokes for measure 2\nmeasure_strokes = labels.get_strokes_for_measure(sample_id, measure_index=2)\n\nprint(f\"Strokes in measure 2: {len(measure_strokes)}\")\nfor _, stroke in measure_strokes.iterrows():\n    print(f\"  {stroke['hand']} at {stroke['actual_time_ms']:.1f}ms\")\n</code></pre>"},{"location":"examples/hierarchical-labels/#aligning-strokes-with-audio","title":"Aligning Strokes with Audio","text":"<p>For sequence models, align stroke times with audio sample indices:</p> <pre><code># Align strokes with audio at 16kHz\naligned = labels.align_strokes_with_audio(\n    sample_id,\n    sample_rate=16000,\n)\n\nprint(\"First 5 strokes with sample indices:\")\nfor _, stroke in aligned.head().iterrows():\n    print(f\"  Stroke at {stroke['actual_time_ms']:.1f}ms -&gt; \"\n          f\"sample {stroke['actual_sample_idx']}\")\n</code></pre>"},{"location":"examples/hierarchical-labels/#creating-stroke-sequences-for-rnnstransformers","title":"Creating Stroke Sequences for RNNs/Transformers","text":""},{"location":"examples/hierarchical-labels/#fixed-length-sequences","title":"Fixed-Length Sequences","text":"<pre><code># Create padded stroke sequence\nsequence = labels.create_stroke_sequence(\n    sample_id,\n    max_strokes=128,\n    features=[\"timing_error_ms\", \"actual_velocity\", \"is_accent\", \"is_grace_note\"],\n)\n\nprint(f\"Sequence shape: {sequence['sequence'].shape}\")  # (128, 4)\nprint(f\"Valid strokes: {sequence['mask'].sum()}\")\nprint(f\"Feature names: {sequence['feature_names']}\")\n</code></pre>"},{"location":"examples/hierarchical-labels/#pytorch-tensors","title":"PyTorch Tensors","text":"<pre><code># Get as PyTorch tensors\nsequence_torch = labels.create_stroke_sequence_torch(\n    sample_id,\n    max_strokes=128,\n)\n\nprint(f\"Sequence: {sequence_torch['sequence'].shape}\")  # torch.Size([128, 4])\nprint(f\"Mask: {sequence_torch['mask'].shape}\")          # torch.Size([128])\n</code></pre>"},{"location":"examples/hierarchical-labels/#frame-level-targets","title":"Frame-Level Targets","text":""},{"location":"examples/hierarchical-labels/#onset-detection-targets","title":"Onset Detection Targets","text":"<p>Create binary onset labels aligned with audio frames:</p> <pre><code># Create onset targets at 10ms resolution\nduration_ms = strokes[\"actual_time_ms\"].max() + 500  # Add buffer\nonset_targets = labels.create_onset_targets(\n    sample_id,\n    duration_ms=duration_ms,\n    resolution_ms=10.0,\n)\n\nprint(f\"Onset target shape: {onset_targets.shape}\")\nprint(f\"Number of onsets: {onset_targets.sum()}\")\n</code></pre>"},{"location":"examples/hierarchical-labels/#hand-classification-targets","title":"Hand Classification Targets","text":"<pre><code># Create hand (L/R) targets for each frame\nhand_targets = labels.create_hand_targets(\n    sample_id,\n    duration_ms=duration_ms,\n    resolution_ms=10.0,\n)\n\nprint(f\"Hand target shape: {hand_targets.shape}\")\nprint(f\"Unique values: {np.unique(hand_targets)}\")\n# 0 = no stroke, 1 = left hand, 2 = right hand\n</code></pre>"},{"location":"examples/hierarchical-labels/#timing-error-sequences","title":"Timing Error Sequences","text":"<pre><code># Get normalized timing errors for all strokes\ntiming_errors = labels.get_timing_error_sequence(\n    sample_id,\n    normalize=True,  # Normalized to ~[-1, 1]\n)\n\nprint(f\"Timing errors shape: {timing_errors.shape}\")\nprint(f\"Range: [{timing_errors.min():.3f}, {timing_errors.max():.3f}]\")\n</code></pre>"},{"location":"examples/hierarchical-labels/#measure-level-summary","title":"Measure-Level Summary","text":"<p>Compute extended measure statistics:</p> <pre><code># Get detailed measure summary\nsummary = labels.compute_measure_summary(sample_id)\n\nprint(\"Measure summary:\")\nfor _, measure in summary.iterrows():\n    print(f\"  Measure {measure['index']}: \"\n          f\"{measure['num_strokes']} strokes, \"\n          f\"{measure['num_accents']} accents, \"\n          f\"{measure['pct_right_hand']:.0%} right hand\")\n</code></pre>"},{"location":"examples/hierarchical-labels/#visualization","title":"Visualization","text":"<p>Visualize stroke timeline with audio:</p> <pre><code>import torchaudio\n\n# Load audio\naudio_path = f\"output/dataset/{labels.get_sample_metadata(sample_id)['audio_path']}\"\nwaveform, sr = torchaudio.load(audio_path)\naudio = waveform.squeeze().numpy()\n\n# Resample if needed\nif sr != 16000:\n    resampler = torchaudio.transforms.Resample(sr, 16000)\n    audio = resampler(waveform).squeeze().numpy()\n\n# Create visualization\nfig, axes = labels.visualize_strokes(\n    sample_id,\n    audio_array=audio,\n    sample_rate=16000,\n    figsize=(14, 10),\n    save_path=\"stroke_visualization.png\",\n)\n</code></pre> <p>The visualization includes:</p> <ol> <li>Waveform with stroke markers (red = right hand, blue = left hand)</li> <li>Timing errors over time</li> <li>Velocity pattern</li> <li>Sticking pattern (L/R alternation)</li> </ol>"},{"location":"examples/hierarchical-labels/#multi-task-learning-example","title":"Multi-Task Learning Example","text":"<p>Use hierarchical labels for multi-task targets:</p> <pre><code>import torch\nfrom torch.utils.data import Dataset\n\n\nclass HierarchicalLabelDataset(Dataset):\n    \"\"\"Dataset with multi-level targets.\"\"\"\n\n    def __init__(\n        self,\n        data_dir: str,\n        max_strokes: int = 128,\n    ):\n        self.labels = HierarchicalLabels(data_dir)\n        self.sample_ids = self.labels.get_sample_ids()\n        self.max_strokes = max_strokes\n\n    def __len__(self):\n        return len(self.sample_ids)\n\n    def __getitem__(self, idx):\n        sample_id = self.sample_ids[idx]\n\n        # Exercise-level targets\n        exercise = self.labels.get_exercise(sample_id)\n        exercise_target = torch.tensor([\n            exercise[\"overall_score\"] / 100.0,\n            exercise[\"timing_accuracy\"] / 100.0,\n            exercise[\"tempo_stability\"] / 100.0,\n        ])\n\n        # Stroke-level sequence\n        stroke_seq = self.labels.create_stroke_sequence_torch(\n            sample_id,\n            max_strokes=self.max_strokes,\n        )\n\n        # Measure count (for auxiliary task)\n        measures = self.labels.get_measures(sample_id)\n        num_measures = len(measures)\n\n        return {\n            \"sample_id\": sample_id,\n            \"exercise_target\": exercise_target,       # (3,)\n            \"stroke_sequence\": stroke_seq[\"sequence\"], # (max_strokes, 4)\n            \"stroke_mask\": stroke_seq[\"mask\"],         # (max_strokes,)\n            \"num_measures\": num_measures,\n        }\n\n\n# Usage\ndataset = HierarchicalLabelDataset(\"output/dataset\")\nsample = dataset[0]\nprint(f\"Exercise target: {sample['exercise_target']}\")\nprint(f\"Stroke sequence: {sample['stroke_sequence'].shape}\")\n</code></pre>"},{"location":"examples/hierarchical-labels/#regression-vs-classification-tasks","title":"Regression vs Classification Tasks","text":""},{"location":"examples/hierarchical-labels/#stroke-level-regression","title":"Stroke-Level Regression","text":"<p>Predict timing errors for each stroke:</p> <pre><code># Target: timing error per stroke\nstroke_seq = labels.create_stroke_sequence(\n    sample_id,\n    features=[\"timing_error_ms\"],\n)\n# Use stroke_seq[\"sequence\"][:, 0] as regression target\n</code></pre>"},{"location":"examples/hierarchical-labels/#measure-level-classification","title":"Measure-Level Classification","text":"<p>Classify measures as \"good\" or \"needs work\":</p> <pre><code>measures = labels.get_measures(sample_id)\n\n# Create binary labels based on timing std\nthreshold_ms = 15.0\nmeasure_labels = (measures[\"timing_std_ms\"] &lt; threshold_ms).astype(int)\n# 1 = good timing consistency, 0 = needs work\n</code></pre>"},{"location":"examples/hierarchical-labels/#exercise-level-ordinal-regression","title":"Exercise-Level Ordinal Regression","text":"<p>Predict skill tier from ordinal labels:</p> <pre><code>SKILL_TIER_ORDINAL = {\n    \"beginner\": 0,\n    \"intermediate\": 1,\n    \"advanced\": 2,\n    \"professional\": 3,\n}\n\nmetadata = labels.get_sample_metadata(sample_id)\nordinal_label = SKILL_TIER_ORDINAL[metadata[\"skill_tier\"]]\n</code></pre>"},{"location":"examples/hierarchical-labels/#example-stroke-level-transformer","title":"Example: Stroke-Level Transformer","text":"<pre><code>import torch\nimport torch.nn as nn\n\n\nclass StrokeTransformer(nn.Module):\n    \"\"\"Transformer for stroke sequence analysis.\"\"\"\n\n    def __init__(\n        self,\n        input_dim: int = 4,\n        hidden_dim: int = 128,\n        num_layers: int = 4,\n        num_heads: int = 4,\n        num_classes: int = 4,  # Skill tiers\n    ):\n        super().__init__()\n\n        self.input_proj = nn.Linear(input_dim, hidden_dim)\n\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=hidden_dim,\n            nhead=num_heads,\n            dim_feedforward=hidden_dim * 4,\n            batch_first=True,\n        )\n        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n\n        self.classifier = nn.Linear(hidden_dim, num_classes)\n\n    def forward(self, stroke_seq, mask):\n        # stroke_seq: (B, max_strokes, input_dim)\n        # mask: (B, max_strokes)\n\n        x = self.input_proj(stroke_seq)\n\n        # Create attention mask (True = ignore)\n        attn_mask = ~mask\n\n        x = self.encoder(x, src_key_padding_mask=attn_mask)\n\n        # Pool over valid strokes\n        x = x * mask.unsqueeze(-1)\n        x = x.sum(dim=1) / mask.sum(dim=1, keepdim=True)\n\n        return self.classifier(x)\n\n\n# Usage\nmodel = StrokeTransformer()\n\nsample = dataset[0]\nstroke_seq = sample[\"stroke_sequence\"].unsqueeze(0)  # Add batch dim\nmask = sample[\"stroke_mask\"].unsqueeze(0)\n\nlogits = model(stroke_seq, mask)\nprint(f\"Output shape: {logits.shape}\")  # (1, 4)\n</code></pre>"},{"location":"examples/hierarchical-labels/#next-steps","title":"Next Steps","text":"<ul> <li>PyTorch DataLoader - Combine with audio features</li> <li>Feature Extraction - Extract mel spectrograms aligned with strokes</li> <li>Filtering Samples - Filter by exercise-level scores</li> </ul>"},{"location":"examples/pytorch-dataloader/","title":"PyTorch DataLoader Integration","text":"<p>This guide shows how to integrate the SOUSA dataset with PyTorch for training neural networks. We cover custom <code>Dataset</code> classes, collate functions for variable-length audio, and efficient DataLoader configurations.</p> <p>Source Code</p> <p>Complete implementation: <code>examples/pytorch_dataloader.py</code></p>"},{"location":"examples/pytorch-dataloader/#prerequisites","title":"Prerequisites","text":"<pre><code>pip install torch torchaudio\n</code></pre>"},{"location":"examples/pytorch-dataloader/#basic-usage","title":"Basic Usage","text":""},{"location":"examples/pytorch-dataloader/#loading-the-dataset","title":"Loading the Dataset","text":"<pre><code>from examples.pytorch_dataloader import SOUSADataset, create_dataloader\n\n# Create dataset for training\ndataset = SOUSADataset(\n    data_dir=\"output/dataset\",\n    split=\"train\",\n    target=\"skill_tier\",      # Classification target\n    resample_rate=16000,      # Resample to 16kHz\n    max_length_sec=5.0,       # Truncate to 5 seconds\n)\n\nprint(f\"Dataset size: {len(dataset)}\")\nprint(f\"Number of classes: {dataset.num_classes}\")\n</code></pre>"},{"location":"examples/pytorch-dataloader/#creating-a-dataloader","title":"Creating a DataLoader","text":"<pre><code># Create DataLoader with custom collate function\ndataloader = create_dataloader(\n    dataset,\n    batch_size=16,\n    shuffle=True,\n    num_workers=4,\n    fixed_length=80000,  # 5 seconds at 16kHz (optional)\n)\n\n# Iterate over batches\nfor batch in dataloader:\n    waveforms = batch[\"waveforms\"]       # (B, max_length)\n    lengths = batch[\"lengths\"]           # (B,)\n    labels = batch[\"labels\"]             # (B,)\n    attention_mask = batch[\"attention_mask\"]  # (B, max_length)\n\n    # Your training code here\n    break\n</code></pre>"},{"location":"examples/pytorch-dataloader/#sousadataset-class","title":"SOUSADataset Class","text":"<p>The <code>SOUSADataset</code> class wraps the SOUSA parquet files and provides a standard PyTorch <code>Dataset</code> interface.</p>"},{"location":"examples/pytorch-dataloader/#constructor-arguments","title":"Constructor Arguments","text":"Argument Type Default Description <code>data_dir</code> <code>str \\| Path</code> required Path to SOUSA dataset directory <code>split</code> <code>str \\| None</code> <code>None</code> <code>\"train\"</code>, <code>\"validation\"</code>, <code>\"test\"</code>, or <code>None</code> for all <code>target</code> <code>str \\| list[str]</code> <code>\"overall_score\"</code> Target variable(s) for labels <code>resample_rate</code> <code>int \\| None</code> <code>16000</code> Target sample rate (None keeps 44.1kHz) <code>max_length_sec</code> <code>float \\| None</code> <code>None</code> Maximum audio length in seconds <code>transform</code> <code>Callable \\| None</code> <code>None</code> Optional audio transform <code>normalize_audio</code> <code>bool</code> <code>True</code> Normalize audio to [-1, 1]"},{"location":"examples/pytorch-dataloader/#target-options","title":"Target Options","text":"ClassificationRegressionMulti-Task <pre><code># Skill tier classification (4 classes)\ndataset = SOUSADataset(\n    data_dir=\"output/dataset\",\n    target=\"skill_tier\",\n)\n# Labels: 0=beginner, 1=intermediate, 2=advanced, 3=professional\n\n# Rudiment classification (40 classes)\ndataset = SOUSADataset(\n    data_dir=\"output/dataset\",\n    target=\"rudiment_slug\",\n)\n</code></pre> <pre><code># Single score regression\ndataset = SOUSADataset(\n    data_dir=\"output/dataset\",\n    target=\"overall_score\",\n)\n# Labels: normalized to [0, 1]\n\n# Any score column works\ndataset = SOUSADataset(\n    data_dir=\"output/dataset\",\n    target=\"timing_accuracy\",\n)\n</code></pre> <pre><code># Multiple targets for multi-task learning\ndataset = SOUSADataset(\n    data_dir=\"output/dataset\",\n    target=[\"overall_score\", \"timing_accuracy\", \"tempo_stability\"],\n)\n# Labels: tensor of shape (3,) with normalized scores\n</code></pre>"},{"location":"examples/pytorch-dataloader/#accessing-samples","title":"Accessing Samples","text":"<pre><code># Get a single sample\nsample = dataset[0]\n\nprint(sample[\"waveform\"].shape)    # torch.Size([80000]) - 5 sec @ 16kHz\nprint(sample[\"label\"])             # 2 (for skill_tier target)\nprint(sample[\"sample_id\"])         # \"adv042_single_paradiddle_100bpm_...\"\nprint(sample[\"skill_tier\"])        # \"advanced\"\nprint(sample[\"rudiment_slug\"])     # \"single_paradiddle\"\nprint(sample[\"tempo_bpm\"])         # 100\n</code></pre>"},{"location":"examples/pytorch-dataloader/#collate-functions","title":"Collate Functions","text":"<p>SOUSA audio samples have variable lengths. We provide two collate strategies:</p>"},{"location":"examples/pytorch-dataloader/#variable-length-collation","title":"Variable-Length Collation","text":"<p>Pads each batch to the maximum length within that batch. Memory-efficient for diverse lengths.</p> <pre><code>from examples.pytorch_dataloader import collate_variable_length\n\ndataloader = DataLoader(\n    dataset,\n    batch_size=16,\n    collate_fn=collate_variable_length,\n)\n\nbatch = next(iter(dataloader))\n# batch[\"waveforms\"].shape varies per batch\n# batch[\"attention_mask\"] indicates valid vs padded positions\n</code></pre>"},{"location":"examples/pytorch-dataloader/#fixed-length-collation","title":"Fixed-Length Collation","text":"<p>Pads/truncates all samples to a fixed length. More efficient for training (consistent tensor shapes).</p> <pre><code>from examples.pytorch_dataloader import collate_fixed_length\nfrom functools import partial\n\n# Create collate function with fixed length\ncollate_fn = partial(collate_fixed_length, target_length=80000)\n\ndataloader = DataLoader(\n    dataset,\n    batch_size=16,\n    collate_fn=collate_fn,\n)\n\nbatch = next(iter(dataloader))\n# batch[\"waveforms\"].shape is always (B, 80000)\n</code></pre>"},{"location":"examples/pytorch-dataloader/#batch-dictionary","title":"Batch Dictionary","text":"<p>Both collate functions return the same structure:</p> <pre><code>{\n    \"waveforms\": torch.Tensor,      # (B, max_length) - padded audio\n    \"lengths\": torch.Tensor,        # (B,) - original lengths\n    \"labels\": torch.Tensor,         # (B,) or (B, num_targets)\n    \"attention_mask\": torch.Tensor, # (B, max_length) - 1=valid, 0=padding\n    \"sample_ids\": list[str],        # Sample identifiers\n    \"metadata\": {\n        \"skill_tiers\": list[str],\n        \"rudiment_slugs\": list[str],\n        \"tempo_bpms\": list[int],\n    }\n}\n</code></pre>"},{"location":"examples/pytorch-dataloader/#stratified-batch-sampler","title":"Stratified Batch Sampler","text":"<p>For classification tasks, use stratified sampling to ensure balanced batches:</p> <pre><code>from examples.pytorch_dataloader import StratifiedBatchSampler\n\nsampler = StratifiedBatchSampler(\n    dataset,\n    batch_size=16,\n    drop_last=False,\n)\n\ndataloader = DataLoader(\n    dataset,\n    batch_sampler=sampler,\n    collate_fn=collate_variable_length,\n)\n\n# Each batch contains ~equal samples from each skill tier\n</code></pre>"},{"location":"examples/pytorch-dataloader/#training-loop-example","title":"Training Loop Example","text":"<p>Here's a complete training loop for skill tier classification:</p> <pre><code>import torch\nimport torch.nn as nn\nfrom torch.optim import AdamW\nfrom torch.utils.data import DataLoader\n\nfrom examples.pytorch_dataloader import (\n    SOUSADataset,\n    create_dataloader,\n    SKILL_TIER_TO_ID,\n    ID_TO_SKILL_TIER,\n)\n\n\nclass SimpleClassifier(nn.Module):\n    \"\"\"Simple 1D CNN classifier for demonstration.\"\"\"\n\n    def __init__(self, num_classes: int = 4):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv1d(1, 32, kernel_size=80, stride=16),\n            nn.ReLU(),\n            nn.MaxPool1d(4),\n            nn.Conv1d(32, 64, kernel_size=3),\n            nn.ReLU(),\n            nn.AdaptiveAvgPool1d(1),\n        )\n        self.classifier = nn.Linear(64, num_classes)\n\n    def forward(self, x, attention_mask=None):\n        # x: (B, T) -&gt; (B, 1, T)\n        x = x.unsqueeze(1)\n        x = self.conv(x)\n        x = x.squeeze(-1)\n        return self.classifier(x)\n\n\ndef train_epoch(model, dataloader, optimizer, criterion, device):\n    model.train()\n    total_loss = 0\n    correct = 0\n    total = 0\n\n    for batch in dataloader:\n        waveforms = batch[\"waveforms\"].to(device)\n        labels = batch[\"labels\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n\n        optimizer.zero_grad()\n        outputs = model(waveforms, attention_mask)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n        _, predicted = outputs.max(1)\n        correct += predicted.eq(labels).sum().item()\n        total += labels.size(0)\n\n    return total_loss / len(dataloader), correct / total\n\n\ndef main():\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # Create datasets\n    train_dataset = SOUSADataset(\n        data_dir=\"output/dataset\",\n        split=\"train\",\n        target=\"skill_tier\",\n        resample_rate=16000,\n        max_length_sec=5.0,\n    )\n\n    val_dataset = SOUSADataset(\n        data_dir=\"output/dataset\",\n        split=\"validation\",\n        target=\"skill_tier\",\n        resample_rate=16000,\n        max_length_sec=5.0,\n    )\n\n    # Create dataloaders\n    train_loader = create_dataloader(\n        train_dataset,\n        batch_size=16,\n        shuffle=True,\n        fixed_length=80000,\n    )\n\n    val_loader = create_dataloader(\n        val_dataset,\n        batch_size=16,\n        shuffle=False,\n        fixed_length=80000,\n    )\n\n    # Model, optimizer, loss\n    model = SimpleClassifier(num_classes=4).to(device)\n    optimizer = AdamW(model.parameters(), lr=1e-4)\n    criterion = nn.CrossEntropyLoss()\n\n    # Training loop\n    for epoch in range(10):\n        train_loss, train_acc = train_epoch(\n            model, train_loader, optimizer, criterion, device\n        )\n        print(f\"Epoch {epoch+1}: Loss={train_loss:.4f}, Acc={train_acc:.2%}\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"examples/pytorch-dataloader/#custom-transforms","title":"Custom Transforms","text":"<p>Add audio augmentation during training:</p> <pre><code>import torchaudio.transforms as T\n\n\nclass AudioTransform:\n    \"\"\"Compose multiple audio transforms.\"\"\"\n\n    def __init__(self, sample_rate: int = 16000):\n        self.time_stretch = T.TimeStretch()\n        self.freq_mask = T.FrequencyMasking(freq_mask_param=30)\n        self.time_mask = T.TimeMasking(time_mask_param=100)\n\n    def __call__(self, waveform: torch.Tensor) -&gt; torch.Tensor:\n        # Add your augmentations here\n        # Note: Some transforms work on spectrograms, not waveforms\n        return waveform\n\n\n# Use with dataset\ndataset = SOUSADataset(\n    data_dir=\"output/dataset\",\n    split=\"train\",\n    target=\"skill_tier\",\n    transform=AudioTransform(),\n)\n</code></pre>"},{"location":"examples/pytorch-dataloader/#performance-tips","title":"Performance Tips","text":"<p>Optimization Recommendations</p> <ol> <li> <p>Use <code>fixed_length</code> collation for faster training (consistent tensor shapes)</p> </li> <li> <p>Increase <code>num_workers</code> based on your CPU cores:    <pre><code>dataloader = create_dataloader(dataset, num_workers=8)\n</code></pre></p> </li> <li> <p>Enable <code>pin_memory</code> for faster GPU transfer (enabled by default)</p> </li> <li> <p>Resample to 16kHz - sufficient for drum audio, faster processing</p> </li> <li> <p>Use smaller batch sizes if memory-limited:    <pre><code>dataloader = create_dataloader(dataset, batch_size=8)\n</code></pre></p> </li> </ol>"},{"location":"examples/pytorch-dataloader/#label-mappings","title":"Label Mappings","text":"<pre><code>from examples.pytorch_dataloader import (\n    SKILL_TIER_LABELS,      # [\"beginner\", \"intermediate\", \"advanced\", \"professional\"]\n    SKILL_TIER_TO_ID,       # {\"beginner\": 0, \"intermediate\": 1, ...}\n    ID_TO_SKILL_TIER,       # {0: \"beginner\", 1: \"intermediate\", ...}\n)\n\n# Convert predictions back to labels\npredictions = model(batch[\"waveforms\"])\npredicted_ids = predictions.argmax(dim=1)\npredicted_labels = [ID_TO_SKILL_TIER[i.item()] for i in predicted_ids]\n</code></pre>"},{"location":"examples/pytorch-dataloader/#next-steps","title":"Next Steps","text":"<ul> <li>Feature Extraction - Extract mel spectrograms and pretrained features</li> <li>Filtering Samples - Filter dataset before creating DataLoader</li> <li>Hierarchical Labels - Use stroke/measure-level labels for sequence models</li> </ul>"},{"location":"getting-started/","title":"Getting Started","text":"<p>Welcome to SOUSA! This section will help you get up and running with the synthetic drum rudiment dataset generator.</p>"},{"location":"getting-started/#overview","title":"Overview","text":"<p>SOUSA (Synthetic Open Unified Snare Assessment) generates synthetic drum rudiment datasets for machine learning training. You can either:</p> <ol> <li>Use the pre-generated dataset from HuggingFace Hub (easiest)</li> <li>Generate your own dataset locally with custom configurations</li> </ol>"},{"location":"getting-started/#quick-navigation","title":"Quick Navigation","text":"<ul> <li> <p> Installation</p> <p>Set up Python, install SOUSA, and configure FluidSynth for audio synthesis.</p> <p> Installation</p> </li> <li> <p> Quick Start</p> <p>Generate a test dataset and explore the samples in 5 minutes.</p> <p> Quick Start</p> </li> <li> <p> Loading Data</p> <p>Load the dataset from HuggingFace Hub or local files.</p> <p> Loading Data</p> </li> </ul>"},{"location":"getting-started/#recommended-path","title":"Recommended Path","text":""},{"location":"getting-started/#for-ml-practitioners","title":"For ML Practitioners","text":"<p>If you want to use SOUSA for training models, the fastest path is:</p> <pre><code>from datasets import load_dataset\n\n# Load from HuggingFace (no local installation needed)\ndataset = load_dataset(\"zkeown/sousa\")\n</code></pre> <p>See the Loading Data guide for more options.</p>"},{"location":"getting-started/#for-dataset-customization","title":"For Dataset Customization","text":"<p>If you want to generate custom datasets with different configurations:</p> <ol> <li>Follow the Installation guide</li> <li>Work through the Quick Start tutorial</li> <li>Explore the User Guide for advanced options</li> </ol>"},{"location":"getting-started/#requirements-at-a-glance","title":"Requirements at a Glance","text":"Requirement Minimum Recommended Python 3.10 3.11+ RAM 4 GB 8+ GB Disk (small preset) 1 GB 2 GB Disk (full preset) 100 GB 150 GB FluidSynth Required for audio -"},{"location":"getting-started/#what-youll-learn","title":"What You'll Learn","text":"<p>By the end of this section, you will be able to:</p> <ul> <li>Install SOUSA and its dependencies</li> <li>Generate a test dataset with MIDI and audio</li> <li>Load and explore samples programmatically</li> <li>Filter samples by skill level, rudiment, or score</li> <li>Prepare data for ML training pipelines</li> </ul>"},{"location":"getting-started/#need-help","title":"Need Help?","text":"<ul> <li>Check the User Guide for detailed examples</li> <li>See Limitations for known constraints</li> <li>Review Validation for quality assurance details</li> <li>Open an issue on GitHub for bugs or questions</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>This guide covers installing SOUSA and its dependencies for dataset generation.</p> <p>Just want to use the dataset?</p> <p>If you only need to load the pre-generated dataset, you can skip local installation entirely: <pre><code>pip install datasets\nfrom datasets import load_dataset\ndataset = load_dataset(\"zkeown/sousa\")\n</code></pre></p>"},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.10+ (3.11 or 3.12 recommended)</li> <li>FluidSynth (required for audio synthesis)</li> <li>pip (Python package manager)</li> </ul>"},{"location":"getting-started/installation/#python-installation","title":"Python Installation","text":"<p>SOUSA requires Python 3.10 or later. Check your version:</p> <pre><code>python --version\n# or\npython3 --version\n</code></pre> <p>If you need to install Python, visit python.org/downloads or use a version manager like <code>pyenv</code>.</p>"},{"location":"getting-started/installation/#install-sousa","title":"Install SOUSA","text":""},{"location":"getting-started/installation/#basic-installation","title":"Basic Installation","text":"<p>Clone the repository and install in editable mode:</p> <pre><code>git clone https://github.com/zakkeown/SOUSA.git\ncd SOUSA\n\n# Create virtual environment (recommended)\npython -m venv .venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n\n# Install base package\npip install -e .\n</code></pre>"},{"location":"getting-started/installation/#installation-extras","title":"Installation Extras","text":"<p>SOUSA provides optional dependency groups for different use cases:</p> DevelopmentHuggingFace HubMachine LearningDocumentationAll Extras <pre><code># Install with development tools (pytest, black, ruff)\npip install -e '.[dev]'\n</code></pre> <pre><code># Install with HuggingFace dependencies for uploading/downloading\npip install -e '.[hub]'\n</code></pre> <pre><code># Install with ML dependencies (PyTorch, transformers, etc.)\npip install -e '.[ml]'\n</code></pre> <pre><code># Install with MkDocs for building documentation\npip install -e '.[docs]'\n</code></pre> <pre><code># Install everything\npip install -e '.[dev,hub,ml,docs]'\n</code></pre>"},{"location":"getting-started/installation/#fluidsynth-installation","title":"FluidSynth Installation","text":"<p>FluidSynth is required for synthesizing audio from MIDI. Install it for your operating system:</p> macOSUbuntu/DebianWindowsConda <p>Using Homebrew:</p> <pre><code>brew install fluid-synth\n</code></pre> <p>Verify installation:</p> <pre><code>fluidsynth --version\n</code></pre> <p>Using apt:</p> <pre><code>sudo apt update\nsudo apt install fluidsynth libfluidsynth-dev\n</code></pre> <p>Verify installation:</p> <pre><code>fluidsynth --version\n</code></pre> <p>Option 1: Using Chocolatey</p> <pre><code>choco install fluidsynth\n</code></pre> <p>Option 2: Manual Installation</p> <ol> <li>Download FluidSynth from GitHub Releases</li> <li>Extract to a folder (e.g., <code>C:\\FluidSynth</code>)</li> <li>Add the <code>bin</code> folder to your system PATH</li> </ol> <p>Verify installation:</p> <pre><code>fluidsynth --version\n</code></pre> <p>If using Conda/Mamba:</p> <pre><code>conda install -c conda-forge fluidsynth\n</code></pre> <p>FluidSynth Required for Audio</p> <p>Without FluidSynth, you can still generate MIDI files and labels, but audio synthesis will fail. Use <code>--no-audio</code> flag if FluidSynth is not available: <pre><code>python scripts/generate_dataset.py --preset small\n</code></pre></p>"},{"location":"getting-started/installation/#soundfont-setup","title":"Soundfont Setup","text":"<p>Soundfonts (<code>.sf2</code> files) define the sounds used for audio synthesis. SOUSA includes a setup script to download recommended soundfonts:</p> <pre><code># Download recommended soundfonts (~170 MB)\npython scripts/setup_soundfonts.py\n</code></pre>"},{"location":"getting-started/installation/#soundfont-options","title":"Soundfont Options","text":"Default (Recommended)MinimalAll SoundfontsSpecific Soundfont <p>Downloads a good set for rudiment generation:</p> <pre><code>python scripts/setup_soundfonts.py\n</code></pre> <p>Includes:</p> <ul> <li>FluidR3_GM_GS (141 MB) - High-quality General MIDI</li> <li>Marching_Snare (0.2 MB) - Marching snare drum</li> <li>MT_Power_DrumKit (8.7 MB) - Punchy acoustic kit</li> </ul> <p>Just one good soundfont (~141 MB):</p> <pre><code>python scripts/setup_soundfonts.py --minimal\n</code></pre> <p>Download all available soundfonts (~460 MB):</p> <pre><code>python scripts/setup_soundfonts.py --all\n</code></pre> <p>Download a specific soundfont by name:</p> <pre><code>python scripts/setup_soundfonts.py --name Marching_Snare\n</code></pre>"},{"location":"getting-started/installation/#list-available-soundfonts","title":"List Available Soundfonts","text":"<pre><code>python scripts/setup_soundfonts.py --list\n</code></pre> <p>Output:</p> <pre><code>Available Soundfonts (all verified working)\n=================================================================\n\n*[INSTALLED] FluidR3_GM_GS\n   FluidR3 GM+GS merged - excellent quality, MIT license\n   Size: ~141 MB | License: MIT\n\n [available] GeneralUser_GS\n   Compact GM soundfont with great drums (~30MB)\n   Size: ~30 MB | License: Free for any use\n\n [available] Marching_Snare\n   Marching snare drum - great for rudiments\n   Size: ~0.2 MB | License: CC BY\n...\n</code></pre>"},{"location":"getting-started/installation/#custom-soundfonts","title":"Custom Soundfonts","text":"<p>You can use your own soundfonts by placing <code>.sf2</code> files in <code>data/soundfonts/</code> or specifying a path:</p> <pre><code>python scripts/generate_dataset.py --soundfont /path/to/my_soundfont.sf2\n</code></pre>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<p>Run the test suite to verify everything is working:</p> <pre><code># Run all tests\npytest\n\n# Run with verbose output\npytest -v\n\n# Run specific test module\npytest tests/test_profiles.py\n</code></pre> <p>Quick verification:</p> <pre><code># Check that SOUSA can be imported\npython -c \"from dataset_gen.pipeline.generate import DatasetGenerator; print('SOUSA installed successfully!')\"\n\n# Check FluidSynth binding\npython -c \"import fluidsynth; print('FluidSynth binding OK')\"\n</code></pre>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#fluidsynth-not-found","title":"FluidSynth Not Found","text":"<p>If you get <code>ModuleNotFoundError: No module named 'fluidsynth'</code>:</p> <pre><code># Reinstall the Python binding\npip install --force-reinstall pyfluidsynth\n</code></pre> <p>If the system library is not found:</p> macOSLinux <pre><code># Ensure Homebrew's lib path is in DYLD_LIBRARY_PATH\nexport DYLD_LIBRARY_PATH=\"/opt/homebrew/lib:$DYLD_LIBRARY_PATH\"\n</code></pre> <pre><code># Ensure libfluidsynth is installed\nsudo apt install libfluidsynth-dev\n\n# Check library path\nldconfig -p | grep fluidsynth\n</code></pre>"},{"location":"getting-started/installation/#soundfont-download-fails","title":"Soundfont Download Fails","text":"<p>If soundfont downloads fail:</p> <ol> <li>Check your internet connection</li> <li>Try downloading manually from musical-artifacts.com</li> <li>Place <code>.sf2</code> files in <code>data/soundfonts/</code></li> </ol>"},{"location":"getting-started/installation/#import-errors","title":"Import Errors","text":"<p>If you get import errors after installation:</p> <pre><code># Ensure you're in the virtual environment\nsource .venv/bin/activate\n\n# Reinstall in editable mode\npip install -e .\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<p>Once installation is complete:</p> <ol> <li>Quick Start - Generate your first dataset</li> <li>Loading Data - Access the pre-generated dataset</li> <li>User Guide - Detailed examples for ML tasks</li> </ol>"},{"location":"getting-started/loading/","title":"Loading the Dataset","text":"<p>This guide covers all methods for loading and accessing the SOUSA dataset.</p>"},{"location":"getting-started/loading/#from-huggingface-hub","title":"From HuggingFace Hub","text":"<p>The easiest way to use SOUSA is via the HuggingFace Hub. No local generation required.</p>"},{"location":"getting-started/loading/#basic-loading","title":"Basic Loading","text":"<pre><code>from datasets import load_dataset\n\n# Load the full dataset (all splits)\ndataset = load_dataset(\"zkeown/sousa\")\n\n# Access splits\ntrain = dataset[\"train\"]\nval = dataset[\"validation\"]\ntest = dataset[\"test\"]\n\n# Get a sample\nsample = train[0]\nprint(f\"Rudiment: {sample['rudiment_slug']}\")\nprint(f\"Overall Score: {sample['overall_score']:.1f}\")\n</code></pre>"},{"location":"getting-started/loading/#load-specific-split","title":"Load Specific Split","text":"<pre><code>from datasets import load_dataset\n\n# Load only the training split\ntrain = load_dataset(\"zkeown/sousa\", split=\"train\")\n\n# Load only validation\nval = load_dataset(\"zkeown/sousa\", split=\"validation\")\n\n# Load a subset (first 1000 samples)\nsubset = load_dataset(\"zkeown/sousa\", split=\"train[:1000]\")\n</code></pre>"},{"location":"getting-started/loading/#streaming-for-memory-efficiency","title":"Streaming for Memory Efficiency","text":"<p>For large-scale training or limited memory, use streaming mode:</p> <pre><code>from datasets import load_dataset\n\n# Stream the dataset (loads samples on-demand)\ndataset = load_dataset(\"zkeown/sousa\", streaming=True)\n\n# Iterate through samples\nfor sample in dataset[\"train\"]:\n    # Process sample\n    print(sample[\"rudiment_slug\"])\n    break  # Just show first sample\n\n# Take first N samples\nfrom itertools import islice\nfirst_100 = list(islice(dataset[\"train\"], 100))\n</code></pre> <p>When to Use Streaming</p> <ul> <li>Training on the full 100K dataset</li> <li>Limited RAM (&lt; 16 GB)</li> <li>Preprocessing on-the-fly</li> <li>Distributed training</li> </ul>"},{"location":"getting-started/loading/#load-specific-columns","title":"Load Specific Columns","text":"<p>Reduce memory by loading only needed columns:</p> <pre><code>from datasets import load_dataset\n\n# Load only audio and labels\ndataset = load_dataset(\n    \"zkeown/sousa\",\n    columns=[\"audio\", \"overall_score\", \"skill_tier\", \"rudiment_slug\"]\n)\n</code></pre>"},{"location":"getting-started/loading/#from-local-files","title":"From Local Files","text":"<p>If you generated the dataset locally, load it from the output directory.</p>"},{"location":"getting-started/loading/#using-pandas","title":"Using Pandas","text":"<pre><code>import pandas as pd\nfrom pathlib import Path\n\ndataset_dir = Path(\"output/dataset\")\n\n# Load the main samples table\nsamples_df = pd.read_parquet(dataset_dir / \"labels\" / \"samples.parquet\")\n\n# Load hierarchical labels\nexercises_df = pd.read_parquet(dataset_dir / \"labels\" / \"exercises.parquet\")\nmeasures_df = pd.read_parquet(dataset_dir / \"labels\" / \"measures.parquet\")\nstrokes_df = pd.read_parquet(dataset_dir / \"labels\" / \"strokes.parquet\")\n\nprint(f\"Samples: {len(samples_df)}\")\nprint(f\"Exercises: {len(exercises_df)}\")\nprint(f\"Measures: {len(measures_df)}\")\nprint(f\"Strokes: {len(strokes_df)}\")\n</code></pre>"},{"location":"getting-started/loading/#using-huggingface-datasets-local","title":"Using HuggingFace Datasets (Local)","text":"<p>Load local parquet files with the HuggingFace datasets library:</p> <pre><code>from datasets import load_dataset\n\n# Load from local parquet files\ndataset = load_dataset(\n    \"parquet\",\n    data_dir=\"./output/dataset/labels\",\n    split=\"train\"  # or specify a data_files dict\n)\n</code></pre>"},{"location":"getting-started/loading/#using-pyarrow","title":"Using PyArrow","text":"<p>For maximum performance with large datasets:</p> <pre><code>import pyarrow.parquet as pq\n\n# Read with PyArrow\ntable = pq.read_table(\"output/dataset/labels/samples.parquet\")\n\n# Convert to pandas if needed\ndf = table.to_pandas()\n\n# Or iterate over row groups for memory efficiency\nparquet_file = pq.ParquetFile(\"output/dataset/labels/samples.parquet\")\nfor batch in parquet_file.iter_batches(batch_size=1000):\n    df_batch = batch.to_pandas()\n    # Process batch\n</code></pre>"},{"location":"getting-started/loading/#accessing-audio","title":"Accessing Audio","text":""},{"location":"getting-started/loading/#from-huggingface-dataset","title":"From HuggingFace Dataset","text":"<p>Audio is automatically loaded as numpy arrays:</p> <pre><code>sample = dataset[\"train\"][0]\naudio = sample[\"audio\"]\n\n# Audio data\naudio_array = audio[\"array\"]       # NumPy array (float32)\nsample_rate = audio[\"sampling_rate\"]  # 44100 Hz\n\nprint(f\"Duration: {len(audio_array) / sample_rate:.2f} seconds\")\nprint(f\"Shape: {audio_array.shape}\")\n</code></pre>"},{"location":"getting-started/loading/#play-audio-in-jupyter","title":"Play Audio in Jupyter","text":"<pre><code>from IPython.display import Audio\n\nsample = dataset[\"train\"][0]\nAudio(sample[\"audio\"][\"array\"], rate=sample[\"audio\"][\"sampling_rate\"])\n</code></pre>"},{"location":"getting-started/loading/#save-audio-to-file","title":"Save Audio to File","text":"<pre><code>import soundfile as sf\n\nsample = dataset[\"train\"][0]\naudio = sample[\"audio\"]\n\nsf.write(\n    \"output_sample.wav\",\n    audio[\"array\"],\n    audio[\"sampling_rate\"]\n)\n</code></pre>"},{"location":"getting-started/loading/#from-local-files_1","title":"From Local Files","text":"<pre><code>import soundfile as sf\nfrom pathlib import Path\n\n# Load audio by sample ID\nsample_id = \"beg042_single_paradiddle_100bpm_marching_practicedry\"\naudio_path = Path(\"output/dataset/audio\") / f\"{sample_id}.flac\"\n\naudio_data, sample_rate = sf.read(audio_path)\nprint(f\"Loaded {len(audio_data)} samples at {sample_rate} Hz\")\n</code></pre>"},{"location":"getting-started/loading/#batch-audio-loading","title":"Batch Audio Loading","text":"<pre><code>import soundfile as sf\nfrom pathlib import Path\nfrom concurrent.futures import ThreadPoolExecutor\n\ndef load_audio(sample_id):\n    \"\"\"Load audio for a single sample.\"\"\"\n    audio_path = Path(\"output/dataset/audio\") / f\"{sample_id}.flac\"\n    if audio_path.exists():\n        return sf.read(audio_path)\n    return None, None\n\n# Load multiple audio files in parallel\nsample_ids = samples_df[\"sample_id\"].head(100).tolist()\n\nwith ThreadPoolExecutor(max_workers=4) as executor:\n    results = list(executor.map(load_audio, sample_ids))\n\naudio_data = [(data, sr) for data, sr in results if data is not None]\nprint(f\"Loaded {len(audio_data)} audio files\")\n</code></pre>"},{"location":"getting-started/loading/#accessing-midi","title":"Accessing MIDI","text":""},{"location":"getting-started/loading/#from-local-files_2","title":"From Local Files","text":"<pre><code>import mido\nfrom pathlib import Path\n\n# Load MIDI by sample ID\nsample_id = \"beg042_single_paradiddle_100bpm_marching_practicedry\"\nmidi_path = Path(\"output/dataset/midi\") / f\"{sample_id}.mid\"\n\nmid = mido.MidiFile(midi_path)\n\n# Get basic info\nprint(f\"Tracks: {len(mid.tracks)}\")\nprint(f\"Ticks per beat: {mid.ticks_per_beat}\")\n\n# Iterate through messages\nfor track in mid.tracks:\n    for msg in track:\n        if msg.type == \"note_on\" and msg.velocity &gt; 0:\n            print(f\"Note: {msg.note}, Velocity: {msg.velocity}, Time: {msg.time}\")\n</code></pre>"},{"location":"getting-started/loading/#extract-note-events","title":"Extract Note Events","text":"<pre><code>def extract_notes(midi_path):\n    \"\"\"Extract note events from MIDI file.\"\"\"\n    mid = mido.MidiFile(midi_path)\n    notes = []\n    current_time = 0\n\n    for track in mid.tracks:\n        for msg in track:\n            current_time += msg.time\n            if msg.type == \"note_on\" and msg.velocity &gt; 0:\n                notes.append({\n                    \"time\": current_time,\n                    \"note\": msg.note,\n                    \"velocity\": msg.velocity\n                })\n\n    return notes\n\nnotes = extract_notes(\"output/dataset/midi/sample.mid\")\nprint(f\"Total notes: {len(notes)}\")\n</code></pre>"},{"location":"getting-started/loading/#convert-midi-to-dataframe","title":"Convert MIDI to DataFrame","text":"<pre><code>import pandas as pd\nimport mido\n\ndef midi_to_dataframe(midi_path):\n    \"\"\"Convert MIDI file to pandas DataFrame.\"\"\"\n    mid = mido.MidiFile(midi_path)\n    events = []\n    current_time = 0\n\n    for track in mid.tracks:\n        for msg in track:\n            current_time += msg.time\n            if msg.type == \"note_on\":\n                events.append({\n                    \"time_ticks\": current_time,\n                    \"time_seconds\": mido.tick2second(\n                        current_time, mid.ticks_per_beat, 500000\n                    ),\n                    \"note\": msg.note,\n                    \"velocity\": msg.velocity,\n                    \"is_note_on\": msg.velocity &gt; 0\n                })\n\n    return pd.DataFrame(events)\n\ndf = midi_to_dataframe(\"output/dataset/midi/sample.mid\")\nprint(df.head())\n</code></pre>"},{"location":"getting-started/loading/#working-with-splits","title":"Working with Splits","text":""},{"location":"getting-started/loading/#load-split-assignments","title":"Load Split Assignments","text":"<pre><code>import json\n\nwith open(\"output/dataset/splits.json\") as f:\n    splits = json.load(f)\n\nprint(f\"Train profiles: {len(splits['train_profile_ids'])}\")\nprint(f\"Val profiles: {len(splits['val_profile_ids'])}\")\nprint(f\"Test profiles: {len(splits['test_profile_ids'])}\")\n</code></pre>"},{"location":"getting-started/loading/#filter-by-split","title":"Filter by Split","text":"<pre><code># Using pandas\ntrain_df = samples_df[samples_df[\"split\"] == \"train\"]\nval_df = samples_df[samples_df[\"split\"] == \"val\"]\ntest_df = samples_df[samples_df[\"split\"] == \"test\"]\n\n# Using HuggingFace datasets\ntrain = dataset.filter(lambda x: x[\"split\"] == \"train\")\n</code></pre>"},{"location":"getting-started/loading/#creating-pytorch-dataloaders","title":"Creating PyTorch DataLoaders","text":""},{"location":"getting-started/loading/#basic-dataloader","title":"Basic DataLoader","text":"<pre><code>import torch\nfrom torch.utils.data import Dataset, DataLoader\nimport soundfile as sf\nfrom pathlib import Path\n\nclass SOUSADataset(Dataset):\n    def __init__(self, samples_df, audio_dir, transform=None):\n        self.samples = samples_df.reset_index(drop=True)\n        self.audio_dir = Path(audio_dir)\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        row = self.samples.iloc[idx]\n\n        # Load audio\n        audio_path = self.audio_dir / f\"{row['sample_id']}.flac\"\n        audio, sr = sf.read(audio_path)\n\n        # Get label\n        label = row[\"overall_score\"] / 100.0  # Normalize to 0-1\n\n        if self.transform:\n            audio = self.transform(audio)\n\n        return torch.tensor(audio, dtype=torch.float32), torch.tensor(label)\n\n# Create dataset and dataloader\ntrain_dataset = SOUSADataset(train_df, \"output/dataset/audio\")\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)\n</code></pre>"},{"location":"getting-started/loading/#with-huggingface-datasets","title":"With HuggingFace Datasets","text":"<pre><code>from datasets import load_dataset\nfrom transformers import AutoFeatureExtractor\n\n# Load dataset\ndataset = load_dataset(\"zkeown/sousa\")\n\n# Load feature extractor\nfeature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\")\n\ndef preprocess(batch):\n    audio = batch[\"audio\"]\n    inputs = feature_extractor(\n        audio[\"array\"],\n        sampling_rate=audio[\"sampling_rate\"],\n        return_tensors=\"pt\",\n        padding=True,\n    )\n    inputs[\"labels\"] = batch[\"overall_score\"] / 100.0\n    return inputs\n\n# Apply preprocessing\ndataset = dataset.map(preprocess, remove_columns=[\"audio\"])\n\n# Convert to torch format\ndataset.set_format(\"torch\")\n\n# Use with DataLoader\ntrain_loader = DataLoader(dataset[\"train\"], batch_size=16, shuffle=True)\n</code></pre>"},{"location":"getting-started/loading/#caching-preprocessed-data","title":"Caching Preprocessed Data","text":"<p>For faster iteration during model development:</p> <pre><code>from datasets import load_dataset\n\n# Map with caching (persists across sessions)\ndataset = dataset.map(\n    preprocess_function,\n    cache_file_name=\"./cache/preprocessed_{split}\",\n    num_proc=4,  # Parallel processing\n)\n</code></pre>"},{"location":"getting-started/loading/#manual-feature-caching","title":"Manual Feature Caching","text":"<pre><code>import pickle\nfrom pathlib import Path\n\nCACHE_DIR = Path(\"./feature_cache\")\nCACHE_DIR.mkdir(exist_ok=True)\n\ndef get_cached_features(sample_id, audio_array, extract_fn):\n    \"\"\"Cache extracted features to disk.\"\"\"\n    cache_path = CACHE_DIR / f\"{sample_id}.pkl\"\n\n    if cache_path.exists():\n        with open(cache_path, \"rb\") as f:\n            return pickle.load(f)\n\n    features = extract_fn(audio_array)\n\n    with open(cache_path, \"wb\") as f:\n        pickle.dump(features, f)\n\n    return features\n</code></pre>"},{"location":"getting-started/loading/#memory-management","title":"Memory Management","text":""},{"location":"getting-started/loading/#for-large-datasets","title":"For Large Datasets","text":"<pre><code># Use streaming mode\ndataset = load_dataset(\"zkeown/sousa\", streaming=True)\n\n# Process in batches\ndef process_batch(batch):\n    # Your processing logic\n    return batch\n\n# Use batched mapping\ndataset = dataset.map(process_batch, batched=True, batch_size=100)\n</code></pre>"},{"location":"getting-started/loading/#reduce-memory-with-column-selection","title":"Reduce Memory with Column Selection","text":"<pre><code># Load only necessary columns\ndataset = load_dataset(\n    \"zkeown/sousa\",\n    columns=[\"audio\", \"overall_score\", \"skill_tier\"]\n)\n</code></pre>"},{"location":"getting-started/loading/#use-memory-mapping","title":"Use Memory Mapping","text":"<pre><code>from datasets import load_dataset\n\n# Keep data on disk, memory-map for access\ndataset = load_dataset(\"zkeown/sousa\", keep_in_memory=False)\n</code></pre>"},{"location":"getting-started/loading/#next-steps","title":"Next Steps","text":"<ul> <li>User Guide - ML task examples and best practices</li> <li>Data Format - Full schema documentation</li> <li>Limitations - Known constraints and recommendations</li> </ul>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>Generate and explore a SOUSA dataset in 5 minutes.</p> <p>Prerequisites</p> <p>This guide assumes you have completed the Installation steps, including:</p> <ul> <li>Python 3.10+ installed</li> <li>SOUSA installed (<code>pip install -e .</code>)</li> <li>Soundfonts downloaded (<code>python scripts/setup_soundfonts.py</code>)</li> <li>FluidSynth installed (for audio generation)</li> </ul>"},{"location":"getting-started/quickstart/#generate-a-test-dataset","title":"Generate a Test Dataset","text":"<p>Generate a small test dataset (~1,200 samples) to verify your setup:</p> <pre><code>python scripts/generate_dataset.py --preset small --with-audio\n</code></pre> Without Audio <p>If you don't have FluidSynth installed or want faster generation: <pre><code>python scripts/generate_dataset.py --preset small\n</code></pre> This generates MIDI files and labels only (no audio).</p> <p>Expected output:</p> <pre><code>============================================================\nDATASET GENERATION PLAN\n============================================================\nPreset: small - Quick testing (~1,200 samples)\n\nConfiguration:\n  10 player profiles\n  40 rudiments\n  3 tempos per rudiment\n  1 augmentations per sample\n\nTotal samples: 1,200\n\nEstimated storage:\n  MIDI files:  ~2.3 MB\n  Labels:      ~0.6 MB\n  Audio files: ~1.8 GB\n  TOTAL:       ~1.8 GB\n============================================================\n\nLoaded 40 rudiments\n...\nDataset saved to: output/dataset\nTotal time: X.X minutes\n</code></pre>"},{"location":"getting-started/quickstart/#explore-the-dataset","title":"Explore the Dataset","text":""},{"location":"getting-started/quickstart/#dataset-structure","title":"Dataset Structure","text":"<p>After generation, your dataset is organized as:</p> <pre><code>output/dataset/\n\u251c\u2500\u2500 midi/              # MIDI files (*.mid)\n\u251c\u2500\u2500 audio/             # Audio files (*.flac)\n\u251c\u2500\u2500 labels/            # Parquet files with labels\n\u2502   \u251c\u2500\u2500 samples.parquet\n\u2502   \u251c\u2500\u2500 exercises.parquet\n\u2502   \u251c\u2500\u2500 measures.parquet\n\u2502   \u2514\u2500\u2500 strokes.parquet\n\u251c\u2500\u2500 splits.json        # Train/val/test split assignments\n\u2514\u2500\u2500 validation_report.json\n</code></pre>"},{"location":"getting-started/quickstart/#load-samples-in-python","title":"Load Samples in Python","text":"<pre><code>import pandas as pd\nfrom pathlib import Path\n\n# Load the samples table\nsamples_df = pd.read_parquet(\"output/dataset/labels/samples.parquet\")\n\n# View basic info\nprint(f\"Total samples: {len(samples_df)}\")\nprint(f\"Columns: {list(samples_df.columns)}\")\nprint(samples_df.head())\n</code></pre> <p>Output:</p> <pre><code>Total samples: 1200\nColumns: ['sample_id', 'profile_id', 'rudiment_slug', 'tempo_bpm', 'soundfont',\n          'augmentation_preset', 'split', 'skill_tier', 'overall_score', ...]\n</code></pre>"},{"location":"getting-started/quickstart/#access-a-sample","title":"Access a Sample","text":"<pre><code># Get the first sample\nsample = samples_df.iloc[0]\n\nprint(f\"Sample ID: {sample['sample_id']}\")\nprint(f\"Rudiment: {sample['rudiment_slug']}\")\nprint(f\"Tempo: {sample['tempo_bpm']} BPM\")\nprint(f\"Skill Tier: {sample['skill_tier']}\")\nprint(f\"Overall Score: {sample['overall_score']:.1f}\")\n</code></pre>"},{"location":"getting-started/quickstart/#load-audio","title":"Load Audio","text":"<pre><code>import soundfile as sf\n\n# Get audio path from sample\naudio_path = Path(\"output/dataset/audio\") / f\"{sample['sample_id']}.flac\"\n\n# Load audio\naudio_data, sample_rate = sf.read(audio_path)\nprint(f\"Duration: {len(audio_data) / sample_rate:.2f} seconds\")\nprint(f\"Sample rate: {sample_rate} Hz\")\n</code></pre>"},{"location":"getting-started/quickstart/#load-midi","title":"Load MIDI","text":"<pre><code>import mido\n\n# Get MIDI path from sample\nmidi_path = Path(\"output/dataset/midi\") / f\"{sample['sample_id']}.mid\"\n\n# Load MIDI\nmid = mido.MidiFile(midi_path)\n\n# Count notes\nnote_count = sum(1 for track in mid.tracks for msg in track if msg.type == 'note_on')\nprint(f\"Total notes: {note_count}\")\n</code></pre>"},{"location":"getting-started/quickstart/#filter-samples","title":"Filter Samples","text":""},{"location":"getting-started/quickstart/#by-skill-level","title":"By Skill Level","text":"<pre><code># Filter to beginners only\nbeginners = samples_df[samples_df['skill_tier'] == 'beginner']\nprint(f\"Beginner samples: {len(beginners)}\")\n\n# Filter to advanced and professional\nadvanced = samples_df[samples_df['skill_tier'].isin(['advanced', 'professional'])]\nprint(f\"Advanced/Professional samples: {len(advanced)}\")\n</code></pre>"},{"location":"getting-started/quickstart/#by-rudiment","title":"By Rudiment","text":"<pre><code># Get all paradiddle variants\nparadiddles = samples_df[samples_df['rudiment_slug'].str.contains('paradiddle')]\nprint(f\"Paradiddle samples: {len(paradiddles)}\")\n\n# List unique rudiments\nrudiments = samples_df['rudiment_slug'].unique()\nprint(f\"Rudiments in dataset: {len(rudiments)}\")\n</code></pre>"},{"location":"getting-started/quickstart/#by-score","title":"By Score","text":"<pre><code># High performers (score &gt; 80)\nhigh_scores = samples_df[samples_df['overall_score'] &gt; 80]\nprint(f\"High score samples: {len(high_scores)}\")\n\n# Poor timing (timing_accuracy &lt; 50)\npoor_timing = samples_df[samples_df['timing_accuracy'] &lt; 50]\nprint(f\"Poor timing samples: {len(poor_timing)}\")\n</code></pre>"},{"location":"getting-started/quickstart/#by-split","title":"By Split","text":"<pre><code># Get training set\ntrain = samples_df[samples_df['split'] == 'train']\nval = samples_df[samples_df['split'] == 'val']\ntest = samples_df[samples_df['split'] == 'test']\n\nprint(f\"Train: {len(train)}, Val: {len(val)}, Test: {len(test)}\")\n</code></pre>"},{"location":"getting-started/quickstart/#view-validation-report","title":"View Validation Report","text":"<p>The generation process automatically validates the dataset:</p> <pre><code>import json\n\nwith open(\"output/dataset/validation_report.json\") as f:\n    report = json.load(f)\n\nprint(f\"All checks passed: {report['verification']['all_passed']}\")\nprint(f\"Total checks: {report['verification']['total_checks']}\")\n</code></pre>"},{"location":"getting-started/quickstart/#play-audio-jupyter","title":"Play Audio (Jupyter)","text":"<p>If you're in a Jupyter notebook:</p> <pre><code>from IPython.display import Audio\n\n# Load audio\naudio_path = Path(\"output/dataset/audio\") / f\"{sample['sample_id']}.flac\"\naudio_data, sr = sf.read(audio_path)\n\n# Play in notebook\nAudio(audio_data, rate=sr)\n</code></pre>"},{"location":"getting-started/quickstart/#quick-statistics","title":"Quick Statistics","text":"<pre><code># Score statistics by skill tier\nprint(samples_df.groupby('skill_tier')['overall_score'].describe())\n\n# Sample counts by rudiment\nprint(samples_df['rudiment_slug'].value_counts().head(10))\n\n# Tempo distribution\nprint(samples_df['tempo_bpm'].value_counts().sort_index())\n</code></pre>"},{"location":"getting-started/quickstart/#generation-presets","title":"Generation Presets","text":"<p>SOUSA provides three presets for different use cases:</p> Preset Use Case Samples Time Storage <code>small</code> Testing/debugging ~1,200 ~5 min ~1 GB <code>medium</code> Development ~12,000 ~30 min ~10 GB <code>full</code> Production ~100,000 ~4 hours ~97 GB <pre><code># Quick test\npython scripts/generate_dataset.py --preset small --with-audio\n\n# Development\npython scripts/generate_dataset.py --preset medium --with-audio\n\n# Full production dataset\npython scripts/generate_dataset.py --preset full --with-audio\n</code></pre>"},{"location":"getting-started/quickstart/#custom-configuration","title":"Custom Configuration","text":"<p>Override preset values for custom generation:</p> <pre><code># Custom profile count\npython scripts/generate_dataset.py --preset small --profiles 20 --with-audio\n\n# Custom tempo sampling\npython scripts/generate_dataset.py --preset small --tempos 5 --with-audio\n\n# Custom output directory\npython scripts/generate_dataset.py --preset small --output ./my_dataset --with-audio\n\n# Parallel generation (faster)\npython scripts/generate_dataset.py --preset medium --workers 4 --with-audio\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<p>Now that you have a dataset:</p> <ol> <li>Loading Data - Load from HuggingFace or local files</li> <li>User Guide - Detailed ML task examples</li> <li>Data Format - Full schema documentation</li> <li>Validation - Quality assurance details</li> </ol>"},{"location":"plans/2025-01-27-hf-upload-tar-sharding-design/","title":"HuggingFace Upload: TAR Sharding Design","text":"<p>Date: 2025-01-27 Status: Approved Problem: Current upload script tries to push 240k individual files, exceeding HuggingFace's 100k file limit and causing rate limiting (128 commits/hour)</p>"},{"location":"plans/2025-01-27-hf-upload-tar-sharding-design/#solution","title":"Solution","text":"<p>Bundle audio/MIDI files into sharded TAR archives, reducing total file count to ~100.</p>"},{"location":"plans/2025-01-27-hf-upload-tar-sharding-design/#archive-structure","title":"Archive Structure","text":"<pre><code>output/dataset/hf_staging/\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 data/\n\u2502   \u251c\u2500\u2500 train.parquet\n\u2502   \u251c\u2500\u2500 validation.parquet\n\u2502   \u2514\u2500\u2500 test.parquet\n\u251c\u2500\u2500 audio/\n\u2502   \u251c\u2500\u2500 train-00000.tar\n\u2502   \u251c\u2500\u2500 train-00001.tar\n\u2502   \u251c\u2500\u2500 ... (~96 shards at 1GB each)\n\u2502   \u251c\u2500\u2500 validation-00000.tar\n\u2502   \u2514\u2500\u2500 test-00000.tar\n\u2514\u2500\u2500 midi/\n    \u251c\u2500\u2500 train.tar\n    \u251c\u2500\u2500 validation.tar\n    \u2514\u2500\u2500 test.tar\n</code></pre>"},{"location":"plans/2025-01-27-hf-upload-tar-sharding-design/#parquet-schema-changes","title":"Parquet Schema Changes","text":"<p>Current columns: - <code>audio_path</code>: <code>\"audio/adv000_double_drag_tap_112bpm_douglasn_practiceroom.flac\"</code> - <code>midi_path</code>: <code>\"midi/adv000_double_drag_tap_112bpm.mid\"</code></p> <p>New columns: - <code>audio_shard</code>: <code>\"train-00042.tar\"</code> - <code>audio_filename</code>: <code>\"adv000_double_drag_tap_112bpm_douglasn_practiceroom.flac\"</code> - <code>midi_shard</code>: <code>\"train.tar\"</code> - <code>midi_filename</code>: <code>\"adv000_double_drag_tap_112bpm.mid\"</code></p>"},{"location":"plans/2025-01-27-hf-upload-tar-sharding-design/#implementation","title":"Implementation","text":""},{"location":"plans/2025-01-27-hf-upload-tar-sharding-design/#new-file-dataset_genhubarchiverpy","title":"New File: <code>dataset_gen/hub/archiver.py</code>","text":"<pre><code>@dataclass\nclass ShardInfo:\n    shard_name: str\n    filename: str\n\ndef create_sharded_archives(\n    source_dir: Path,\n    output_dir: Path,\n    filenames_by_split: dict[str, list[str]],  # split -&gt; list of filenames\n    target_shard_size_bytes: int = 1_000_000_000,  # 1GB\n    extension: str = \"flac\",\n) -&gt; dict[str, ShardInfo]:\n    \"\"\"\n    Create TAR archives from source files, sharded by size.\n\n    Args:\n        source_dir: Directory containing source files\n        output_dir: Directory to write TAR archives\n        filenames_by_split: Mapping of split name to list of filenames\n        target_shard_size_bytes: Target size per shard\n        extension: File extension to process\n\n    Returns:\n        Mapping of original filename to ShardInfo(shard_name, filename)\n    \"\"\"\n</code></pre> <p>Key behaviors: - Groups files by split first - Creates shards when accumulated size exceeds target - Returns mapping for parquet updates - Uses deterministic ordering for reproducibility</p>"},{"location":"plans/2025-01-27-hf-upload-tar-sharding-design/#changes-to-dataset_genhubuploaderpy","title":"Changes to <code>dataset_gen/hub/uploader.py</code>","text":"<ol> <li> <p>Add <code>use_tar_shards: bool = True</code> to <code>HubConfig</code></p> </li> <li> <p>Replace <code>_copy_media_files()</code> logic: <pre><code>def _create_media_archives(self, media_type: str, extension: str) -&gt; dict[str, ShardInfo]:\n    \"\"\"Create sharded TAR archives for audio or MIDI files.\"\"\"\n    # Get filenames grouped by split from the merged dataframe\n    # Call create_sharded_archives()\n    # Return shard mapping\n</code></pre></p> </li> <li> <p>Update <code>_merge_dataframes()</code> to add shard columns: <pre><code># After creating archives:\nmerged_df[\"audio_shard\"] = merged_df[\"sample_id\"].map(\n    lambda sid: audio_shard_map[sid].shard_name\n)\nmerged_df[\"audio_filename\"] = merged_df[\"sample_id\"].map(\n    lambda sid: audio_shard_map[sid].filename\n)\n</code></pre></p> </li> <li> <p>Keep <code>upload_large_folder()</code> - works fine with ~100 files</p> </li> </ol>"},{"location":"plans/2025-01-27-hf-upload-tar-sharding-design/#changes-to-scriptspush_to_hubpy","title":"Changes to <code>scripts/push_to_hub.py</code>","text":"<p>Add flag to disable sharding (for testing): <pre><code>parser.add_argument(\n    \"--no-sharding\",\n    action=\"store_true\",\n    help=\"Disable TAR sharding (not recommended for large datasets)\",\n)\n</code></pre></p>"},{"location":"plans/2025-01-27-hf-upload-tar-sharding-design/#consumer-usage","title":"Consumer Usage","text":""},{"location":"plans/2025-01-27-hf-upload-tar-sharding-design/#basic-with-helper","title":"Basic (with helper)","text":"<pre><code>from datasets import load_dataset\nfrom huggingface_hub import hf_hub_download\nimport tarfile\nimport soundfile as sf\nimport io\n\ndef load_audio(sample, repo_id=\"zkeown/sousa\"):\n    \"\"\"Load audio from a dataset sample.\"\"\"\n    shard_path = hf_hub_download(\n        repo_id,\n        f\"audio/{sample['audio_shard']}\",\n        repo_type=\"dataset\"\n    )\n    with tarfile.open(shard_path) as tar:\n        audio_bytes = tar.extractfile(sample[\"audio_filename\"]).read()\n        return sf.read(io.BytesIO(audio_bytes))\n\nds = load_dataset(\"zkeown/sousa\")\naudio, sr = load_audio(ds[\"train\"][0])\n</code></pre>"},{"location":"plans/2025-01-27-hf-upload-tar-sharding-design/#batch-processing-extract-shards","title":"Batch Processing (extract shards)","text":"<pre><code># Download and extract all training audio\nfor shard in ds[\"train\"][\"audio_shard\"].unique():\n    shard_path = hf_hub_download(\"zkeown/sousa\", f\"audio/{shard}\", repo_type=\"dataset\")\n    with tarfile.open(shard_path) as tar:\n        tar.extractall(\"./audio/\")\n</code></pre>"},{"location":"plans/2025-01-27-hf-upload-tar-sharding-design/#file-count-estimate","title":"File Count Estimate","text":"Content Files Parquet (3 splits) 3 Audio shards (~96GB / 1GB) ~96 Validation audio ~1-2 Test audio ~1-2 MIDI (3 splits, small) 3 README 1 Total ~106 <p>Well under 100k limit.</p>"},{"location":"plans/2025-01-27-hf-upload-tar-sharding-design/#testing","title":"Testing","text":"<ol> <li>Unit tests for <code>archiver.py</code>:</li> <li>Shard size targeting</li> <li>Split separation</li> <li> <p>Deterministic output</p> </li> <li> <p>Integration test:</p> </li> <li>Generate small dataset</li> <li>Run upload with <code>--dry-run</code></li> <li>Verify archive structure</li> <li>Verify parquet shard columns</li> </ol>"},{"location":"plans/2025-01-27-hf-upload-tar-sharding-design/#migration","title":"Migration","text":"<p>No migration needed - this changes the upload format, not the generation format. Existing generated datasets work with the new uploader.</p>"},{"location":"plans/2025-01-27-tar-sharding-implementation/","title":"TAR Sharding Implementation Plan","text":"<p>For Claude: REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.</p> <p>Goal: Bundle audio/MIDI files into sharded TAR archives to reduce file count from 240k to ~100 for HuggingFace upload.</p> <p>Architecture: New <code>archiver.py</code> module creates TAR shards grouped by split. Modified <code>uploader.py</code> calls archiver instead of copying files, then updates parquet with shard references.</p> <p>Tech Stack: Python tarfile (stdlib), pandas, existing hub module</p>"},{"location":"plans/2025-01-27-tar-sharding-implementation/#task-1-create-archiver-module-with-shardinfo-dataclass","title":"Task 1: Create archiver module with ShardInfo dataclass","text":"<p>Files: - Create: <code>dataset_gen/hub/archiver.py</code> - Test: <code>tests/test_hub_archiver.py</code></p> <p>Step 1: Write the failing test for ShardInfo</p> <pre><code># tests/test_hub_archiver.py\n\"\"\"Tests for TAR sharding archiver.\"\"\"\n\nimport pytest\nfrom dataset_gen.hub.archiver import ShardInfo\n\n\nclass TestShardInfo:\n    \"\"\"Tests for ShardInfo dataclass.\"\"\"\n\n    def test_shard_info_creation(self):\n        \"\"\"ShardInfo stores shard name and filename.\"\"\"\n        info = ShardInfo(shard_name=\"train-00001.tar\", filename=\"sample_001.flac\")\n        assert info.shard_name == \"train-00001.tar\"\n        assert info.filename == \"sample_001.flac\"\n</code></pre> <p>Step 2: Run test to verify it fails</p> <p>Run: <code>pytest tests/test_hub_archiver.py::TestShardInfo::test_shard_info_creation -v</code> Expected: FAIL with \"ModuleNotFoundError: No module named 'dataset_gen.hub.archiver'\"</p> <p>Step 3: Write minimal implementation</p> <pre><code># dataset_gen/hub/archiver.py\n\"\"\"\nTAR archive creation for HuggingFace Hub uploads.\n\nCreates sharded TAR archives from audio/MIDI files, grouped by split,\nto reduce total file count for HuggingFace's 100k file limit.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\n\n\n@dataclass\nclass ShardInfo:\n    \"\"\"Information about which shard contains a file.\"\"\"\n\n    shard_name: str  # e.g., \"train-00001.tar\"\n    filename: str  # e.g., \"sample_001.flac\"\n</code></pre> <p>Step 4: Run test to verify it passes</p> <p>Run: <code>pytest tests/test_hub_archiver.py::TestShardInfo::test_shard_info_creation -v</code> Expected: PASS</p> <p>Step 5: Commit</p> <pre><code>git add dataset_gen/hub/archiver.py tests/test_hub_archiver.py\ngit commit -m \"feat(hub): add ShardInfo dataclass for TAR sharding\"\n</code></pre>"},{"location":"plans/2025-01-27-tar-sharding-implementation/#task-2-add-create_sharded_archives-function-signature-and-basic-test","title":"Task 2: Add create_sharded_archives function signature and basic test","text":"<p>Files: - Modify: <code>dataset_gen/hub/archiver.py</code> - Modify: <code>tests/test_hub_archiver.py</code></p> <p>Step 1: Write the failing test</p> <pre><code># Add to tests/test_hub_archiver.py\nimport tempfile\nfrom pathlib import Path\n\nfrom dataset_gen.hub.archiver import ShardInfo, create_sharded_archives\n\n\nclass TestCreateShardedArchives:\n    \"\"\"Tests for create_sharded_archives function.\"\"\"\n\n    def test_creates_single_shard_for_small_files(self, tmp_path):\n        \"\"\"Small files all go into one shard.\"\"\"\n        # Create source files\n        src_dir = tmp_path / \"audio\"\n        src_dir.mkdir()\n        for i in range(3):\n            (src_dir / f\"sample_{i:03d}.flac\").write_bytes(b\"x\" * 100)\n\n        out_dir = tmp_path / \"output\"\n        out_dir.mkdir()\n\n        filenames_by_split = {\n            \"train\": [f\"sample_{i:03d}.flac\" for i in range(3)]\n        }\n\n        result = create_sharded_archives(\n            source_dir=src_dir,\n            output_dir=out_dir,\n            filenames_by_split=filenames_by_split,\n            target_shard_size_bytes=10000,  # 10KB - larger than our test files\n            extension=\"flac\",\n        )\n\n        # Should create one shard\n        assert (out_dir / \"train-00000.tar\").exists()\n        assert not (out_dir / \"train-00001.tar\").exists()\n\n        # All files mapped to that shard\n        for i in range(3):\n            filename = f\"sample_{i:03d}.flac\"\n            assert filename in result\n            assert result[filename].shard_name == \"train-00000.tar\"\n            assert result[filename].filename == filename\n</code></pre> <p>Step 2: Run test to verify it fails</p> <p>Run: <code>pytest tests/test_hub_archiver.py::TestCreateShardedArchives::test_creates_single_shard_for_small_files -v</code> Expected: FAIL with \"cannot import name 'create_sharded_archives'\"</p> <p>Step 3: Write minimal implementation</p> <pre><code># Add to dataset_gen/hub/archiver.py after ShardInfo class\n\nimport logging\nimport tarfile\nfrom pathlib import Path\n\nlogger = logging.getLogger(__name__)\n\n\ndef create_sharded_archives(\n    source_dir: Path,\n    output_dir: Path,\n    filenames_by_split: dict[str, list[str]],\n    target_shard_size_bytes: int = 1_000_000_000,  # 1GB\n    extension: str = \"flac\",\n) -&gt; dict[str, ShardInfo]:\n    \"\"\"\n    Create TAR archives from source files, sharded by size.\n\n    Args:\n        source_dir: Directory containing source files\n        output_dir: Directory to write TAR archives\n        filenames_by_split: Mapping of split name to list of filenames\n        target_shard_size_bytes: Target size per shard (default 1GB)\n        extension: File extension being processed (for logging)\n\n    Returns:\n        Mapping of original filename to ShardInfo(shard_name, filename)\n    \"\"\"\n    output_dir.mkdir(parents=True, exist_ok=True)\n    result: dict[str, ShardInfo] = {}\n\n    for split_name, filenames in filenames_by_split.items():\n        shard_index = 0\n        current_shard_size = 0\n        current_shard_path = output_dir / f\"{split_name}-{shard_index:05d}.tar\"\n        current_tar = tarfile.open(current_shard_path, \"w\")\n\n        for filename in sorted(filenames):  # Sort for determinism\n            src_file = source_dir / filename\n            if not src_file.exists():\n                logger.warning(f\"Source file not found: {src_file}\")\n                continue\n\n            file_size = src_file.stat().st_size\n\n            # Check if we need a new shard (but always put at least one file per shard)\n            if current_shard_size &gt; 0 and current_shard_size + file_size &gt; target_shard_size_bytes:\n                current_tar.close()\n                logger.info(\n                    f\"Closed {current_shard_path.name} with {current_shard_size / (1024**2):.1f} MB\"\n                )\n                shard_index += 1\n                current_shard_size = 0\n                current_shard_path = output_dir / f\"{split_name}-{shard_index:05d}.tar\"\n                current_tar = tarfile.open(current_shard_path, \"w\")\n\n            # Add file to current shard\n            current_tar.add(src_file, arcname=filename)\n            current_shard_size += file_size\n\n            shard_name = f\"{split_name}-{shard_index:05d}.tar\"\n            result[filename] = ShardInfo(shard_name=shard_name, filename=filename)\n\n        # Close final shard\n        current_tar.close()\n        if current_shard_size &gt; 0:\n            logger.info(\n                f\"Closed {current_shard_path.name} with {current_shard_size / (1024**2):.1f} MB\"\n            )\n\n    return result\n</code></pre> <p>Step 4: Run test to verify it passes</p> <p>Run: <code>pytest tests/test_hub_archiver.py::TestCreateShardedArchives::test_creates_single_shard_for_small_files -v</code> Expected: PASS</p> <p>Step 5: Commit</p> <pre><code>git add dataset_gen/hub/archiver.py tests/test_hub_archiver.py\ngit commit -m \"feat(hub): add create_sharded_archives function\"\n</code></pre>"},{"location":"plans/2025-01-27-tar-sharding-implementation/#task-3-test-multiple-shards-are-created-when-size-exceeded","title":"Task 3: Test multiple shards are created when size exceeded","text":"<p>Files: - Modify: <code>tests/test_hub_archiver.py</code></p> <p>Step 1: Write the test</p> <pre><code># Add to TestCreateShardedArchives class in tests/test_hub_archiver.py\n\n    def test_creates_multiple_shards_when_size_exceeded(self, tmp_path):\n        \"\"\"Files are split across shards when target size exceeded.\"\"\"\n        src_dir = tmp_path / \"audio\"\n        src_dir.mkdir()\n\n        # Create 5 files of 100 bytes each\n        for i in range(5):\n            (src_dir / f\"sample_{i:03d}.flac\").write_bytes(b\"x\" * 100)\n\n        out_dir = tmp_path / \"output\"\n        out_dir.mkdir()\n\n        filenames_by_split = {\n            \"train\": [f\"sample_{i:03d}.flac\" for i in range(5)]\n        }\n\n        # Target 250 bytes per shard - should fit 2 files each (with TAR overhead, may vary)\n        result = create_sharded_archives(\n            source_dir=src_dir,\n            output_dir=out_dir,\n            filenames_by_split=filenames_by_split,\n            target_shard_size_bytes=250,\n            extension=\"flac\",\n        )\n\n        # Should have multiple shards\n        shards = list(out_dir.glob(\"train-*.tar\"))\n        assert len(shards) &gt;= 2, f\"Expected multiple shards, got {len(shards)}\"\n\n        # All files should be mapped\n        assert len(result) == 5\n</code></pre> <p>Step 2: Run test</p> <p>Run: <code>pytest tests/test_hub_archiver.py::TestCreateShardedArchives::test_creates_multiple_shards_when_size_exceeded -v</code> Expected: PASS (implementation already handles this)</p> <p>Step 3: Commit if needed</p> <pre><code>git add tests/test_hub_archiver.py\ngit commit -m \"test(hub): add test for multiple shard creation\"\n</code></pre>"},{"location":"plans/2025-01-27-tar-sharding-implementation/#task-4-test-separate-shards-per-split","title":"Task 4: Test separate shards per split","text":"<p>Files: - Modify: <code>tests/test_hub_archiver.py</code></p> <p>Step 1: Write the test</p> <pre><code># Add to TestCreateShardedArchives class\n\n    def test_separate_shards_per_split(self, tmp_path):\n        \"\"\"Each split gets its own shard files.\"\"\"\n        src_dir = tmp_path / \"audio\"\n        src_dir.mkdir()\n\n        # Create files for train and validation\n        for i in range(3):\n            (src_dir / f\"train_{i:03d}.flac\").write_bytes(b\"x\" * 100)\n            (src_dir / f\"val_{i:03d}.flac\").write_bytes(b\"x\" * 100)\n\n        out_dir = tmp_path / \"output\"\n        out_dir.mkdir()\n\n        filenames_by_split = {\n            \"train\": [f\"train_{i:03d}.flac\" for i in range(3)],\n            \"validation\": [f\"val_{i:03d}.flac\" for i in range(3)],\n        }\n\n        result = create_sharded_archives(\n            source_dir=src_dir,\n            output_dir=out_dir,\n            filenames_by_split=filenames_by_split,\n            target_shard_size_bytes=10000,\n            extension=\"flac\",\n        )\n\n        # Should have separate shards for each split\n        assert (out_dir / \"train-00000.tar\").exists()\n        assert (out_dir / \"validation-00000.tar\").exists()\n\n        # Verify files are in correct shards\n        assert result[\"train_000.flac\"].shard_name == \"train-00000.tar\"\n        assert result[\"val_000.flac\"].shard_name == \"validation-00000.tar\"\n</code></pre> <p>Step 2: Run test</p> <p>Run: <code>pytest tests/test_hub_archiver.py::TestCreateShardedArchives::test_separate_shards_per_split -v</code> Expected: PASS</p> <p>Step 3: Commit</p> <pre><code>git add tests/test_hub_archiver.py\ngit commit -m \"test(hub): add test for split separation in shards\"\n</code></pre>"},{"location":"plans/2025-01-27-tar-sharding-implementation/#task-5-test-tar-contents-are-correct","title":"Task 5: Test TAR contents are correct","text":"<p>Files: - Modify: <code>tests/test_hub_archiver.py</code></p> <p>Step 1: Write the test</p> <pre><code># Add to TestCreateShardedArchives class\n\n    def test_tar_contains_correct_files(self, tmp_path):\n        \"\"\"TAR archives contain the source files with correct content.\"\"\"\n        src_dir = tmp_path / \"audio\"\n        src_dir.mkdir()\n\n        # Create files with distinct content\n        (src_dir / \"a.flac\").write_bytes(b\"content_a\")\n        (src_dir / \"b.flac\").write_bytes(b\"content_b\")\n\n        out_dir = tmp_path / \"output\"\n        out_dir.mkdir()\n\n        filenames_by_split = {\"train\": [\"a.flac\", \"b.flac\"]}\n\n        create_sharded_archives(\n            source_dir=src_dir,\n            output_dir=out_dir,\n            filenames_by_split=filenames_by_split,\n            target_shard_size_bytes=10000,\n            extension=\"flac\",\n        )\n\n        # Verify TAR contents\n        import tarfile\n\n        with tarfile.open(out_dir / \"train-00000.tar\") as tar:\n            names = tar.getnames()\n            assert \"a.flac\" in names\n            assert \"b.flac\" in names\n\n            # Check content\n            a_content = tar.extractfile(\"a.flac\").read()\n            assert a_content == b\"content_a\"\n</code></pre> <p>Step 2: Run test</p> <p>Run: <code>pytest tests/test_hub_archiver.py::TestCreateShardedArchives::test_tar_contains_correct_files -v</code> Expected: PASS</p> <p>Step 3: Commit</p> <pre><code>git add tests/test_hub_archiver.py\ngit commit -m \"test(hub): add test verifying TAR contents\"\n</code></pre>"},{"location":"plans/2025-01-27-tar-sharding-implementation/#task-6-export-archiver-from-hub-module","title":"Task 6: Export archiver from hub module","text":"<p>Files: - Modify: <code>dataset_gen/hub/__init__.py</code></p> <p>Step 1: Write the failing test</p> <pre><code># Add to tests/test_hub_archiver.py at top level\n\ndef test_archiver_exported_from_hub_module():\n    \"\"\"Archiver functions are exported from hub module.\"\"\"\n    from dataset_gen.hub import ShardInfo, create_sharded_archives\n\n    assert ShardInfo is not None\n    assert create_sharded_archives is not None\n</code></pre> <p>Step 2: Run test to verify it fails</p> <p>Run: <code>pytest tests/test_hub_archiver.py::test_archiver_exported_from_hub_module -v</code> Expected: FAIL with \"cannot import name 'ShardInfo' from 'dataset_gen.hub'\"</p> <p>Step 3: Update init.py</p> <pre><code># dataset_gen/hub/__init__.py\n\"\"\"\nHuggingFace Hub integration for SOUSA dataset.\n\nThis module provides utilities to:\n- Prepare the dataset for HuggingFace Hub format\n- Upload the dataset to HuggingFace Hub\n- Generate consolidated parquet files for efficient loading\n- Create sharded TAR archives for large media files\n\"\"\"\n\nfrom dataset_gen.hub.archiver import (\n    ShardInfo,\n    create_sharded_archives,\n)\nfrom dataset_gen.hub.uploader import (\n    HubConfig,\n    DatasetUploader,\n    prepare_hf_structure,\n    push_to_hub,\n)\n\n__all__ = [\n    # Archiver\n    \"ShardInfo\",\n    \"create_sharded_archives\",\n    # Uploader\n    \"HubConfig\",\n    \"DatasetUploader\",\n    \"prepare_hf_structure\",\n    \"push_to_hub\",\n]\n</code></pre> <p>Step 4: Run test to verify it passes</p> <p>Run: <code>pytest tests/test_hub_archiver.py::test_archiver_exported_from_hub_module -v</code> Expected: PASS</p> <p>Step 5: Commit</p> <pre><code>git add dataset_gen/hub/__init__.py\ngit commit -m \"feat(hub): export archiver from hub module\"\n</code></pre>"},{"location":"plans/2025-01-27-tar-sharding-implementation/#task-7-add-use_tar_shards-option-to-hubconfig","title":"Task 7: Add use_tar_shards option to HubConfig","text":"<p>Files: - Modify: <code>dataset_gen/hub/uploader.py</code> - Create: <code>tests/test_hub_uploader.py</code></p> <p>Step 1: Write the failing test</p> <pre><code># tests/test_hub_uploader.py\n\"\"\"Tests for HuggingFace Hub uploader.\"\"\"\n\nimport pytest\nfrom pathlib import Path\n\nfrom dataset_gen.hub.uploader import HubConfig\n\n\nclass TestHubConfig:\n    \"\"\"Tests for HubConfig dataclass.\"\"\"\n\n    def test_use_tar_shards_default_true(self, tmp_path):\n        \"\"\"use_tar_shards defaults to True.\"\"\"\n        config = HubConfig(\n            dataset_dir=tmp_path,\n            repo_id=\"test/repo\",\n        )\n        assert config.use_tar_shards is True\n\n    def test_use_tar_shards_can_be_disabled(self, tmp_path):\n        \"\"\"use_tar_shards can be set to False.\"\"\"\n        config = HubConfig(\n            dataset_dir=tmp_path,\n            repo_id=\"test/repo\",\n            use_tar_shards=False,\n        )\n        assert config.use_tar_shards is False\n\n    def test_tar_shard_size_default(self, tmp_path):\n        \"\"\"tar_shard_size_bytes has sensible default.\"\"\"\n        config = HubConfig(\n            dataset_dir=tmp_path,\n            repo_id=\"test/repo\",\n        )\n        assert config.tar_shard_size_bytes == 1_000_000_000  # 1GB\n</code></pre> <p>Step 2: Run test to verify it fails</p> <p>Run: <code>pytest tests/test_hub_uploader.py::TestHubConfig -v</code> Expected: FAIL with \"unexpected keyword argument 'use_tar_shards'\"</p> <p>Step 3: Update HubConfig</p> <pre><code># In dataset_gen/hub/uploader.py, modify HubConfig dataclass:\n\n@dataclass\nclass HubConfig:\n    \"\"\"Configuration for HuggingFace Hub upload.\"\"\"\n\n    # Source dataset directory\n    dataset_dir: Path\n\n    # Hub settings\n    repo_id: str\n    private: bool = False\n    token: str | None = None\n\n    # Content options\n    include_audio: bool = True\n    include_midi: bool = True\n\n    # Output staging directory (where HF-format files are prepared)\n    staging_dir: Path | None = None\n\n    # Shard settings for large datasets\n    max_shard_size: str = \"500MB\"\n\n    # TAR sharding options (for staying under HuggingFace 100k file limit)\n    use_tar_shards: bool = True\n    tar_shard_size_bytes: int = 1_000_000_000  # 1GB per shard\n\n    def __post_init__(self):\n        self.dataset_dir = Path(self.dataset_dir)\n        if self.staging_dir is None:\n            self.staging_dir = self.dataset_dir / \"hf_staging\"\n        else:\n            self.staging_dir = Path(self.staging_dir)\n</code></pre> <p>Step 4: Run test to verify it passes</p> <p>Run: <code>pytest tests/test_hub_uploader.py::TestHubConfig -v</code> Expected: PASS</p> <p>Step 5: Commit</p> <pre><code>git add dataset_gen/hub/uploader.py tests/test_hub_uploader.py\ngit commit -m \"feat(hub): add use_tar_shards option to HubConfig\"\n</code></pre>"},{"location":"plans/2025-01-27-tar-sharding-implementation/#task-8-add-_get_filenames_by_split-helper-method","title":"Task 8: Add _get_filenames_by_split helper method","text":"<p>Files: - Modify: <code>dataset_gen/hub/uploader.py</code> - Modify: <code>tests/test_hub_uploader.py</code></p> <p>Step 1: Write the failing test</p> <pre><code># Add to tests/test_hub_uploader.py\n\nimport json\nimport pandas as pd\nfrom dataset_gen.hub.uploader import HubConfig, DatasetUploader\n\n\nclass TestDatasetUploaderHelpers:\n    \"\"\"Tests for DatasetUploader helper methods.\"\"\"\n\n    @pytest.fixture\n    def sample_dataset(self, tmp_path):\n        \"\"\"Create a minimal dataset structure for testing.\"\"\"\n        # Create labels directory\n        labels_dir = tmp_path / \"labels\"\n        labels_dir.mkdir()\n\n        # Create samples parquet\n        samples_df = pd.DataFrame({\n            \"sample_id\": [\"s1\", \"s2\", \"s3\", \"s4\"],\n            \"profile_id\": [\"p1\", \"p1\", \"p2\", \"p3\"],\n            \"audio_path\": [\"audio/a1.flac\", \"audio/a2.flac\", \"audio/a3.flac\", \"audio/a4.flac\"],\n            \"midi_path\": [\"midi/m1.mid\", \"midi/m2.mid\", \"midi/m3.mid\", \"midi/m4.mid\"],\n        })\n        samples_df.to_parquet(labels_dir / \"samples.parquet\")\n\n        # Create exercises parquet (minimal)\n        exercises_df = pd.DataFrame({\n            \"sample_id\": [\"s1\", \"s2\", \"s3\", \"s4\"],\n            \"overall_score\": [80.0, 85.0, 75.0, 90.0],\n        })\n        exercises_df.to_parquet(labels_dir / \"exercises.parquet\")\n\n        # Create splits\n        splits = {\n            \"train_profile_ids\": [\"p1\"],\n            \"val_profile_ids\": [\"p2\"],\n            \"test_profile_ids\": [\"p3\"],\n        }\n        with open(tmp_path / \"splits.json\", \"w\") as f:\n            json.dump(splits, f)\n\n        return tmp_path\n\n    def test_get_filenames_by_split_audio(self, sample_dataset):\n        \"\"\"_get_filenames_by_split returns audio files grouped by split.\"\"\"\n        config = HubConfig(dataset_dir=sample_dataset, repo_id=\"test/repo\")\n        uploader = DatasetUploader(config)\n\n        result = uploader._get_filenames_by_split(\"audio\")\n\n        assert \"train\" in result\n        assert \"validation\" in result\n        assert \"test\" in result\n\n        # p1 has 2 samples -&gt; train\n        assert set(result[\"train\"]) == {\"a1.flac\", \"a2.flac\"}\n        # p2 has 1 sample -&gt; validation\n        assert set(result[\"validation\"]) == {\"a3.flac\"}\n        # p3 has 1 sample -&gt; test\n        assert set(result[\"test\"]) == {\"a4.flac\"}\n</code></pre> <p>Step 2: Run test to verify it fails</p> <p>Run: <code>pytest tests/test_hub_uploader.py::TestDatasetUploaderHelpers::test_get_filenames_by_split_audio -v</code> Expected: FAIL with \"'DatasetUploader' object has no attribute '_get_filenames_by_split'\"</p> <p>Step 3: Add the helper method to DatasetUploader</p> <pre><code># Add to DatasetUploader class in dataset_gen/hub/uploader.py\n\n    def _get_filenames_by_split(self, media_type: str) -&gt; dict[str, list[str]]:\n        \"\"\"\n        Get filenames grouped by split for a media type.\n\n        Args:\n            media_type: Either \"audio\" or \"midi\"\n\n        Returns:\n            Dict mapping split name to list of filenames\n        \"\"\"\n        # Load source data\n        samples_df = pd.read_parquet(self.config.dataset_dir / \"labels\" / \"samples.parquet\")\n\n        # Load splits\n        with open(self.config.dataset_dir / \"splits.json\") as f:\n            splits = json.load(f)\n\n        # Determine path column\n        path_col = f\"{media_type}_path\"\n        if path_col not in samples_df.columns:\n            return {\"train\": [], \"validation\": [], \"test\": []}\n\n        result: dict[str, list[str]] = {\"train\": [], \"validation\": [], \"test\": []}\n\n        for _, row in samples_df.iterrows():\n            path = row[path_col]\n            if pd.isna(path) or not path:\n                continue\n\n            filename = Path(path).name\n            profile_id = row[\"profile_id\"]\n\n            if profile_id in splits.get(\"train_profile_ids\", []):\n                result[\"train\"].append(filename)\n            elif profile_id in splits.get(\"val_profile_ids\", []):\n                result[\"validation\"].append(filename)\n            elif profile_id in splits.get(\"test_profile_ids\", []):\n                result[\"test\"].append(filename)\n            else:\n                result[\"train\"].append(filename)  # Default to train\n\n        return result\n</code></pre> <p>Step 4: Run test to verify it passes</p> <p>Run: <code>pytest tests/test_hub_uploader.py::TestDatasetUploaderHelpers::test_get_filenames_by_split_audio -v</code> Expected: PASS</p> <p>Step 5: Commit</p> <pre><code>git add dataset_gen/hub/uploader.py tests/test_hub_uploader.py\ngit commit -m \"feat(hub): add _get_filenames_by_split helper\"\n</code></pre>"},{"location":"plans/2025-01-27-tar-sharding-implementation/#task-9-add-_create_media_archives-method","title":"Task 9: Add _create_media_archives method","text":"<p>Files: - Modify: <code>dataset_gen/hub/uploader.py</code> - Modify: <code>tests/test_hub_uploader.py</code></p> <p>Step 1: Write the failing test</p> <pre><code># Add to TestDatasetUploaderHelpers class\n\n    def test_create_media_archives_creates_tar_files(self, sample_dataset):\n        \"\"\"_create_media_archives creates TAR files in staging directory.\"\"\"\n        # Create actual audio files\n        audio_dir = sample_dataset / \"audio\"\n        audio_dir.mkdir()\n        for name in [\"a1.flac\", \"a2.flac\", \"a3.flac\", \"a4.flac\"]:\n            (audio_dir / name).write_bytes(b\"fake audio content\")\n\n        config = HubConfig(dataset_dir=sample_dataset, repo_id=\"test/repo\")\n        uploader = DatasetUploader(config)\n\n        # Create staging dir\n        staging = config.staging_dir\n        staging.mkdir(parents=True)\n\n        shard_map = uploader._create_media_archives(\"audio\", \"flac\")\n\n        # Should have created TAR files\n        audio_staging = staging / \"audio\"\n        assert audio_staging.exists()\n        assert (audio_staging / \"train-00000.tar\").exists()\n        assert (audio_staging / \"validation-00000.tar\").exists()\n        assert (audio_staging / \"test-00000.tar\").exists()\n\n        # Shard map should have all files\n        assert len(shard_map) == 4\n        assert \"a1.flac\" in shard_map\n        assert shard_map[\"a1.flac\"].shard_name == \"train-00000.tar\"\n</code></pre> <p>Step 2: Run test to verify it fails</p> <p>Run: <code>pytest tests/test_hub_uploader.py::TestDatasetUploaderHelpers::test_create_media_archives_creates_tar_files -v</code> Expected: FAIL with \"'DatasetUploader' object has no attribute '_create_media_archives'\"</p> <p>Step 3: Add the method</p> <pre><code># Add to DatasetUploader class in dataset_gen/hub/uploader.py\n# Also add import at top: from dataset_gen.hub.archiver import ShardInfo, create_sharded_archives\n\n    def _create_media_archives(self, media_type: str, extension: str) -&gt; dict[str, ShardInfo]:\n        \"\"\"\n        Create sharded TAR archives for media files.\n\n        Args:\n            media_type: Either \"audio\" or \"midi\"\n            extension: File extension (e.g., \"flac\", \"mid\")\n\n        Returns:\n            Dict mapping filename to ShardInfo\n        \"\"\"\n        src_dir = self.config.dataset_dir / media_type\n        dst_dir = self.config.staging_dir / media_type\n\n        if not src_dir.exists():\n            logger.warning(f\"Source directory {src_dir} does not exist\")\n            return {}\n\n        filenames_by_split = self._get_filenames_by_split(media_type)\n\n        shard_map = create_sharded_archives(\n            source_dir=src_dir,\n            output_dir=dst_dir,\n            filenames_by_split=filenames_by_split,\n            target_shard_size_bytes=self.config.tar_shard_size_bytes,\n            extension=extension,\n        )\n\n        # Update stats\n        total_files = sum(len(files) for files in filenames_by_split.values())\n        if media_type == \"audio\":\n            self.stats.audio_files = total_files\n        elif media_type == \"midi\":\n            self.stats.midi_files = total_files\n\n        return shard_map\n</code></pre> <p>Step 4: Run test to verify it passes</p> <p>Run: <code>pytest tests/test_hub_uploader.py::TestDatasetUploaderHelpers::test_create_media_archives_creates_tar_files -v</code> Expected: PASS</p> <p>Step 5: Commit</p> <pre><code>git add dataset_gen/hub/uploader.py tests/test_hub_uploader.py\ngit commit -m \"feat(hub): add _create_media_archives method\"\n</code></pre>"},{"location":"plans/2025-01-27-tar-sharding-implementation/#task-10-update-prepare-to-use-tar-sharding-and-add-shard-columns","title":"Task 10: Update prepare() to use TAR sharding and add shard columns","text":"<p>Files: - Modify: <code>dataset_gen/hub/uploader.py</code> - Modify: <code>tests/test_hub_uploader.py</code></p> <p>Step 1: Write the failing test</p> <pre><code># Add to TestDatasetUploaderHelpers class\n\n    def test_prepare_with_tar_shards_adds_shard_columns(self, sample_dataset):\n        \"\"\"prepare() adds audio_shard and audio_filename columns when using TAR shards.\"\"\"\n        # Create actual audio and midi files\n        audio_dir = sample_dataset / \"audio\"\n        audio_dir.mkdir()\n        for name in [\"a1.flac\", \"a2.flac\", \"a3.flac\", \"a4.flac\"]:\n            (audio_dir / name).write_bytes(b\"fake audio content\")\n\n        midi_dir = sample_dataset / \"midi\"\n        midi_dir.mkdir()\n        for name in [\"m1.mid\", \"m2.mid\", \"m3.mid\", \"m4.mid\"]:\n            (midi_dir / name).write_bytes(b\"fake midi content\")\n\n        config = HubConfig(dataset_dir=sample_dataset, repo_id=\"test/repo\", use_tar_shards=True)\n        uploader = DatasetUploader(config)\n\n        staging_dir = uploader.prepare()\n\n        # Check parquet has shard columns\n        train_df = pd.read_parquet(staging_dir / \"data\" / \"train-00000-of-00001.parquet\")\n\n        assert \"audio_shard\" in train_df.columns\n        assert \"audio_filename\" in train_df.columns\n        assert \"midi_shard\" in train_df.columns\n        assert \"midi_filename\" in train_df.columns\n\n        # Values should be set\n        assert train_df[\"audio_shard\"].iloc[0] == \"train-00000.tar\"\n        assert train_df[\"audio_filename\"].iloc[0] in [\"a1.flac\", \"a2.flac\"]\n</code></pre> <p>Step 2: Run test to verify it fails</p> <p>Run: <code>pytest tests/test_hub_uploader.py::TestDatasetUploaderHelpers::test_prepare_with_tar_shards_adds_shard_columns -v</code> Expected: FAIL (prepare() doesn't add shard columns yet)</p> <p>Step 3: Update prepare() method</p> <p>This requires significant changes to the prepare() method. Replace the media copying logic with archive creation and add shard columns to the dataframe:</p> <pre><code># In DatasetUploader.prepare(), replace the media handling section.\n# After \"merged_df = self._merge_dataframes(samples_df, exercises_df)\"\n# and before \"# Create split assignment column\"\n\n        # Handle media files (TAR shards or individual files)\n        audio_shard_map: dict[str, ShardInfo] = {}\n        midi_shard_map: dict[str, ShardInfo] = {}\n\n        if self.config.use_tar_shards:\n            # Create TAR archives\n            if self.config.include_audio and not skip_media_copy:\n                audio_shard_map = self._create_media_archives(\"audio\", \"flac\")\n            if self.config.include_midi and not skip_media_copy:\n                midi_shard_map = self._create_media_archives(\"midi\", \"mid\")\n\n            # Add shard columns to dataframe\n            if self.config.include_audio:\n                merged_df[\"audio_shard\"] = merged_df[\"audio_path\"].apply(\n                    lambda p: audio_shard_map.get(Path(p).name, ShardInfo(\"\", \"\")).shard_name\n                    if pd.notna(p) and p else None\n                )\n                merged_df[\"audio_filename\"] = merged_df[\"audio_path\"].apply(\n                    lambda p: Path(p).name if pd.notna(p) and p else None\n                )\n            if self.config.include_midi:\n                merged_df[\"midi_shard\"] = merged_df[\"midi_path\"].apply(\n                    lambda p: midi_shard_map.get(Path(p).name, ShardInfo(\"\", \"\")).shard_name\n                    if pd.notna(p) and p else None\n                )\n                merged_df[\"midi_filename\"] = merged_df[\"midi_path\"].apply(\n                    lambda p: Path(p).name if pd.notna(p) and p else None\n                )\n        else:\n            # Original behavior: copy/link individual files\n            if self.config.include_audio:\n                merged_df[\"audio\"] = merged_df[\"audio_path\"].apply(\n                    lambda p: f\"audio/{Path(p).name}\" if pd.notna(p) and p else None\n                )\n            if self.config.include_midi:\n                merged_df[\"midi\"] = merged_df[\"midi_path\"].apply(\n                    lambda p: f\"midi/{Path(p).name}\" if pd.notna(p) and p else None\n                )\n\n# Then later in the method, update the media file handling:\n\n        if not self.config.use_tar_shards:\n            # Copy/link audio files (original behavior)\n            if self.config.include_audio:\n                if skip_media_copy:\n                    self._count_media_files(\"audio\", \"flac\")\n                else:\n                    self._copy_media_files(\"audio\", \"flac\", use_symlinks=use_symlinks)\n\n            # Copy/link MIDI files (original behavior)\n            if self.config.include_midi:\n                if skip_media_copy:\n                    self._count_media_files(\"midi\", \"mid\")\n                else:\n                    self._copy_media_files(\"midi\", \"mid\", use_symlinks=use_symlinks)\n</code></pre> <p>Step 4: Run test to verify it passes</p> <p>Run: <code>pytest tests/test_hub_uploader.py::TestDatasetUploaderHelpers::test_prepare_with_tar_shards_adds_shard_columns -v</code> Expected: PASS</p> <p>Step 5: Commit</p> <pre><code>git add dataset_gen/hub/uploader.py tests/test_hub_uploader.py\ngit commit -m \"feat(hub): integrate TAR sharding into prepare()\"\n</code></pre>"},{"location":"plans/2025-01-27-tar-sharding-implementation/#task-11-update-push_to_hubpy-script-with-no-sharding-flag","title":"Task 11: Update push_to_hub.py script with --no-sharding flag","text":"<p>Files: - Modify: <code>scripts/push_to_hub.py</code></p> <p>Step 1: Update the script</p> <p>Add argument after <code>--dry-run</code>:</p> <pre><code>    parser.add_argument(\n        \"--no-sharding\",\n        action=\"store_true\",\n        help=\"Disable TAR sharding (not recommended for large datasets)\",\n    )\n</code></pre> <p>And pass it to HubConfig:</p> <pre><code>    config = HubConfig(\n        dataset_dir=args.dataset_dir,\n        repo_id=args.repo_id,\n        token=args.token,\n        private=args.private,\n        include_audio=not args.no_audio,\n        include_midi=not args.no_midi,\n        staging_dir=args.staging_dir,\n        use_tar_shards=not args.no_sharding,\n    )\n</code></pre> <p>Step 2: Test manually</p> <p>Run: <code>python scripts/push_to_hub.py --help</code> Expected: Shows <code>--no-sharding</code> option</p> <p>Step 3: Commit</p> <pre><code>git add scripts/push_to_hub.py\ngit commit -m \"feat(hub): add --no-sharding flag to push_to_hub script\"\n</code></pre>"},{"location":"plans/2025-01-27-tar-sharding-implementation/#task-12-run-full-test-suite-and-verify","title":"Task 12: Run full test suite and verify","text":"<p>Files: None (verification only)</p> <p>Step 1: Run all hub-related tests</p> <p>Run: <code>pytest tests/test_hub_archiver.py tests/test_hub_uploader.py -v</code> Expected: All tests PASS</p> <p>Step 2: Run full test suite</p> <p>Run: <code>pytest tests/ -v</code> Expected: All 157+ tests PASS</p> <p>Step 3: Run linting</p> <p>Run: <code>ruff check dataset_gen/hub/ &amp;&amp; black --check dataset_gen/hub/</code> Expected: No errors</p> <p>Step 4: Final commit if any fixes needed</p>"},{"location":"plans/2025-01-27-tar-sharding-implementation/#task-13-test-with-real-dataset-manual-integration-test","title":"Task 13: Test with real dataset (manual integration test)","text":"<p>Files: None (manual test)</p> <p>Step 1: Copy a small sample of real data to worktree</p> <pre><code>mkdir -p output/dataset/audio output/dataset/midi output/dataset/labels\ncp /path/to/main/repo/output/dataset/labels/*.parquet output/dataset/labels/\ncp /path/to/main/repo/output/dataset/splits.json output/dataset/\n# Copy just 100 audio and midi files for testing\nls /path/to/main/repo/output/dataset/audio/*.flac | head -100 | xargs -I {} cp {} output/dataset/audio/\nls /path/to/main/repo/output/dataset/midi/*.mid | head -100 | xargs -I {} cp {} output/dataset/midi/\n</code></pre> <p>Step 2: Run dry-run</p> <p>Run: <code>python scripts/push_to_hub.py zkeown/sousa-test --dry-run</code> Expected: Creates TAR archives in <code>output/dataset/hf_staging/</code></p> <p>Step 3: Verify structure</p> <p><pre><code>ls -la output/dataset/hf_staging/\nls -la output/dataset/hf_staging/audio/\nls -la output/dataset/hf_staging/midi/\n</code></pre> Expected: TAR files instead of individual audio/midi files</p> <p>Step 4: Verify parquet has shard columns</p> <p><pre><code>import pandas as pd\ndf = pd.read_parquet(\"output/dataset/hf_staging/data/train-00000-of-00001.parquet\")\nprint(df[[\"audio_shard\", \"audio_filename\", \"midi_shard\", \"midi_filename\"]].head())\n</code></pre> Expected: Shard columns populated correctly</p>"},{"location":"reference/","title":"Technical Reference","text":"<p>This section provides comprehensive technical documentation for SOUSA's internal systems, data structures, and algorithms.</p>"},{"location":"reference/#overview","title":"Overview","text":"<p>SOUSA (Synthetic Open Unified Snare Assessment) generates synthetic drum rudiment datasets for machine learning training. The system produces over 100,000 labeled samples covering all 40 PAS (Percussive Arts Society) drum rudiments with MIDI, audio, and hierarchical performance labels.</p>"},{"location":"reference/#reference-documentation","title":"Reference Documentation","text":""},{"location":"reference/#architecture","title":"Architecture","text":"<p>System architecture documentation including:</p> <ul> <li>Pipeline Flow: Complete data flow from rudiment definitions through audio output</li> <li>Class Diagrams: Data structure relationships (Sample, StrokeLabel, MeasureLabel, ExerciseScores)</li> <li>Module Dependencies: How components interact</li> </ul>"},{"location":"reference/#rudiment-schema","title":"Rudiment Schema","text":"<p>YAML schema specification for rudiment definitions:</p> <ul> <li>Schema Fields: Complete field reference with types and constraints</li> <li>Stroke Types: tap, accent, grace, diddle, buzz</li> <li>Category Parameters: Flam, drag, diddle, and roll-specific configurations</li> <li>Examples: Annotated YAML files for common rudiments</li> </ul>"},{"location":"reference/#score-computation","title":"Score Computation","text":"<p>Mathematical foundations for performance scoring:</p> <ul> <li>Timing Metrics: Accuracy, consistency, tempo stability formulas</li> <li>Dynamics Metrics: Velocity control, accent differentiation</li> <li>Hand Balance: Combined velocity and timing balance scoring</li> <li>Overall Score: Weighted composite calculation</li> <li>Perceptual Scaling: Sigmoid transformations for human-aligned scores</li> </ul>"},{"location":"reference/#audio-processing","title":"Audio Processing","text":"<p>Audio augmentation pipeline documentation:</p> <ul> <li>Preamp Simulation: CLEAN, WARM, AGGRESSIVE, VINTAGE types</li> <li>Compression: Threshold, ratio, attack, release, knee parameters</li> <li>Equalization: Highpass, lowpass, and shelving filters</li> <li>Room Simulation: Impulse response convolution</li> <li>Degradation: Sample rate, bit depth, and noise injection</li> </ul>"},{"location":"reference/#data-format","title":"Data Format","text":"<p>Complete data schema documentation:</p> <ul> <li>File Structure: Directory layout for local and HuggingFace datasets</li> <li>Parquet Schemas: Column definitions for samples, exercises, measures, strokes</li> <li>Audio/MIDI Formats: Technical specifications</li> <li>Augmentation Presets: Configuration details for each preset</li> </ul>"},{"location":"reference/#quick-reference","title":"Quick Reference","text":""},{"location":"reference/#key-data-types","title":"Key Data Types","text":"Type Description Location <code>Rudiment</code> Complete rudiment definition <code>dataset_gen/rudiments/schema.py</code> <code>PlayerProfile</code> Player execution characteristics <code>dataset_gen/profiles/archetypes.py</code> <code>Sample</code> Complete labeled sample <code>dataset_gen/labels/schema.py</code> <code>StrokeEvent</code> Generated stroke with timing/velocity <code>dataset_gen/midi_gen/generator.py</code>"},{"location":"reference/#pipeline-stages","title":"Pipeline Stages","text":"Stage Module Description 1. Load <code>rudiments/loader.py</code> Parse YAML definitions 2. Profile <code>profiles/archetypes.py</code> Generate player profiles 3. MIDI <code>midi_gen/generator.py</code> Generate stroke events 4. Audio <code>audio_synth/synthesizer.py</code> Render via FluidSynth 5. Augment <code>audio_aug/pipeline.py</code> Apply audio augmentations 6. Label <code>labels/compute.py</code> Compute hierarchical scores 7. Store <code>pipeline/storage.py</code> Write Parquet files"},{"location":"reference/#score-ranges","title":"Score Ranges","text":"<p>All scores use a 0-100 scale where higher values indicate better performance:</p> Score Perfect Professional Advanced Intermediate Beginner <code>overall_score</code> 100 70-85 55-70 40-55 25-40 <code>timing_accuracy</code> 100 80-95 65-80 45-65 20-45 <code>hand_balance</code> 100 90-98 80-90 70-80 50-70"},{"location":"reference/#see-also","title":"See Also","text":"<ul> <li>Getting Started Guide - Installation and first steps</li> <li>User Guide - Configuration and generation</li> <li>API Reference - Python API documentation</li> </ul>"},{"location":"reference/architecture/","title":"Pipeline Architecture","text":"<p>This document describes SOUSA's data generation pipeline architecture, including data flow, class relationships, and module organization.</p>"},{"location":"reference/architecture/#pipeline-overview","title":"Pipeline Overview","text":"<p>The generation pipeline transforms rudiment definitions and player profiles into labeled audio samples through seven stages:</p> <pre><code>flowchart LR\n    subgraph Input\n        A[Rudiment YAML] --&gt; B[Loader]\n        C[Profile Config] --&gt; D[Generator]\n    end\n\n    subgraph Generation\n        B --&gt; E[StickingPattern]\n        D --&gt; F[PlayerProfile]\n        E --&gt; G[MIDI Generator]\n        F --&gt; G\n        G --&gt; H[StrokeEvents]\n    end\n\n    subgraph Audio\n        H --&gt; I[FluidSynth]\n        I --&gt; J[Raw Audio]\n        J --&gt; K[Room Sim]\n        K --&gt; L[Mic Sim]\n        L --&gt; M[Chain/EQ]\n        M --&gt; N[Degradation]\n    end\n\n    subgraph Labels\n        H --&gt; O[Stroke Labels]\n        O --&gt; P[Measure Labels]\n        P --&gt; Q[Exercise Scores]\n    end\n\n    subgraph Output\n        N --&gt; R[FLAC Audio]\n        G --&gt; S[MIDI File]\n        Q --&gt; T[Parquet Labels]\n    end</code></pre>"},{"location":"reference/architecture/#detailed-pipeline-stages","title":"Detailed Pipeline Stages","text":""},{"location":"reference/architecture/#stage-1-rudiment-loading","title":"Stage 1: Rudiment Loading","text":"<pre><code>flowchart TD\n    A[YAML Files] --&gt; B[yaml.safe_load]\n    B --&gt; C[Pydantic Validation]\n    C --&gt; D[Rudiment Object]\n\n    subgraph Validation\n        C --&gt; C1{Valid Schema?}\n        C1 --&gt;|Yes| D\n        C1 --&gt;|No| E[ValidationError]\n    end</code></pre> <p>Module: <code>dataset_gen/rudiments/loader.py</code></p> <p>The loader reads YAML definitions from <code>dataset_gen/rudiments/definitions/</code> and validates them against Pydantic schemas. Each rudiment file specifies:</p> <ul> <li>Pattern structure (strokes with hand, type, timing)</li> <li>Subdivision and tempo range</li> <li>Category-specific parameters</li> </ul>"},{"location":"reference/architecture/#stage-2-profile-generation","title":"Stage 2: Profile Generation","text":"<pre><code>flowchart TD\n    A[SkillTier] --&gt; B[ARCHETYPE_PARAMS]\n    B --&gt; C[Gaussian Sampling]\n    C --&gt; D[Dimension Clamping]\n    D --&gt; E[PlayerProfile]\n\n    subgraph Dimensions\n        C --&gt; C1[TimingDimensions]\n        C --&gt; C2[DynamicsDimensions]\n        C --&gt; C3[HandBalanceDimensions]\n        C --&gt; C4[RudimentSpecificDimensions]\n    end</code></pre> <p>Module: <code>dataset_gen/profiles/archetypes.py</code></p> <p>Profiles are generated from archetype parameters with Gaussian sampling. Each skill tier (beginner, intermediate, advanced, professional) has distinct parameter ranges derived from percussion research literature.</p>"},{"location":"reference/architecture/#stage-3-midi-generation","title":"Stage 3: MIDI Generation","text":"<pre><code>flowchart TD\n    A[Rudiment + Profile] --&gt; B[Ideal Events]\n    B --&gt; C[Apply Deviations]\n\n    subgraph Deviations\n        C --&gt; D[Timing Error]\n        C --&gt; E[Tempo Drift]\n        C --&gt; F[Hand Bias]\n        C --&gt; G[Fatigue Effect]\n        C --&gt; H[Velocity Variance]\n    end\n\n    D --&gt; I[StrokeEvents]\n    E --&gt; I\n    F --&gt; I\n    G --&gt; I\n    H --&gt; I\n\n    I --&gt; J[MIDI Bytes]</code></pre> <p>Module: <code>dataset_gen/midi_gen/generator.py</code></p> <p>The MIDI generator:</p> <ol> <li>Creates ideal stroke events from the rudiment pattern</li> <li>Applies player-specific deviations based on profile dimensions</li> <li>Encodes events as Standard MIDI Format bytes</li> </ol>"},{"location":"reference/architecture/#stage-4-audio-synthesis","title":"Stage 4: Audio Synthesis","text":"<pre><code>flowchart TD\n    A[MIDI Bytes] --&gt; B[FluidSynth]\n    B --&gt; C{Soundfont}\n    C --&gt; D[GeneralUser GS]\n    C --&gt; E[Marching Snare]\n    C --&gt; F[MT Power Drums]\n    C --&gt; G[Other SF2]\n\n    D --&gt; H[Raw Audio]\n    E --&gt; H\n    F --&gt; H\n    G --&gt; H</code></pre> <p>Module: <code>dataset_gen/audio_synth/synthesizer.py</code></p> <p>FluidSynth renders MIDI to audio using SF2 soundfonts. Multiple soundfonts provide timbral variety across practice pad, marching snare, and drum kit sounds.</p>"},{"location":"reference/architecture/#stage-5-audio-augmentation","title":"Stage 5: Audio Augmentation","text":"<pre><code>flowchart TD\n    A[Raw Audio] --&gt; B[Room Simulation]\n    B --&gt; C[Mic Simulation]\n    C --&gt; D[Recording Chain]\n    D --&gt; E[Degradation]\n    E --&gt; F[Augmented Audio]\n\n    subgraph Room\n        B --&gt; B1[IR Convolution]\n        B --&gt; B2[Wet/Dry Mix]\n    end\n\n    subgraph Mic\n        C --&gt; C1[Frequency Response]\n        C --&gt; C2[Proximity Effect]\n        C --&gt; C3[Distance Rolloff]\n    end\n\n    subgraph Chain\n        D --&gt; D1[Preamp]\n        D --&gt; D2[Compression]\n        D --&gt; D3[EQ]\n    end\n\n    subgraph Degrade\n        E --&gt; E1[Bit Depth]\n        E --&gt; E2[Sample Rate]\n        E --&gt; E3[Noise]\n    end</code></pre> <p>Module: <code>dataset_gen/audio_aug/pipeline.py</code></p> <p>The augmentation pipeline applies realistic recording conditions through four stages, each with configurable parameters.</p>"},{"location":"reference/architecture/#stage-6-label-computation","title":"Stage 6: Label Computation","text":"<pre><code>flowchart TD\n    A[StrokeEvents] --&gt; B[compute_stroke_labels]\n    B --&gt; C[StrokeLabel list]\n    C --&gt; D[compute_measure_labels]\n    D --&gt; E[MeasureLabel list]\n    C --&gt; F[compute_exercise_scores]\n    E --&gt; F\n    F --&gt; G[ExerciseScores]\n\n    C --&gt; H[Sample]\n    E --&gt; H\n    G --&gt; H</code></pre> <p>Module: <code>dataset_gen/labels/compute.py</code></p> <p>Labels are computed hierarchically:</p> <ol> <li>Stroke level: Individual timing/velocity errors</li> <li>Measure level: Aggregate statistics per measure</li> <li>Exercise level: Composite performance scores</li> </ol>"},{"location":"reference/architecture/#stage-7-storage","title":"Stage 7: Storage","text":"<pre><code>flowchart TD\n    A[Sample Objects] --&gt; B[DataFrame Conversion]\n    B --&gt; C[Parquet Writer]\n\n    C --&gt; D[samples.parquet]\n    C --&gt; E[exercises.parquet]\n    C --&gt; F[measures.parquet]\n    C --&gt; G[strokes.parquet]\n\n    H[MIDI Bytes] --&gt; I[midi/*.mid]\n    J[Audio Array] --&gt; K[audio/*.flac]</code></pre> <p>Module: <code>dataset_gen/pipeline/storage.py</code></p> <p>Final outputs are written to:</p> <ul> <li>Parquet files for structured labels</li> <li>MIDI files for symbolic data</li> <li>FLAC files for lossless audio</li> </ul>"},{"location":"reference/architecture/#data-structure-relationships","title":"Data Structure Relationships","text":""},{"location":"reference/architecture/#class-diagram","title":"Class Diagram","text":"<pre><code>classDiagram\n    class Sample {\n        +str sample_id\n        +str profile_id\n        +str rudiment_slug\n        +int tempo_bpm\n        +float duration_sec\n        +int num_cycles\n        +str skill_tier\n        +str dominant_hand\n        +List~StrokeLabel~ strokes\n        +List~MeasureLabel~ measures\n        +ExerciseScores exercise_scores\n        +AudioAugmentation audio_augmentation\n    }\n\n    class StrokeLabel {\n        +int index\n        +str hand\n        +str stroke_type\n        +float intended_time_ms\n        +float actual_time_ms\n        +float timing_error_ms\n        +int intended_velocity\n        +int actual_velocity\n        +int velocity_error\n        +bool is_grace_note\n        +bool is_accent\n        +int diddle_position\n        +float flam_spacing_ms\n        +int parent_stroke_index\n    }\n\n    class MeasureLabel {\n        +int index\n        +int stroke_start\n        +int stroke_end\n        +float timing_mean_error_ms\n        +float timing_std_ms\n        +float timing_max_error_ms\n        +float velocity_mean\n        +float velocity_std\n        +float velocity_consistency\n        +float lr_velocity_ratio\n        +float lr_timing_diff_ms\n    }\n\n    class ExerciseScores {\n        +float timing_accuracy\n        +float timing_consistency\n        +float tempo_stability\n        +float subdivision_evenness\n        +float velocity_control\n        +float accent_differentiation\n        +float accent_accuracy\n        +float hand_balance\n        +float weak_hand_index\n        +float flam_quality\n        +float diddle_quality\n        +float roll_sustain\n        +float groove_feel_proxy\n        +float overall_score\n        +float tier_confidence\n    }\n\n    class AudioAugmentation {\n        +str soundfont\n        +str room_type\n        +float room_wet_dry\n        +float mic_distance\n        +str mic_type\n        +float compression_ratio\n        +float noise_level_db\n        +int bit_depth\n        +int sample_rate\n    }\n\n    Sample \"1\" *-- \"many\" StrokeLabel : contains\n    Sample \"1\" *-- \"many\" MeasureLabel : contains\n    Sample \"1\" *-- \"1\" ExerciseScores : contains\n    Sample \"1\" *-- \"0..1\" AudioAugmentation : contains\n    MeasureLabel --&gt; StrokeLabel : references via stroke_start/end</code></pre>"},{"location":"reference/architecture/#rudiment-structure","title":"Rudiment Structure","text":"<pre><code>classDiagram\n    class Rudiment {\n        +str name\n        +str slug\n        +RudimentCategory category\n        +StickingPattern pattern\n        +Subdivision subdivision\n        +Tuple tempo_range\n        +RudimentParams params\n        +int pas_number\n        +str description\n        +bool starts_on_left\n    }\n\n    class StickingPattern {\n        +List~Stroke~ strokes\n        +float beats_per_cycle\n        +stroke_count()\n        +accent_positions()\n        +grace_note_positions()\n    }\n\n    class Stroke {\n        +Hand hand\n        +StrokeType stroke_type\n        +float grace_offset\n        +int diddle_position\n        +is_accented()\n        +is_grace_note()\n    }\n\n    class RudimentParams {\n        +Tuple flam_spacing_range\n        +Tuple diddle_ratio_range\n        +str roll_type\n        +int roll_strokes_per_beat\n        +Tuple buzz_strokes_range\n        +Tuple drag_spacing_range\n    }\n\n    Rudiment \"1\" *-- \"1\" StickingPattern : contains\n    Rudiment \"1\" *-- \"1\" RudimentParams : contains\n    StickingPattern \"1\" *-- \"many\" Stroke : contains</code></pre>"},{"location":"reference/architecture/#player-profile-structure","title":"Player Profile Structure","text":"<pre><code>classDiagram\n    class PlayerProfile {\n        +str id\n        +SkillTier skill_tier\n        +ExecutionDimensions dimensions\n        +str dominant_hand\n        +float fatigue_coefficient\n        +Tuple tempo_comfort_range\n        +get_tempo_penalty()\n    }\n\n    class ExecutionDimensions {\n        +TimingDimensions timing\n        +DynamicsDimensions dynamics\n        +HandBalanceDimensions hand_balance\n        +RudimentSpecificDimensions rudiment_specific\n    }\n\n    class TimingDimensions {\n        +float timing_accuracy\n        +float timing_consistency\n        +float tempo_drift\n        +float subdivision_evenness\n    }\n\n    class DynamicsDimensions {\n        +float velocity_mean\n        +float velocity_variance\n        +float accent_differentiation\n        +float accent_accuracy\n    }\n\n    class HandBalanceDimensions {\n        +float lr_velocity_ratio\n        +float lr_timing_bias\n        +float lr_consistency_delta\n    }\n\n    class RudimentSpecificDimensions {\n        +float flam_spacing\n        +float flam_spacing_variance\n        +float diddle_evenness\n        +float diddle_variance\n        +float roll_sustain\n        +float buzz_density_consistency\n    }\n\n    PlayerProfile \"1\" *-- \"1\" ExecutionDimensions : contains\n    ExecutionDimensions \"1\" *-- \"1\" TimingDimensions\n    ExecutionDimensions \"1\" *-- \"1\" DynamicsDimensions\n    ExecutionDimensions \"1\" *-- \"1\" HandBalanceDimensions\n    ExecutionDimensions \"1\" *-- \"1\" RudimentSpecificDimensions</code></pre>"},{"location":"reference/architecture/#module-organization","title":"Module Organization","text":"<pre><code>dataset_gen/\n\u251c\u2500\u2500 rudiments/\n\u2502   \u251c\u2500\u2500 definitions/     # 40 YAML files\n\u2502   \u251c\u2500\u2500 schema.py        # Pydantic models\n\u2502   \u2514\u2500\u2500 loader.py        # YAML loading\n\u2502\n\u251c\u2500\u2500 profiles/\n\u2502   \u251c\u2500\u2500 archetypes.py    # Profile generation\n\u2502   \u2514\u2500\u2500 sampler.py       # Batch sampling\n\u2502\n\u251c\u2500\u2500 midi_gen/\n\u2502   \u251c\u2500\u2500 generator.py     # MIDI generation\n\u2502   \u2514\u2500\u2500 articulations.py # Articulation handling\n\u2502\n\u251c\u2500\u2500 audio_synth/\n\u2502   \u2514\u2500\u2500 synthesizer.py   # FluidSynth wrapper\n\u2502\n\u251c\u2500\u2500 audio_aug/\n\u2502   \u251c\u2500\u2500 room.py          # Room simulation\n\u2502   \u251c\u2500\u2500 mic.py           # Mic simulation\n\u2502   \u251c\u2500\u2500 chain.py         # Recording chain\n\u2502   \u251c\u2500\u2500 degradation.py   # Quality degradation\n\u2502   \u2514\u2500\u2500 pipeline.py      # Augmentation orchestration\n\u2502\n\u251c\u2500\u2500 labels/\n\u2502   \u251c\u2500\u2500 schema.py        # Label Pydantic models\n\u2502   \u251c\u2500\u2500 compute.py       # Score computation\n\u2502   \u2514\u2500\u2500 groove.py        # Groove metrics\n\u2502\n\u251c\u2500\u2500 pipeline/\n\u2502   \u251c\u2500\u2500 generate.py      # Main orchestration\n\u2502   \u251c\u2500\u2500 parallel.py      # Multiprocessing\n\u2502   \u251c\u2500\u2500 checkpoint.py    # Resumable generation\n\u2502   \u251c\u2500\u2500 storage.py       # Parquet writing\n\u2502   \u2514\u2500\u2500 splits.py        # Train/val/test splits\n\u2502\n\u251c\u2500\u2500 validation/\n\u2502   \u251c\u2500\u2500 verify.py        # Data integrity checks\n\u2502   \u251c\u2500\u2500 realism.py       # Literature validation\n\u2502   \u2514\u2500\u2500 report.py        # Report generation\n\u2502\n\u2514\u2500\u2500 hub/\n    \u2514\u2500\u2500 uploader.py      # HuggingFace upload\n</code></pre>"},{"location":"reference/architecture/#parallel-processing","title":"Parallel Processing","text":"<pre><code>flowchart TD\n    A[Sample Tasks] --&gt; B[Task Queue]\n    B --&gt; C1[Worker 1]\n    B --&gt; C2[Worker 2]\n    B --&gt; C3[Worker N]\n\n    C1 --&gt; D[Result Queue]\n    C2 --&gt; D\n    C3 --&gt; D\n\n    D --&gt; E[Checkpoint Manager]\n    E --&gt; F[Storage Writer]\n\n    subgraph Checkpoint\n        E --&gt; G[Progress File]\n        E --&gt; H[Partial Results]\n    end</code></pre> <p>Module: <code>dataset_gen/pipeline/parallel.py</code></p> <p>The pipeline supports:</p> <ul> <li>Multiprocessing: CPU-parallel sample generation</li> <li>Checkpointing: Resume interrupted generation</li> <li>Batch writing: Efficient Parquet append operations</li> </ul>"},{"location":"reference/architecture/#data-flow-summary","title":"Data Flow Summary","text":"Stage Input Output Key Module Load YAML files <code>Rudiment</code> objects <code>rudiments/loader.py</code> Profile Skill tier <code>PlayerProfile</code> objects <code>profiles/archetypes.py</code> MIDI Rudiment + Profile <code>StrokeEvent</code> list + MIDI bytes <code>midi_gen/generator.py</code> Synth MIDI bytes Raw audio array <code>audio_synth/synthesizer.py</code> Augment Raw audio Augmented audio <code>audio_aug/pipeline.py</code> Label StrokeEvents <code>Sample</code> with all labels <code>labels/compute.py</code> Store Samples Parquet + MIDI + FLAC files <code>pipeline/storage.py</code>"},{"location":"reference/architecture/#see-also","title":"See Also","text":"<ul> <li>Rudiment Schema - YAML format specification</li> <li>Score Computation - Scoring algorithms</li> <li>Audio Processing - Augmentation details</li> </ul>"},{"location":"reference/audio-processing/","title":"Audio Processing","text":"<p>This document describes SOUSA's audio augmentation pipeline, which simulates realistic recording conditions to create diverse training data.</p>"},{"location":"reference/audio-processing/#pipeline-overview","title":"Pipeline Overview","text":"<p>The audio augmentation pipeline processes raw synthesized audio through four stages:</p> <pre><code>flowchart TD\n    A[Raw Audio] --&gt; B[Room Simulation]\n    B --&gt; C[Microphone Simulation]\n    C --&gt; D[Recording Chain]\n    D --&gt; E[Degradation]\n    E --&gt; F[Output Audio]\n\n    subgraph \"Stage 1: Room\"\n        B --&gt; B1[IR Convolution]\n        B --&gt; B2[Wet/Dry Mix]\n    end\n\n    subgraph \"Stage 2: Mic\"\n        C --&gt; C1[Frequency Response]\n        C --&gt; C2[Proximity Effect]\n        C --&gt; C3[Distance Rolloff]\n    end\n\n    subgraph \"Stage 3: Chain\"\n        D --&gt; D1[Preamp/Saturation]\n        D --&gt; D2[Compression]\n        D --&gt; D3[EQ]\n    end\n\n    subgraph \"Stage 4: Degrade\"\n        E --&gt; E1[Sample Rate]\n        E --&gt; E2[Bit Depth]\n        E --&gt; E3[Noise Injection]\n    end</code></pre>"},{"location":"reference/audio-processing/#stage-1-room-simulation","title":"Stage 1: Room Simulation","text":"<p>Module: <code>dataset_gen/audio_aug/room.py</code></p> <p>Room simulation adds acoustic space characteristics using convolution reverb.</p>"},{"location":"reference/audio-processing/#room-types","title":"Room Types","text":"Room Type RT60 (sec) Early Reflections (ms) Default Wet/Dry <code>PRACTICE_ROOM</code> 0.3 8 0.15 <code>STUDIO</code> 0.5 15 0.25 <code>BEDROOM</code> 0.25 5 0.10 <code>GARAGE</code> 0.8 25 0.30 <code>GYM</code> 1.5 50 0.35 <code>CONCERT_HALL</code> 2.0 40 0.40 <code>CHURCH</code> 3.0 60 0.50 <code>OUTDOOR</code> 0.1 100 0.05"},{"location":"reference/audio-processing/#configuration","title":"Configuration","text":"<pre><code>@dataclass\nclass RoomConfig:\n    room_type: RoomType = RoomType.STUDIO\n    wet_dry_mix: float = 0.3        # 0 = dry, 1 = wet only\n    ir_path: Path | None = None     # Custom IR file\n    decay_time_sec: float = 0.5     # RT60 approximation\n    early_reflection_delay_ms: float = 20.0\n    pre_delay_ms: float = 10.0\n    ir_trim_sec: float | None = None\n    ir_gain: float = 1.0\n</code></pre>"},{"location":"reference/audio-processing/#algorithm","title":"Algorithm","text":"<p>Impulse Response (IR) Convolution:</p> <p>When an IR file is available:</p> \\[ y(t) = x(t) * h(t) = \\int_{-\\infty}^{\\infty} x(\\tau) \\cdot h(t - \\tau) \\, d\\tau \\] <p>Where:</p> <ul> <li>\\(x(t)\\) is the input audio</li> <li>\\(h(t)\\) is the impulse response</li> <li>\\(y(t)\\) is the wet (reverberant) signal</li> </ul> <p>Synthetic IR Generation:</p> <p>When no IR is available, a synthetic reverb is generated:</p> <ol> <li> <p>Create exponential decay envelope:    $\\(\\text{envelope}(t) = e^{-6.91 \\cdot t / \\text{RT60}}\\)$</p> </li> <li> <p>Add early reflections at calculated delays</p> </li> <li> <p>Generate diffuse tail from filtered noise</p> </li> <li> <p>Mix dry and wet signals:    $\\(\\text{output} = (1 - \\text{wet\\_dry}) \\cdot \\text{dry} + \\text{wet\\_dry} \\cdot \\text{wet}\\)$</p> </li> </ol>"},{"location":"reference/audio-processing/#stage-2-microphone-simulation","title":"Stage 2: Microphone Simulation","text":"<p>Module: <code>dataset_gen/audio_aug/mic.py</code></p> <p>Microphone simulation applies frequency response curves and distance-based effects.</p>"},{"location":"reference/audio-processing/#microphone-types","title":"Microphone Types","text":"Type Character Frequency Response <code>DYNAMIC</code> SM57-style Presence peak at 3-5kHz, rolled-off highs <code>CONDENSER</code> Flat, detailed Slight air boost at 10kHz <code>RIBBON</code> Warm, vintage Significant high rolloff above 8kHz <code>PIEZO</code> Contact mic Harsh mids, limited range"},{"location":"reference/audio-processing/#microphone-positions","title":"Microphone Positions","text":"Position Effect <code>CENTER</code> On-axis, no additional filtering <code>OFF_AXIS</code> Reduced highs (10kHz lowpass) <code>OVERHEAD</code> More room, less attack (5kHz lowpass) <code>DISTANT</code> Significant rolloff (3kHz lowpass)"},{"location":"reference/audio-processing/#configuration_1","title":"Configuration","text":"<pre><code>@dataclass\nclass MicConfig:\n    mic_type: MicType = MicType.CONDENSER\n    position: MicPosition = MicPosition.CENTER\n    distance_meters: float = 0.5\n    proximity_effect: bool = True\n    distance_rolloff: bool = True\n    self_noise_db: float = -80.0\n    output_gain: float = 1.0\n</code></pre>"},{"location":"reference/audio-processing/#frequency-response-curves","title":"Frequency Response Curves","text":"<p>Each microphone type has a defined frequency response:</p> <p>Dynamic (SM57-style):</p> Frequency (Hz) Gain (dB) 20 -6 80 -3 200 0 1000 0 3000 +3 5000 +2 8000 0 12000 -3 16000 -8 20000 -15 <p>Condenser:</p> Frequency (Hz) Gain (dB) 20 -3 50 0 5000 +1 10000 +2 15000 +1 20000 -2"},{"location":"reference/audio-processing/#proximity-effect","title":"Proximity Effect","text":"<p>Bass boost for close-miking (&lt; 0.3m):</p> \\[ \\text{boost\\_dB} = \\max(0, (0.3 - \\text{distance}) \\times 20) \\] <p>Applied as a low shelf filter at 200Hz.</p>"},{"location":"reference/audio-processing/#distance-rolloff","title":"Distance Rolloff","text":"<p>High frequency rolloff based on distance:</p> \\[ f_{\\text{rolloff}} = \\frac{20000}{1 + \\text{distance} \\times 2} \\] <p>Clamped to 3000-15000 Hz range.</p> <p>Amplitude reduction (inverse square law approximation):</p> \\[ \\text{amplitude} = \\frac{1}{1 + \\text{distance}^{1.5}} \\]"},{"location":"reference/audio-processing/#stage-3-recording-chain","title":"Stage 3: Recording Chain","text":"<p>Module: <code>dataset_gen/audio_aug/chain.py</code></p> <p>The recording chain simulates analog processing: preamp, compression, and EQ.</p>"},{"location":"reference/audio-processing/#preamp-types","title":"Preamp Types","text":"Type Character Saturation Style <code>CLEAN</code> Transparent Minimal, soft clipping <code>WARM</code> Tube-style Asymmetric, even harmonics, low-mid bump <code>AGGRESSIVE</code> Solid-state Hard clipping, odd harmonics <code>VINTAGE</code> Transformer Soft limiting, frequency coloration"},{"location":"reference/audio-processing/#preamp-configuration","title":"Preamp Configuration","text":"<pre><code>@dataclass\nclass PreampConfig:\n    preamp_type: PreampType = PreampType.CLEAN\n    gain_db: float = 0.0      # Input gain\n    drive: float = 0.0        # Saturation amount (0-1)\n    output_gain_db: float = 0.0\n</code></pre>"},{"location":"reference/audio-processing/#saturation-algorithms","title":"Saturation Algorithms","text":"<p>Soft Clipping (tanh):</p> \\[ y = \\tanh(x \\cdot (1 + \\text{drive} \\times 3)) \\] <p>Hard Clipping:</p> \\[ y = \\text{clip}(x \\cdot (1 + \\text{drive} \\times 2), -\\text{threshold}, \\text{threshold}) \\] <p>Where \\(\\text{threshold} = 1.0 - \\text{drive} \\times 0.7\\)</p> <p>Tube Saturation (asymmetric):</p> <pre><code>positive = np.tanh(positive_signal)\nnegative = np.tanh(negative_signal * 0.9) * 1.1  # Asymmetry\n</code></pre>"},{"location":"reference/audio-processing/#compressor-configuration","title":"Compressor Configuration","text":"<pre><code>@dataclass\nclass CompressorConfig:\n    enabled: bool = True\n    threshold_db: float = -12.0\n    ratio: float = 4.0           # e.g., 4:1\n    attack_ms: float = 10.0\n    release_ms: float = 100.0\n    knee_db: float = 6.0         # Soft knee width\n    makeup_gain_db: float = 0.0\n</code></pre>"},{"location":"reference/audio-processing/#compression-algorithm","title":"Compression Algorithm","text":"<p>Gain Reduction:</p> <p>For input level \\(L\\) above threshold \\(T\\):</p> \\[ G = \\left(\\frac{T}{L}\\right)^{1 - 1/R} \\] <p>Where \\(R\\) is the compression ratio.</p> <p>Soft Knee:</p> <p>In the knee region (\\(T - \\frac{K}{2}\\) to \\(T + \\frac{K}{2}\\)):</p> \\[ R_{\\text{effective}} = 1 + \\frac{L - T_{\\text{start}}}{T_{\\text{end}} - T_{\\text{start}}} \\times (R - 1) \\] <p>Time Constants:</p> \\[ \\alpha_{\\text{attack}} = e^{-1 / (f_s \\times t_{\\text{attack}})} \\] \\[ \\alpha_{\\text{release}} = e^{-1 / (f_s \\times t_{\\text{release}})} \\]"},{"location":"reference/audio-processing/#eq-configuration","title":"EQ Configuration","text":"<pre><code>@dataclass\nclass EQConfig:\n    enabled: bool = True\n    highpass_freq: float = 40.0      # Hz\n    highpass_enabled: bool = True\n    lowpass_freq: float = 18000.0    # Hz\n    lowpass_enabled: bool = True\n    low_shelf: tuple[float, float] = (100.0, 0.0)   # (freq, gain_db)\n    high_shelf: tuple[float, float] = (8000.0, 0.0)\n</code></pre>"},{"location":"reference/audio-processing/#eq-implementation","title":"EQ Implementation","text":"<ul> <li>Highpass/Lowpass: 2<sup>nd</sup>-order Butterworth filters</li> <li>Shelving: 1<sup>st</sup>-order filters with gain mixing</li> </ul>"},{"location":"reference/audio-processing/#stage-4-degradation","title":"Stage 4: Degradation","text":"<p>Module: <code>dataset_gen/audio_aug/degradation.py</code></p> <p>Degradation simulates imperfect recording conditions.</p>"},{"location":"reference/audio-processing/#noise-types","title":"Noise Types","text":"Type Character Implementation <code>WHITE</code> Flat spectrum Gaussian noise <code>PINK</code> 1/f spectrum Voss-McCartney algorithm <code>BROWN</code> 1/f^2 (rumble) Integrated white noise <code>HVAC</code> Air conditioning 60Hz hum + filtered broadband <code>ROOM_TONE</code> Ambient Bandpass filtered pink noise <code>TAPE_HISS</code> High frequency Highpass filtered white noise <code>VINYL</code> Crackle/pops Brown noise + random impulses"},{"location":"reference/audio-processing/#noise-configuration","title":"Noise Configuration","text":"<pre><code>@dataclass\nclass NoiseConfig:\n    enabled: bool = True\n    noise_type: NoiseType = NoiseType.PINK\n    level_db: float = -40.0    # Relative to signal\n    noise_file: Path | None = None\n</code></pre>"},{"location":"reference/audio-processing/#noise-level-calculation","title":"Noise Level Calculation","text":"\\[ \\text{noise\\_amplitude} = \\text{signal\\_rms} \\times 10^{\\text{level\\_dB} / 20} \\]"},{"location":"reference/audio-processing/#bit-depth-reduction","title":"Bit Depth Reduction","text":"<pre><code>@dataclass\nclass BitDepthConfig:\n    enabled: bool = False\n    bit_depth: int = 16       # 24, 16, 12, or 8\n    dither: bool = True       # TPDF dither\n</code></pre> <p>Quantization:</p> \\[ y = \\frac{\\text{round}(x \\times 2^{n-1})}{2^{n-1}} \\] <p>Where \\(n\\) is the target bit depth.</p> <p>TPDF Dither:</p> <p>Triangular probability density function dither is added before quantization to reduce harmonic distortion:</p> \\[ \\text{dither} = \\frac{r_1 + r_2 - 1}{2^{n-1}} \\] <p>Where \\(r_1, r_2\\) are uniform random values in [0, 1].</p>"},{"location":"reference/audio-processing/#sample-rate-degradation","title":"Sample Rate Degradation","text":"<pre><code>@dataclass\nclass SampleRateConfig:\n    enabled: bool = False\n    target_rate: int = 22050\n    anti_alias: bool = True\n</code></pre> <p>Process:</p> <ol> <li>Apply anti-aliasing lowpass filter at \\(f_{\\text{target}} / 2\\)</li> <li>Downsample to target rate</li> <li>Upsample back to original rate</li> </ol> <p>This introduces aliasing artifacts characteristic of low-quality recordings.</p>"},{"location":"reference/audio-processing/#additional-degradations","title":"Additional Degradations","text":"Parameter Range Effect <code>dc_offset</code> -1 to 1 Adds DC offset <code>phase_shift</code> 0 to 1 Phase difference between stereo channels <code>wow_flutter</code> 0 to 1 Tape-style pitch variation <code>dropout_probability</code> 0 to 1 Brief audio dropouts <p>Wow and Flutter:</p> \\[ \\text{modulation}(t) = 1 + \\sin(2\\pi f_{\\text{wow}} t) \\cdot a_{\\text{wow}} + \\sin(2\\pi f_{\\text{flutter}} t) \\cdot a_{\\text{flutter}} \\] <p>Where:</p> <ul> <li>\\(f_{\\text{wow}} \\in [0.5, 2]\\) Hz</li> <li>\\(f_{\\text{flutter}} \\in [5, 15]\\) Hz</li> <li>Amplitudes scale with the <code>wow_flutter</code> parameter</li> </ul>"},{"location":"reference/audio-processing/#augmentation-presets","title":"Augmentation Presets","text":"<p>SOUSA includes predefined augmentation presets for common scenarios:</p> Preset Room Mic Compression Noise Use Case <code>cleanstudio</code> None Close None None Reference recordings <code>cleanclosed</code> Small Close Light None Studio practice <code>practiceroom</code> Practice Medium Light Low Typical practice <code>concerthall</code> Hall Far None Low Live performance <code>gym</code> Gymnasium Far None Medium Marching band <code>garage</code> Garage Medium Medium Medium Band rehearsal <code>vintagetape</code> Medium Medium Tape saturation Tape hiss Vintage aesthetic <code>lofi</code> Variable Variable Heavy High Lo-fi aesthetic <code>phonerecording</code> None Poor Heavy limiting High Phone recordings"},{"location":"reference/audio-processing/#preset-configuration-example","title":"Preset Configuration Example","text":"<pre><code># 'practiceroom' preset\nconfig = AugmentationConfig(\n    room=RoomConfig(\n        room_type=RoomType.PRACTICE_ROOM,\n        wet_dry_mix=0.15,\n    ),\n    mic=MicConfig(\n        mic_type=MicType.DYNAMIC,\n        distance_meters=0.8,\n    ),\n    chain=ChainConfig(\n        compressor=CompressorConfig(\n            threshold_db=-18.0,\n            ratio=2.0,\n        ),\n    ),\n    degradation=DegradationConfig(\n        noise=NoiseConfig(\n            noise_type=NoiseType.ROOM_TONE,\n            level_db=-45.0,\n        ),\n    ),\n)\n</code></pre>"},{"location":"reference/audio-processing/#output-specifications","title":"Output Specifications","text":"Property Value Format FLAC (lossless) Sample Rate 44,100 Hz Bit Depth 24-bit Channels Mono Duration 4-12 seconds typical"},{"location":"reference/audio-processing/#implementation-notes","title":"Implementation Notes","text":""},{"location":"reference/audio-processing/#processing-order","title":"Processing Order","text":"<p>The fixed processing order ensures consistent results:</p> <ol> <li>Room simulation (affects spatial characteristics)</li> <li>Mic simulation (captures source character)</li> <li>Recording chain (analog processing)</li> <li>Degradation (final quality reduction)</li> </ol>"},{"location":"reference/audio-processing/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>IR Caching: Impulse responses are cached to avoid repeated disk reads</li> <li>Noise Caching: External noise files are cached and looped as needed</li> <li>Filter Design: Filters are designed once per configuration</li> </ul>"},{"location":"reference/audio-processing/#clipping-prevention","title":"Clipping Prevention","text":"<p>Each stage includes output limiting:</p> <pre><code>max_val = np.max(np.abs(output))\nif max_val &gt; 1.0:\n    output = output / max_val * 0.99\n</code></pre>"},{"location":"reference/audio-processing/#usage-example","title":"Usage Example","text":"<pre><code>from dataset_gen.audio_aug.pipeline import AugmentationPipeline\n\n# Create pipeline with preset\npipeline = AugmentationPipeline.from_preset(\"practiceroom\")\n\n# Process audio\naugmented = pipeline.process(raw_audio, sample_rate=44100)\n\n# Or with custom config\nfrom dataset_gen.audio_aug.room import RoomConfig, RoomType\nfrom dataset_gen.audio_aug.chain import ChainConfig, PreampType\n\nconfig = AugmentationConfig(\n    room=RoomConfig(room_type=RoomType.GARAGE),\n    chain=ChainConfig(\n        preamp=PreampConfig(preamp_type=PreampType.WARM, drive=0.3)\n    ),\n)\naugmented = pipeline.process(raw_audio, config=config)\n</code></pre>"},{"location":"reference/audio-processing/#see-also","title":"See Also","text":"<ul> <li>Architecture - Pipeline overview</li> <li>Data Format - Augmentation metadata in Parquet</li> </ul>"},{"location":"reference/data-format/","title":"Data Format","text":"<p>This document describes the complete data schema for the SOUSA dataset, including file structure, Parquet schemas, and format specifications.</p>"},{"location":"reference/data-format/#overview","title":"Overview","text":"<p>SOUSA uses a hierarchical label structure with three levels of granularity:</p> <pre><code>Exercise (1 per sample)\n  |-- Measures (N per exercise)\n        |-- Strokes (M per measure)\n</code></pre> <p>Each sample includes:</p> <ul> <li>MIDI file: Symbolic performance data</li> <li>Audio file: Rendered and augmented audio (FLAC)</li> <li>Labels: Hierarchical scores in Parquet format</li> </ul>"},{"location":"reference/data-format/#file-structure","title":"File Structure","text":""},{"location":"reference/data-format/#local-dataset","title":"Local Dataset","text":"<pre><code>output/dataset/\n|-- audio/                    # FLAC audio files (44.1kHz, 24-bit)\n|   |-- {sample_id}.flac\n|-- midi/                     # Standard MIDI files\n|   |-- {sample_id}.mid\n|-- labels/                   # Parquet files with hierarchical labels\n|   |-- samples.parquet       # Sample metadata\n|   |-- exercises.parquet     # Exercise-level scores\n|   |-- measures.parquet      # Measure-level statistics\n|   |-- strokes.parquet       # Stroke-level events\n|-- index.json                # Dataset index\n|-- splits.json               # Train/val/test profile assignments\n|-- README.md                 # Dataset card\n</code></pre>"},{"location":"reference/data-format/#huggingface-format","title":"HuggingFace Format","text":"<pre><code>username/sousa/\n|-- data/\n|   |-- train-00000-of-00001.parquet\n|   |-- validation-00000-of-00001.parquet\n|   |-- test-00000-of-00001.parquet\n|-- audio/\n|   |-- {sample_id}.flac\n|-- midi/\n|   |-- {sample_id}.mid\n|-- README.md\n</code></pre>"},{"location":"reference/data-format/#sample-id-convention","title":"Sample ID Convention","text":"<p>Sample IDs encode key metadata for easy filtering:</p> <pre><code>{skill_tier}{profile_num}_{rudiment}_{tempo}bpm_{soundfont}_{augmentation}\n</code></pre> Component Description Examples <code>skill_tier</code> 3-letter skill code <code>beg</code>, <code>int</code>, <code>adv</code>, <code>pro</code> <code>profile_num</code> 3-digit profile number <code>000</code>-<code>099</code> <code>rudiment</code> Snake_case rudiment name <code>single_paradiddle</code>, <code>flam_tap</code> <code>tempo</code> BPM value <code>60</code>-<code>180</code> <code>soundfont</code> Soundfont identifier <code>generalu</code>, <code>marching</code>, <code>mtpowerd</code> <code>augmentation</code> Augmentation preset <code>cleanstudio</code>, <code>practiceroom</code>, <code>gym</code> <p>Example: <code>adv042_single_paradiddle_100bpm_marching_practiceroom</code></p>"},{"location":"reference/data-format/#schema-definitions","title":"Schema Definitions","text":""},{"location":"reference/data-format/#sample-metadata-samplesparquet","title":"Sample Metadata (<code>samples.parquet</code>)","text":"Column Type Description <code>sample_id</code> string Unique sample identifier <code>profile_id</code> string UUID of player profile <code>rudiment_slug</code> string Rudiment identifier (snake_case) <code>tempo_bpm</code> int Performance tempo <code>duration_sec</code> float Audio duration in seconds <code>num_cycles</code> int Number of rudiment repetitions <code>skill_tier</code> string One of: <code>beginner</code>, <code>intermediate</code>, <code>advanced</code>, <code>professional</code> <code>skill_tier_binary</code> string One of: <code>novice</code>, <code>skilled</code> <code>dominant_hand</code> string <code>right</code> or <code>left</code> <code>midi_path</code> string Relative path to MIDI file <code>audio_path</code> string Relative path to audio file <code>num_strokes</code> int Total strokes in performance <code>num_measures</code> int Total measures in performance"},{"location":"reference/data-format/#audio-augmentation-columns","title":"Audio Augmentation Columns","text":"<p>Prefixed with <code>aug_</code>:</p> Column Type Description <code>aug_soundfont</code> string Soundfont used for synthesis <code>aug_room_type</code> string Room simulation type <code>aug_room_wet_dry</code> float Room reverb wet/dry ratio (0-1) <code>aug_mic_distance</code> float Simulated mic distance (meters) <code>aug_mic_type</code> string Microphone type simulation <code>aug_compression_ratio</code> float Compression ratio applied <code>aug_noise_level_db</code> float Added noise level (dB) <code>aug_bit_depth</code> int Output bit depth <code>aug_sample_rate</code> int Output sample rate (Hz)"},{"location":"reference/data-format/#exercise-scores-exercisesparquet","title":"Exercise Scores (<code>exercises.parquet</code>)","text":"<p>All scores are 0-100 (higher = better performance).</p> Column Type Description <code>sample_id</code> string Sample identifier"},{"location":"reference/data-format/#timing-scores","title":"Timing Scores","text":"Column Type Description <code>timing_accuracy</code> float How close strokes are to intended timing <code>timing_consistency</code> float Variance in timing errors (lower variance = higher score) <code>tempo_stability</code> float Consistency of overall tempo throughout <code>subdivision_evenness</code> float Evenness of note subdivisions"},{"location":"reference/data-format/#dynamics-scores","title":"Dynamics Scores","text":"Column Type Description <code>velocity_control</code> float Control over stroke dynamics <code>accent_differentiation</code> float Clarity between accented/unaccented strokes <code>accent_accuracy</code> float Correct placement of accents"},{"location":"reference/data-format/#balance-scores","title":"Balance Scores","text":"Column Type Description <code>hand_balance</code> float Evenness between L/R hand strokes <code>weak_hand_index</code> float 0=left weak, 100=right weak, 50=balanced"},{"location":"reference/data-format/#rudiment-specific-scores-nullable","title":"Rudiment-Specific Scores (nullable)","text":"Column Type Description <code>flam_quality</code> float? Grace note spacing quality (flam rudiments only) <code>diddle_quality</code> float? Diddle stroke evenness (paradiddles only) <code>roll_sustain</code> float? Roll smoothness (roll rudiments only)"},{"location":"reference/data-format/#composite-scores","title":"Composite Scores","text":"Column Type Description <code>groove_feel_proxy</code> float Groove/feel metric (0-1 scale) <code>overall_score</code> float Weighted composite of all metrics <code>tier_confidence</code> float Confidence that skill_tier label is unambiguous (0-1)"},{"location":"reference/data-format/#measure-labels-measuresparquet","title":"Measure Labels (<code>measures.parquet</code>)","text":"<p>Per-measure aggregate statistics.</p> Column Type Description <code>sample_id</code> string Sample identifier <code>index</code> int Measure index (0-based) <code>stroke_start</code> int First stroke index in measure <code>stroke_end</code> int Last stroke index (exclusive)"},{"location":"reference/data-format/#timing-statistics","title":"Timing Statistics","text":"Column Type Description <code>timing_mean_error_ms</code> float Mean timing error (milliseconds) <code>timing_std_ms</code> float Timing error standard deviation <code>timing_max_error_ms</code> float Maximum timing error"},{"location":"reference/data-format/#velocity-statistics","title":"Velocity Statistics","text":"Column Type Description <code>velocity_mean</code> float Mean velocity (0-127 MIDI scale) <code>velocity_std</code> float Velocity standard deviation <code>velocity_consistency</code> float 1 - (std/mean), higher = more consistent"},{"location":"reference/data-format/#hand-balance","title":"Hand Balance","text":"Column Type Description <code>lr_velocity_ratio</code> float? Left/Right velocity ratio (null if single-hand) <code>lr_timing_diff_ms</code> float? L/R timing difference (ms)"},{"location":"reference/data-format/#stroke-labels-strokesparquet","title":"Stroke Labels (<code>strokes.parquet</code>)","text":"<p>Individual stroke-level events.</p> Column Type Description <code>sample_id</code> string Sample identifier <code>index</code> int Stroke index (0-based) <code>hand</code> string <code>L</code> or <code>R</code> <code>stroke_type</code> string Stroke type identifier"},{"location":"reference/data-format/#timing","title":"Timing","text":"Column Type Description <code>intended_time_ms</code> float Intended stroke time (ms from start) <code>actual_time_ms</code> float Actual stroke time <code>timing_error_ms</code> float actual - intended (positive = late)"},{"location":"reference/data-format/#velocity","title":"Velocity","text":"Column Type Description <code>intended_velocity</code> int Intended velocity (0-127) <code>actual_velocity</code> int Actual velocity <code>velocity_error</code> int actual - intended"},{"location":"reference/data-format/#articulation-flags","title":"Articulation Flags","text":"Column Type Description <code>is_grace_note</code> bool True if grace note (flams) <code>is_accent</code> bool True if accented stroke <code>diddle_position</code> int? Position in diddle (null if not diddle) <code>flam_spacing_ms</code> float? Actual spacing to primary stroke (grace notes only) <code>parent_stroke_index</code> int? Index of primary stroke (grace notes only)"},{"location":"reference/data-format/#audio-format","title":"Audio Format","text":"Property Value Format FLAC (lossless) Sample Rate 44,100 Hz Bit Depth 24-bit Channels Mono Duration 4-12 seconds typical"},{"location":"reference/data-format/#midi-format","title":"MIDI Format","text":"<p>Standard MIDI Type 1 files with:</p> Property Value Track 0 Tempo and time signature Track 1 Note events (channel 10 for drums) Note Numbers Standard GM drum mapping (38 = snare) Velocity 1-127 (dynamics) PPQ 480 ticks per quarter note"},{"location":"reference/data-format/#augmentation-presets","title":"Augmentation Presets","text":"Preset Room Mic Compression Noise <code>cleanstudio</code> None Close None None <code>cleanclosed</code> Small Close Light None <code>practiceroom</code> Small practice Medium Light Low <code>concerthall</code> Large hall Far None Low <code>gym</code> Gymnasium Far None Medium <code>garage</code> Garage Medium Medium Medium <code>vintagetape</code> Medium Medium Tape saturation Tape hiss <code>lofi</code> Variable Variable Heavy High <code>phonerecording</code> None Poor Heavy limiting High"},{"location":"reference/data-format/#soundfonts","title":"Soundfonts","text":"Identifier Description <code>generalu</code> GeneralUser GS - General purpose <code>marching</code> Marching snare focused <code>mtpowerd</code> MT Power Drums - Rock kit <code>douglasn</code> Douglas Drums - Natural kit <code>fluidr3</code> FluidR3 GM - Standard GM"},{"location":"reference/data-format/#trainvaltest-splits","title":"Train/Val/Test Splits","text":"<p>Default 70/15/15 split by profile (not by sample):</p> Split Profiles Samples (approx.) train 70 ~70,000 validation 15 ~15,000 test 15 ~15,000 <p>Profile-Based Splits</p> <p>Splits are assigned at the profile level to prevent data leakage from player-specific timing/velocity patterns. This tests generalization to \"new players\" not seen during training.</p> <p>Stratification: Splits maintain skill tier proportions:</p> <pre><code>Train skill distribution == Val skill distribution == Test skill distribution\n</code></pre>"},{"location":"reference/data-format/#class-balance-and-distribution","title":"Class Balance and Distribution","text":""},{"location":"reference/data-format/#skill-tier-distribution","title":"Skill Tier Distribution","text":"<p>SOUSA uses a non-uniform distribution approximating realistic player populations:</p> Skill Tier Target Proportion Full Dataset (~100K) Medium (~12K) Beginner 25% ~25,000 ~3,000 Intermediate 35% ~35,000 ~4,200 Advanced 25% ~25,000 ~3,000 Professional 15% ~15,000 ~1,800"},{"location":"reference/data-format/#rudiment-distribution","title":"Rudiment Distribution","text":"<p>All 40 PAS rudiments are equally represented:</p> Preset Profiles Tempos/Rudiment Augmentations Samples/Rudiment small 10 3 1 ~30 medium 50 3 2 ~300 full 100 5 5 ~2,500"},{"location":"reference/data-format/#score-specific-availability","title":"Score-Specific Availability","text":"<p>Some scores are only computed for rudiments containing specific articulations:</p> Score Column Available For % Non-Null <code>flam_quality</code> Flam rudiments (20-30) ~27.5% <code>diddle_quality</code> Diddle/roll rudiments (6-19, 24-26, 35-37) ~47.5% <code>roll_sustain</code> Roll rudiments (1-15) ~37.5% <p>Handling in Training:</p> <ul> <li>Use masking for null values in loss computation</li> <li>Consider separate prediction heads per articulation type</li> <li>Filter to non-null subset if focusing on specific rudiments</li> </ul>"},{"location":"reference/data-format/#rudiments","title":"Rudiments","text":""},{"location":"reference/data-format/#roll-rudiments-15","title":"Roll Rudiments (15)","text":"Slug Name <code>single_stroke_roll</code> Single Stroke Roll <code>single_stroke_four</code> Single Stroke Four <code>single_stroke_seven</code> Single Stroke Seven <code>multiple_bounce_roll</code> Multiple Bounce Roll <code>triple_stroke_roll</code> Triple Stroke Roll <code>double_stroke_roll</code> Double Stroke Open Roll <code>five_stroke_roll</code> Five Stroke Roll <code>six_stroke_roll</code> Six Stroke Roll <code>seven_stroke_roll</code> Seven Stroke Roll <code>nine_stroke_roll</code> Nine Stroke Roll <code>ten_stroke_roll</code> Ten Stroke Roll <code>eleven_stroke_roll</code> Eleven Stroke Roll <code>thirteen_stroke_roll</code> Thirteen Stroke Roll <code>fifteen_stroke_roll</code> Fifteen Stroke Roll <code>seventeen_stroke_roll</code> Seventeen Stroke Roll"},{"location":"reference/data-format/#diddle-rudiments-4","title":"Diddle Rudiments (4)","text":"Slug Name <code>single_paradiddle</code> Single Paradiddle <code>double_paradiddle</code> Double Paradiddle <code>triple_paradiddle</code> Triple Paradiddle <code>paradiddle_diddle</code> Paradiddle-Diddle"},{"location":"reference/data-format/#flam-rudiments-11","title":"Flam Rudiments (11)","text":"Slug Name <code>flam</code> Flam <code>flam_accent</code> Flam Accent <code>flam_tap</code> Flam Tap <code>flamacue</code> Flamacue <code>flam_paradiddle</code> Flam Paradiddle <code>single_flammed_mill</code> Single Flammed Mill <code>flam_paradiddle_diddle</code> Flam Paradiddle-Diddle <code>pataflafla</code> Pataflafla <code>swiss_army_triplet</code> Swiss Army Triplet <code>inverted_flam_tap</code> Inverted Flam Tap <code>flam_drag</code> Flam Drag"},{"location":"reference/data-format/#drag-rudiments-10","title":"Drag Rudiments (10)","text":"Slug Name <code>drag</code> Drag <code>single_drag_tap</code> Single Drag Tap <code>double_drag_tap</code> Double Drag Tap <code>lesson_25</code> Lesson 25 <code>single_dragadiddle</code> Single Dragadiddle <code>drag_paradiddle_1</code> Drag Paradiddle #1 <code>drag_paradiddle_2</code> Drag Paradiddle #2 <code>single_ratamacue</code> Single Ratamacue <code>double_ratamacue</code> Double Ratamacue <code>triple_ratamacue</code> Triple Ratamacue"},{"location":"reference/data-format/#score-computation-reference","title":"Score Computation Reference","text":""},{"location":"reference/data-format/#timing-accuracy","title":"Timing Accuracy","text":"<p>Uses sigmoid scaling for perceptual alignment:</p> \\[ \\text{timing\\_accuracy} = 100 \\times \\frac{1}{1 + e^{(\\bar{e} - 25) / 10}} \\] <p>Where <code>max_expected_error_ms</code> scales with tempo (faster = tighter tolerance).</p>"},{"location":"reference/data-format/#velocity-control","title":"Velocity Control","text":"\\[ \\text{velocity\\_control} = 100 \\times (1 - \\text{velocity\\_std} / \\text{expected\\_std\\_for\\_tier}) \\]"},{"location":"reference/data-format/#overall-score","title":"Overall Score","text":"<p>Weighted combination:</p> Component Weight timing_accuracy 0.20 timing_consistency 0.15 velocity_control 0.10 accent_accuracy 0.10 hand_balance 0.15 tempo_stability 0.10 subdivision_evenness 0.10 accent_differentiation 0.10 <p>Plus rudiment-specific bonuses/penalties when applicable.</p>"},{"location":"reference/data-format/#validation-report-schema","title":"Validation Report Schema","text":"<p>The <code>validation_report.json</code> file contains comprehensive dataset validation results.</p>"},{"location":"reference/data-format/#top-level-structure","title":"Top-Level Structure","text":"Field Type Description <code>dataset_path</code> string Path to validated dataset <code>generated_at</code> string ISO 8601 timestamp <code>stats</code> object Statistical analysis results <code>verification</code> object Data integrity check results <code>skill_tier_ordering</code> object Skill tier ordering verification <code>realism</code> object Literature validation results"},{"location":"reference/data-format/#stats-object","title":"Stats Object","text":"Field Type Description <code>num_samples</code> int Total sample count <code>num_profiles</code> int Unique player profiles <code>num_rudiments</code> int Unique rudiments <code>tempo</code> DistributionStats Tempo BPM distribution <code>duration</code> DistributionStats Duration in seconds <code>num_strokes</code> DistributionStats Strokes per sample <code>timing_error</code> DistributionStats Stroke timing error (ms) <code>velocity</code> DistributionStats MIDI velocity values <code>skill_tier_counts</code> object Samples per skill tier <code>rudiment_counts</code> object Samples per rudiment <code>split_counts</code> object Train/val/test counts"},{"location":"reference/data-format/#distributionstats-object","title":"DistributionStats Object","text":"Field Type Description <code>name</code> string Metric name <code>count</code> int Number of observations <code>mean</code> float Arithmetic mean <code>std</code> float Standard deviation <code>min</code> float Minimum value <code>max</code> float Maximum value <code>median</code> float 50<sup>th</sup> percentile <code>q25</code> float 25<sup>th</sup> percentile <code>q75</code> float 75<sup>th</sup> percentile <code>skewness</code> float Distribution skewness <code>kurtosis</code> float Distribution kurtosis <code>by_group</code> object? Optional breakdown by skill tier"},{"location":"reference/data-format/#loading-the-dataset","title":"Loading the Dataset","text":""},{"location":"reference/data-format/#with-pandas","title":"With Pandas","text":"<pre><code>import pandas as pd\n\n# Load label files\nsamples = pd.read_parquet(\"output/dataset/labels/samples.parquet\")\nexercises = pd.read_parquet(\"output/dataset/labels/exercises.parquet\")\nmeasures = pd.read_parquet(\"output/dataset/labels/measures.parquet\")\nstrokes = pd.read_parquet(\"output/dataset/labels/strokes.parquet\")\n\n# Join for analysis\ndata = samples.merge(exercises, on=\"sample_id\")\n</code></pre>"},{"location":"reference/data-format/#with-huggingface-datasets","title":"With HuggingFace Datasets","text":"<pre><code>from datasets import load_dataset\n\ndataset = load_dataset(\"zkeown/sousa\")\n\n# Access splits\ntrain = dataset[\"train\"]\nval = dataset[\"validation\"]\ntest = dataset[\"test\"]\n\n# Filter by rudiment\nparadiddles = train.filter(lambda x: \"paradiddle\" in x[\"rudiment_slug\"])\n</code></pre>"},{"location":"reference/data-format/#loading-audio","title":"Loading Audio","text":"<pre><code>import soundfile as sf\n\n# Load audio file\naudio, sr = sf.read(\"output/dataset/audio/sample_id.flac\")\nprint(f\"Sample rate: {sr}, Duration: {len(audio)/sr:.2f}s\")\n</code></pre>"},{"location":"reference/data-format/#loading-midi","title":"Loading MIDI","text":"<pre><code>import mido\n\nmidi = mido.MidiFile(\"output/dataset/midi/sample_id.mid\")\nfor track in midi.tracks:\n    for msg in track:\n        if msg.type == \"note_on\":\n            print(f\"Note: {msg.note}, Velocity: {msg.velocity}, Time: {msg.time}\")\n</code></pre>"},{"location":"reference/data-format/#see-also","title":"See Also","text":"<ul> <li>Architecture - Pipeline overview</li> <li>Score Computation - Detailed scoring algorithms</li> <li>Rudiment Schema - YAML format for rudiments</li> </ul>"},{"location":"reference/rudiment-schema/","title":"Rudiment Schema Specification","text":"<p>This document defines the YAML schema for rudiment definitions in SOUSA. All 40 PAS rudiments are defined in <code>dataset_gen/rudiments/definitions/</code>.</p>"},{"location":"reference/rudiment-schema/#schema-overview","title":"Schema Overview","text":"<p>Each rudiment is defined in a YAML file with the following structure:</p> <pre><code>name: &lt;string&gt;           # Official PAS name\nslug: &lt;string&gt;           # URL-safe identifier\ncategory: &lt;enum&gt;         # roll | diddle | flam | drag\npas_number: &lt;int&gt;        # Official PAS number (1-40)\ndescription: &lt;string&gt;    # Brief description\n\npattern:\n  strokes: &lt;list&gt;        # Stroke definitions\n  beats_per_cycle: &lt;float&gt;  # Duration of one pattern cycle\n\nsubdivision: &lt;enum&gt;      # Base note subdivision\ntempo_range: [min, max]  # Recommended BPM range\n\nparams:                  # Category-specific parameters\n  &lt;param&gt;: &lt;value&gt;\n</code></pre>"},{"location":"reference/rudiment-schema/#field-reference","title":"Field Reference","text":""},{"location":"reference/rudiment-schema/#top-level-fields","title":"Top-Level Fields","text":"Field Type Required Description <code>name</code> string Yes Official PAS rudiment name <code>slug</code> string Yes URL-safe identifier (lowercase, underscores only) <code>category</code> enum Yes One of: <code>roll</code>, <code>diddle</code>, <code>flam</code>, <code>drag</code> <code>pas_number</code> int No Official PAS number (1-40) <code>description</code> string No Brief description of the rudiment <code>pattern</code> object Yes Sticking pattern definition <code>subdivision</code> enum No Base subdivision (default: <code>sixteenth</code>) <code>tempo_range</code> [int, int] No Min/max BPM (default: [60, 180]) <code>params</code> object No Category-specific parameters <code>starts_on_left</code> bool No If true, alternate version starts on left hand"},{"location":"reference/rudiment-schema/#pattern-object","title":"Pattern Object","text":"Field Type Required Description <code>strokes</code> list Yes List of stroke definitions <code>beats_per_cycle</code> float No Beats per pattern cycle (default: 1.0)"},{"location":"reference/rudiment-schema/#stroke-object","title":"Stroke Object","text":"<p>Each stroke in the <code>strokes</code> list:</p> Field Type Required Description <code>hand</code> enum Yes <code>R</code> (right) or <code>L</code> (left) <code>type</code> enum No Stroke type (default: <code>tap</code>) <code>grace_offset</code> float No Timing offset in beats for grace notes <code>diddle_position</code> int No Position in diddle (1 or 2)"},{"location":"reference/rudiment-schema/#stroke-types","title":"Stroke Types","text":"<p>SOUSA supports five stroke types, each with distinct velocity and timing characteristics:</p>"},{"location":"reference/rudiment-schema/#tap","title":"<code>tap</code>","text":"<p>Standard unaccented stroke. Default velocity: 85.</p> <pre><code>- {hand: R, type: tap}\n</code></pre>"},{"location":"reference/rudiment-schema/#accent","title":"<code>accent</code>","text":"<p>Emphasized stroke with higher velocity. Default velocity: 110.</p> <pre><code>- {hand: R, type: accent}\n</code></pre>"},{"location":"reference/rudiment-schema/#grace","title":"<code>grace</code>","text":"<p>Grace note preceding a primary stroke. Requires <code>grace_offset</code> to specify timing.</p> <pre><code>- {hand: L, type: grace, grace_offset: -0.05}  # 0.05 beats before primary\n- {hand: R, type: accent}                       # Primary stroke\n</code></pre> <p>Grace Note Timing</p> <p><code>grace_offset</code> is specified in beats (negative = before the beat). Typical values:</p> <ul> <li>Flams: <code>-0.05</code> (single grace note)</li> <li>Drags: <code>-0.08</code> and <code>-0.04</code> (two grace notes)</li> </ul>"},{"location":"reference/rudiment-schema/#diddle","title":"<code>diddle</code>","text":"<p>Double stroke (one of two strokes played with the same hand). Must specify <code>diddle_position</code>.</p> <pre><code>- {hand: R, type: diddle, diddle_position: 1}  # First stroke\n- {hand: R, type: diddle, diddle_position: 2}  # Second stroke\n</code></pre>"},{"location":"reference/rudiment-schema/#buzz","title":"<code>buzz</code>","text":"<p>Press roll stroke with multiple bounces. Used in buzz rolls.</p> <pre><code>- {hand: R, type: buzz}\n</code></pre>"},{"location":"reference/rudiment-schema/#subdivisions","title":"Subdivisions","text":"<p>The <code>subdivision</code> field specifies the base rhythmic subdivision:</p> Value Notes per Beat Common Usage <code>quarter</code> 1 Slow exercises <code>eighth</code> 2 Drags, some flams <code>triplet</code> 3 Triplet-based rudiments <code>sixteenth</code> 4 Most rudiments (default) <code>sextuplet</code> 6 Fast rolls <code>thirtysecond</code> 8 Double-time passages"},{"location":"reference/rudiment-schema/#category-parameters","title":"Category Parameters","text":"<p>Category-specific parameters control articulation-specific behaviors.</p>"},{"location":"reference/rudiment-schema/#flam-parameters","title":"Flam Parameters","text":"<p>For rudiments in the <code>flam</code> category:</p> Parameter Type Description Example <code>flam_spacing_range</code> [float, float] Min/max grace note spacing in ms <code>[15, 50]</code> <pre><code>params:\n  flam_spacing_range: [15, 50]  # Grace note 15-50ms before primary\n</code></pre>"},{"location":"reference/rudiment-schema/#diddle-parameters","title":"Diddle Parameters","text":"<p>For rudiments in the <code>diddle</code> category:</p> Parameter Type Description Example <code>diddle_ratio_range</code> [float, float] Ratio range between first/second stroke duration <code>[0.9, 1.1]</code> <pre><code>params:\n  diddle_ratio_range: [0.9, 1.1]  # Nearly even strokes\n</code></pre>"},{"location":"reference/rudiment-schema/#roll-parameters","title":"Roll Parameters","text":"<p>For rudiments in the <code>roll</code> category:</p> Parameter Type Description Example <code>roll_type</code> enum <code>open</code>, <code>closed</code>, or <code>buzz</code> <code>\"open\"</code> <code>roll_strokes_per_beat</code> int Number of strokes per beat <code>4</code> <pre><code>params:\n  roll_type: open\n  roll_strokes_per_beat: 4\n</code></pre>"},{"location":"reference/rudiment-schema/#drag-parameters","title":"Drag Parameters","text":"<p>For rudiments in the <code>drag</code> category:</p> Parameter Type Description Example <code>drag_spacing_range</code> [float, float] Min/max spacing between drag grace notes in ms <code>[10, 30]</code> <pre><code>params:\n  drag_spacing_range: [10, 30]\n</code></pre>"},{"location":"reference/rudiment-schema/#buzz-roll-parameters","title":"Buzz Roll Parameters","text":"<p>For buzz/press roll rudiments:</p> Parameter Type Description Example <code>buzz_strokes_range</code> [int, int] Min/max bounce strokes per primary <code>[3, 8]</code> <pre><code>params:\n  buzz_strokes_range: [3, 8]\n</code></pre>"},{"location":"reference/rudiment-schema/#complete-examples","title":"Complete Examples","text":""},{"location":"reference/rudiment-schema/#single-paradiddle-diddle-category","title":"Single Paradiddle (Diddle Category)","text":"<pre><code># File: 16_single_paradiddle.yaml\n#\n# The single paradiddle: RLRR LRLL\n# One of the most fundamental paradiddle patterns with accents\n# on the first stroke of each group.\n\nname: Single Paradiddle\nslug: single_paradiddle\ncategory: diddle\npas_number: 16\ndescription: RLRR LRLL - the most fundamental paradiddle pattern\n\npattern:\n  strokes:\n    # First group: RLRR\n    - {hand: R, type: accent}              # Accented primary\n    - {hand: L, type: tap}                 # Single tap\n    - {hand: R, type: diddle, diddle_position: 1}  # First diddle\n    - {hand: R, type: diddle, diddle_position: 2}  # Second diddle\n    # Second group: LRLL\n    - {hand: L, type: accent}              # Accented primary\n    - {hand: R, type: tap}                 # Single tap\n    - {hand: L, type: diddle, diddle_position: 1}  # First diddle\n    - {hand: L, type: diddle, diddle_position: 2}  # Second diddle\n  beats_per_cycle: 2  # 8 sixteenths = 2 beats\n\nsubdivision: sixteenth\ntempo_range: [60, 200]\n\nparams:\n  diddle_ratio_range: [0.9, 1.1]  # Diddles should be nearly even\n</code></pre>"},{"location":"reference/rudiment-schema/#flam-tap-flam-category","title":"Flam Tap (Flam Category)","text":"<pre><code># File: 22_flam_tap.yaml\n#\n# Alternating flams with a tap on each hand.\n# Pattern: flam-R tap-R | flam-L tap-L\n\nname: Flam Tap\nslug: flam_tap\ncategory: flam\npas_number: 22\ndescription: Alternating flams with a tap on each hand\n\npattern:\n  strokes:\n    # Flam R, tap R\n    - {hand: L, type: grace, grace_offset: -0.05}  # Grace note (left hand)\n    - {hand: R, type: accent}                       # Primary (right hand)\n    - {hand: R, type: tap}                          # Tap (right hand)\n    # Flam L, tap L\n    - {hand: R, type: grace, grace_offset: -0.05}  # Grace note (right hand)\n    - {hand: L, type: accent}                       # Primary (left hand)\n    - {hand: L, type: tap}                          # Tap (left hand)\n  beats_per_cycle: 1.5  # Triplet feel\n\nsubdivision: sixteenth\ntempo_range: [60, 180]\n\nparams:\n  flam_spacing_range: [15, 50]  # Grace note 15-50ms before primary\n</code></pre>"},{"location":"reference/rudiment-schema/#drag-drag-category","title":"Drag (Drag Category)","text":"<pre><code># File: 31_drag.yaml\n#\n# Two grace notes (diddle) followed by primary stroke.\n# Also known as a \"ruff\" in some contexts.\n\nname: Drag\nslug: drag\ncategory: drag\npas_number: 31\ndescription: Two grace notes (diddle) followed by primary stroke\n\npattern:\n  strokes:\n    # Drag R (grace LL + accent R)\n    - {hand: L, type: grace, grace_offset: -0.08}  # First grace\n    - {hand: L, type: grace, grace_offset: -0.04}  # Second grace\n    - {hand: R, type: accent}                       # Primary stroke\n    # Drag L (grace RR + accent L)\n    - {hand: R, type: grace, grace_offset: -0.08}  # First grace\n    - {hand: R, type: grace, grace_offset: -0.04}  # Second grace\n    - {hand: L, type: accent}                       # Primary stroke\n  beats_per_cycle: 1\n\nsubdivision: eighth\ntempo_range: [60, 160]\n\nparams:\n  drag_spacing_range: [10, 30]  # Spacing between the two grace notes\n</code></pre>"},{"location":"reference/rudiment-schema/#validation-rules","title":"Validation Rules","text":"<p>The schema enforces these validation rules:</p>"},{"location":"reference/rudiment-schema/#slug-format","title":"Slug Format","text":"<pre><code># Must be lowercase alphanumeric with underscores only\nif not re.match(r\"^[a-z0-9_]+$\", slug):\n    raise ValueError(\"Invalid slug format\")\n</code></pre>"},{"location":"reference/rudiment-schema/#diddle-position","title":"Diddle Position","text":"<p>Diddle strokes must specify position 1 or 2:</p> <pre><code>if stroke.stroke_type == StrokeType.DIDDLE:\n    assert stroke.diddle_position in (1, 2)\n</code></pre>"},{"location":"reference/rudiment-schema/#grace-note-offset","title":"Grace Note Offset","text":"<p>Grace notes must have a <code>grace_offset</code> value:</p> <pre><code>if stroke.stroke_type == StrokeType.GRACE:\n    assert stroke.grace_offset is not None\n    assert stroke.grace_offset &lt; 0  # Must be before the beat\n</code></pre>"},{"location":"reference/rudiment-schema/#category-parameter-matching","title":"Category-Parameter Matching","text":"<p>Category-specific parameters are validated against category:</p> Category Valid Parameters <code>flam</code> <code>flam_spacing_range</code> <code>diddle</code> <code>diddle_ratio_range</code> <code>roll</code> <code>roll_type</code>, <code>roll_strokes_per_beat</code>, <code>buzz_strokes_range</code> <code>drag</code> <code>drag_spacing_range</code>"},{"location":"reference/rudiment-schema/#creating-new-rudiments","title":"Creating New Rudiments","text":"<p>To add a custom rudiment:</p> <ol> <li>Create a YAML file in <code>dataset_gen/rudiments/definitions/</code></li> <li>Follow the naming convention: <code>{number}_{slug}.yaml</code></li> <li>Include all required fields</li> <li>Add category-appropriate parameters</li> <li>Test loading with:</li> </ol> <pre><code>from dataset_gen.rudiments.loader import load_rudiment\n\nrudiment = load_rudiment(\"my_custom_rudiment\")\nprint(rudiment.pattern.stroke_count())\n</code></pre> <p>PAS Numbers</p> <p>PAS numbers 1-40 are reserved for official PAS rudiments. Use numbers &gt; 40 for custom rudiments.</p>"},{"location":"reference/rudiment-schema/#complete-rudiment-list","title":"Complete Rudiment List","text":""},{"location":"reference/rudiment-schema/#roll-rudiments-1-15","title":"Roll Rudiments (1-15)","text":"# Slug Contains 1 <code>single_stroke_roll</code> - 2 <code>single_stroke_four</code> - 3 <code>single_stroke_seven</code> - 4 <code>multiple_bounce_roll</code> Buzz 5 <code>triple_stroke_roll</code> - 6 <code>double_stroke_open_roll</code> Diddle 7-15 <code>five_stroke_roll</code> through <code>seventeen_stroke_roll</code> Diddle"},{"location":"reference/rudiment-schema/#paradiddle-rudiments-16-19","title":"Paradiddle Rudiments (16-19)","text":"# Slug Contains 16 <code>single_paradiddle</code> Diddle 17 <code>double_paradiddle</code> Diddle 18 <code>triple_paradiddle</code> Diddle 19 <code>paradiddle_diddle</code> Diddle"},{"location":"reference/rudiment-schema/#flam-rudiments-20-30","title":"Flam Rudiments (20-30)","text":"# Slug Contains 20 <code>flam</code> Flam 21 <code>flam_accent</code> Flam 22 <code>flam_tap</code> Flam 23 <code>flamacue</code> Flam 24 <code>flam_paradiddle</code> Flam, Diddle 25 <code>single_flammed_mill</code> Flam, Diddle 26 <code>flam_paradiddle_diddle</code> Flam, Diddle 27 <code>pataflafla</code> Flam 28 <code>swiss_army_triplet</code> Flam 29 <code>inverted_flam_tap</code> Flam 30 <code>flam_drag</code> Flam, Drag"},{"location":"reference/rudiment-schema/#drag-rudiments-31-40","title":"Drag Rudiments (31-40)","text":"# Slug Contains 31 <code>drag</code> Drag 32 <code>single_drag_tap</code> Drag 33 <code>double_drag_tap</code> Drag 34 <code>lesson_25</code> Drag 35 <code>single_dragadiddle</code> Drag, Diddle 36 <code>drag_paradiddle_1</code> Drag, Diddle 37 <code>drag_paradiddle_2</code> Drag, Diddle 38 <code>single_ratamacue</code> Drag 39 <code>double_ratamacue</code> Drag 40 <code>triple_ratamacue</code> Drag"},{"location":"reference/rudiment-schema/#see-also","title":"See Also","text":"<ul> <li>Architecture - Pipeline overview</li> <li>Score Computation - How scores are calculated from rudiment performances</li> </ul>"},{"location":"reference/score-computation/","title":"Score Computation","text":"<p>This document describes the mathematical foundations for computing performance scores in SOUSA. All scores use a 0-100 scale where higher values indicate better performance.</p>"},{"location":"reference/score-computation/#overview","title":"Overview","text":"<p>SOUSA computes scores at three hierarchical levels:</p> <ol> <li>Stroke Level: Individual timing and velocity measurements</li> <li>Measure Level: Aggregate statistics per measure</li> <li>Exercise Level: Composite performance scores</li> </ol> <p>The exercise-level scores are the primary targets for machine learning, while stroke and measure labels provide fine-grained supervision.</p>"},{"location":"reference/score-computation/#perceptual-scaling","title":"Perceptual Scaling","text":"<p>SOUSA uses sigmoid (logistic) scaling to transform raw error measurements into perceptual scores. This approach aligns with human perception of timing errors:</p> <ul> <li>Errors &lt; 10ms: Nearly imperceptible (high score)</li> <li>Errors 20-30ms: Noticeable but acceptable (medium score)</li> <li>Errors &gt; 50ms: Clearly audible (low score)</li> </ul> <p>The general sigmoid transformation:</p> \\[ \\text{score} = 100 \\times \\frac{1}{1 + e^{(x - c) / k}} \\] <p>Where:</p> <ul> <li>\\(x\\) is the raw measurement (e.g., mean absolute timing error)</li> <li>\\(c\\) is the center point (50% score threshold)</li> <li>\\(k\\) controls the steepness of the transition</li> </ul>"},{"location":"reference/score-computation/#timing-scores","title":"Timing Scores","text":""},{"location":"reference/score-computation/#timing-accuracy","title":"Timing Accuracy","text":"<p>Measures how close strokes are to their intended timing.</p> <p>Formula:</p> \\[ \\text{timing\\_accuracy} = 100 \\times \\frac{1}{1 + e^{(\\bar{e} - 25) / 10}} \\] <p>Where:</p> <ul> <li>\\(\\bar{e}\\) = mean absolute timing error in milliseconds</li> </ul> <p>Parameters:</p> Parameter Value Meaning Center (\\(c\\)) 25 ms 50% score at 25ms mean error Steepness (\\(k\\)) 10 Transition width <p>Score Distribution:</p> Mean Error Score 5 ms 88 10 ms 82 25 ms 50 40 ms 18 60 ms 3"},{"location":"reference/score-computation/#timing-consistency","title":"Timing Consistency","text":"<p>Measures the variance in timing errors (lower variance = higher score).</p> <p>Formula:</p> \\[ \\text{timing\\_consistency} = 100 \\times \\frac{1}{1 + e^{(\\sigma_t - 15) / 8}} \\] <p>Where:</p> <ul> <li>\\(\\sigma_t\\) = standard deviation of timing errors in milliseconds</li> </ul> <p>Parameters:</p> Parameter Value Meaning Center (\\(c\\)) 15 ms 50% score at 15ms std dev Steepness (\\(k\\)) 8 Transition width"},{"location":"reference/score-computation/#tempo-stability","title":"Tempo Stability","text":"<p>Measures drift in timing over the exercise duration.</p> <p>Computation:</p> <ol> <li> <p>Fit a linear regression to timing errors over time:    $\\(\\text{error}_i = \\beta_0 + \\beta_1 \\cdot t_i\\)$</p> </li> <li> <p>Calculate slope in ms/second:    $\\(\\text{slope} = \\beta_1 \\times 1000\\)$</p> </li> <li> <p>Convert to score:    $\\(\\text{tempo\\_stability} = \\max(0, 100 - |\\text{slope}| \\times 10)\\)$</p> </li> </ol> <p>Interpretation:</p> Slope (ms/s) Score Meaning 0 100 No drift 5 50 Moderate drift 10 0 Severe drift"},{"location":"reference/score-computation/#subdivision-evenness","title":"Subdivision Evenness","text":"<p>Measures consistency of inter-onset intervals (IOI).</p> <p>Formula:</p> <ol> <li> <p>Calculate IOI ratios:    $\\(r_i = \\frac{\\text{IOI}_{\\text{actual},i}}{\\text{IOI}_{\\text{intended},i}}\\)$</p> </li> <li> <p>Calculate variance:    $\\(\\sigma_r = \\text{std}(r_1, r_2, \\ldots, r_n)\\)$</p> </li> <li> <p>Convert to score:    $\\(\\text{subdivision\\_evenness} = \\max(0, 100 - \\sigma_r \\times 100)\\)$</p> </li> </ol>"},{"location":"reference/score-computation/#dynamics-scores","title":"Dynamics Scores","text":""},{"location":"reference/score-computation/#velocity-control","title":"Velocity Control","text":"<p>Measures consistency of stroke dynamics.</p> <p>Formula:</p> \\[ \\text{velocity\\_control} = \\max(0, 100 - \\sigma_v \\times 2) \\] <p>Where:</p> <ul> <li>\\(\\sigma_v\\) = standard deviation of MIDI velocities (0-127 scale)</li> </ul> <p>Interpretation:</p> Velocity Std Dev Score 0 100 10 80 25 50 50 0"},{"location":"reference/score-computation/#accent-differentiation","title":"Accent Differentiation","text":"<p>Measures the dynamic contrast between accented and unaccented strokes.</p> <p>Formula:</p> <ol> <li> <p>Calculate velocity difference:    $\\(\\Delta v = \\bar{v}_{\\text{accent}} - \\bar{v}_{\\text{tap}}\\)$</p> </li> <li> <p>Convert to approximate dB:    $\\(\\Delta_{\\text{dB}} = \\frac{\\Delta v}{127} \\times 20\\)$</p> </li> <li> <p>Calculate score:    $\\(\\text{accent\\_differentiation} = \\min(100, \\max(0, \\Delta_{\\text{dB}} \\times 8))\\)$</p> </li> </ol> <p>Target Range:</p> dB Difference Score Quality 0 dB 0 No differentiation 6 dB 48 Minimal 12 dB 96 Good 15+ dB 100 Excellent"},{"location":"reference/score-computation/#accent-accuracy","title":"Accent Accuracy","text":"<p>Measures whether accents are placed on the correct beats.</p> <p>Formula:</p> \\[ \\text{accent\\_accuracy} = \\frac{n_{\\text{correct}}}{n_{\\text{total}}} \\times 100 \\] <p>Where:</p> <ul> <li>\\(n_{\\text{correct}}\\) = accented strokes with velocity above average</li> <li>\\(n_{\\text{total}}\\) = total accented strokes in the pattern</li> </ul>"},{"location":"reference/score-computation/#hand-balance-scores","title":"Hand Balance Scores","text":""},{"location":"reference/score-computation/#hand-balance-composite","title":"Hand Balance (Composite)","text":"<p>Measures evenness between left and right hand performance, combining velocity balance and timing balance equally.</p> <p>Formula:</p> \\[ \\text{hand\\_balance} = 0.5 \\times \\text{velocity\\_balance} + 0.5 \\times \\text{timing\\_balance} \\] <p>Velocity Balance:</p> \\[ \\text{velocity\\_balance} = \\frac{\\min(\\bar{v}_L, \\bar{v}_R)}{\\max(\\bar{v}_L, \\bar{v}_R)} \\times 100 \\] <p>Timing Balance:</p> \\[ \\text{timing\\_balance} = \\frac{\\min(\\bar{|e|}_L, \\bar{|e|}_R)}{\\max(\\bar{|e|}_L, \\bar{|e|}_R)} \\times 100 \\] <p>Where:</p> <ul> <li>\\(\\bar{v}_L\\), \\(\\bar{v}_R\\) = mean velocity for left/right hand</li> <li>\\(\\bar{|e|}_L\\), \\(\\bar{|e|}_R\\) = mean absolute timing error for left/right hand</li> </ul> <p>Composite Score Rationale</p> <p>Earlier versions used velocity-only balance, which exhibited a ceiling effect (nearly always high). Adding timing balance provides better discrimination between skill levels.</p>"},{"location":"reference/score-computation/#weak-hand-index","title":"Weak Hand Index","text":"<p>Identifies which hand is weaker for diagnostic purposes.</p> <p>Formula:</p> \\[ \\text{weak\\_hand\\_index} = \\frac{\\bar{|e|}_L}{\\bar{|e|}_L + \\bar{|e|}_R} \\times 100 \\] <p>Interpretation:</p> Value Meaning 0 Left hand much weaker 50 Balanced 100 Right hand much weaker"},{"location":"reference/score-computation/#rudiment-specific-scores","title":"Rudiment-Specific Scores","text":"<p>These scores are only computed for rudiments containing the relevant articulations.</p>"},{"location":"reference/score-computation/#flam-quality","title":"Flam Quality","text":"<p>Measures grace note spacing consistency for flam rudiments.</p> <p>Ideal Range: 20-40ms (center: 30ms)</p> <p>Computation:</p> <p>For each grace note with spacing \\(s\\) (ms from grace to primary):</p> \\[ \\text{score}_i = \\begin{cases} 100 &amp; \\text{if } 20 \\leq s \\leq 40 \\\\ 100 - (20 - s) \\times 5 &amp; \\text{if } s &lt; 20 \\text{ (too tight)} \\\\ 100 - (s - 40) \\times 3 &amp; \\text{if } s &gt; 40 \\text{ (too wide)} \\end{cases} \\] <p>Final Score:</p> \\[ \\text{flam\\_quality} = \\frac{1}{n} \\sum_{i=1}^{n} \\text{score}_i \\]"},{"location":"reference/score-computation/#diddle-quality","title":"Diddle Quality","text":"<p>Measures evenness of double strokes.</p> <p>Computation:</p> <p>For each diddle pair with actual gap \\(g_a\\) and intended gap \\(g_i\\):</p> \\[ r = \\frac{g_a}{g_i} \\] <p>Score:</p> \\[ \\text{diddle\\_quality} = \\max(0, 100 - \\bar{|r - 1|} \\times 200) \\] <p>Where perfect evenness yields \\(r = 1\\).</p>"},{"location":"reference/score-computation/#roll-sustain","title":"Roll Sustain","text":"<p>Measures velocity consistency across sustained rolls (penalizes decay).</p> <p>Computation:</p> <ol> <li>Calculate mean velocity for first quarter: \\(\\bar{v}_{\\text{start}}\\)</li> <li>Calculate mean velocity for last quarter: \\(\\bar{v}_{\\text{end}}\\)</li> <li>Calculate decay ratio:    $\\(\\text{decay} = \\frac{\\bar{v}_{\\text{end}}}{\\bar{v}_{\\text{start}}}\\)$</li> </ol> <p>Score:</p> \\[ \\text{roll\\_sustain} = \\min(100, \\text{decay} \\times 100) \\] Decay Ratio Score Interpretation 1.0 100 Perfect sustain 0.8 80 Slight decay 0.5 50 Significant decay"},{"location":"reference/score-computation/#composite-scores","title":"Composite Scores","text":""},{"location":"reference/score-computation/#overall-score","title":"Overall Score","text":"<p>Weighted average of all component scores.</p> <p>Formula:</p> \\[ \\text{overall\\_score} = \\sum_{i} w_i \\times s_i \\] <p>Weight Table:</p> Component Weight Rationale <code>timing_accuracy</code> 0.20 Most critical for rhythmic precision <code>timing_consistency</code> 0.15 Important for professional sound <code>tempo_stability</code> 0.10 Shows control over time <code>subdivision_evenness</code> 0.10 Essential for clean rudiments <code>velocity_control</code> 0.10 Dynamics consistency <code>accent_differentiation</code> 0.10 Musical expression <code>accent_accuracy</code> 0.10 Pattern correctness <code>hand_balance</code> 0.15 Technical evenness Total 1.00 <p>Rudiment-Specific Bonuses</p> <p>Rudiment-specific scores (flam_quality, diddle_quality, roll_sustain) are computed separately and not included in the overall score to maintain consistency across rudiment types.</p>"},{"location":"reference/score-computation/#groove-feel-proxy","title":"Groove Feel Proxy","text":"<p>A supplementary metric capturing the \"feel\" of the performance.</p> <p>Formula:</p> \\[ \\text{groove\\_feel} = f(\\text{micro-timing patterns, velocity contours}) \\] <p>This is a 0-1 scale metric computed in <code>labels/groove.py</code>. It captures intentional micro-timing deviations that contribute to musical groove rather than penalizing all deviations equally.</p>"},{"location":"reference/score-computation/#tier-confidence","title":"Tier Confidence","text":"<p>Measures how clearly a sample belongs to its assigned skill tier.</p> <p>Formula:</p> \\[ \\text{tier\\_confidence} = e^{-0.5 \\cdot z^2} \\] <p>Where:</p> \\[ z = \\frac{|\\text{overall\\_score} - \\mu_{\\text{tier}}|}{\\sigma_{\\text{tier}}} \\] <p>Tier Distribution Parameters:</p> Tier Mean (\\(\\mu\\)) Std Dev (\\(\\sigma\\)) Beginner 34.3 6.6 Intermediate 45.7 10.1 Advanced 61.0 10.8 Professional 73.5 10.7 <p>Interpretation:</p> Confidence Meaning &gt; 0.9 Clearly within tier 0.5-0.9 Typical for tier &lt; 0.5 Near tier boundary (label noise) <p>Using Tier Confidence</p> <p>When training classification models, consider:</p> <ul> <li>Filtering samples with <code>tier_confidence &lt; 0.3</code> to reduce label noise</li> <li>Using <code>tier_confidence</code> as sample weights</li> <li>Using the binary <code>skill_tier_binary</code> (novice/skilled) for cleaner separation</li> </ul>"},{"location":"reference/score-computation/#score-correlations","title":"Score Correlations","text":"<p>Understanding score correlations is important for multi-task learning.</p>"},{"location":"reference/score-computation/#high-correlation-pairs-r-07","title":"High Correlation Pairs (\\(r &gt; 0.7\\))","text":"Score 1 Score 2 Expected \\(r\\) Reason <code>timing_accuracy</code> <code>timing_consistency</code> 0.85-0.90 Both from timing errors <code>timing_accuracy</code> <code>subdivision_evenness</code> 0.75-0.85 Both measure rhythm <code>timing_consistency</code> <code>tempo_stability</code> 0.70-0.80 Consistent players don't drift <code>velocity_control</code> <code>accent_differentiation</code> 0.60-0.75 Both require dynamic control"},{"location":"reference/score-computation/#score-clusters","title":"Score Clusters","text":"<p>Cluster 1: Timing Quality</p> <ul> <li><code>timing_accuracy</code></li> <li><code>timing_consistency</code></li> <li><code>tempo_stability</code></li> <li><code>subdivision_evenness</code></li> </ul> <p>Cluster 2: Dynamics Quality</p> <ul> <li><code>velocity_control</code></li> <li><code>accent_differentiation</code></li> <li><code>accent_accuracy</code></li> </ul> <p>Cluster 3: Hand Balance</p> <ul> <li><code>hand_balance</code></li> <li><code>weak_hand_index</code></li> </ul> <p>Cluster 4: Rudiment-Specific (independent)</p> <ul> <li><code>flam_quality</code></li> <li><code>diddle_quality</code></li> <li><code>roll_sustain</code></li> </ul>"},{"location":"reference/score-computation/#recommendations","title":"Recommendations","text":"<p>For score regression:</p> <ul> <li>Predicting <code>overall_score</code> alone is sufficient for most use cases</li> <li>PCA on the 8 core scores captures ~95% variance in 3-4 components</li> </ul> <p>For multi-task learning:</p> <ul> <li>Weight timing cluster lower (high redundancy)</li> <li>Give rudiment-specific scores separate heads with masking</li> <li>Consider predicting: <code>overall_score</code> + <code>hand_balance</code> + rudiment-specific</li> </ul> <p>Minimal orthogonal score set:</p> <ol> <li><code>overall_score</code> (composite, always available)</li> <li><code>hand_balance</code> (independent axis)</li> <li><code>flam_quality</code> (when available)</li> <li><code>diddle_quality</code> (when available)</li> </ol>"},{"location":"reference/score-computation/#implementation-reference","title":"Implementation Reference","text":"<p>Score computation is implemented in <code>dataset_gen/labels/compute.py</code>:</p> <pre><code>def compute_exercise_scores(\n    stroke_labels: list[StrokeLabel],\n    events: list[StrokeEvent],\n    rudiment: Rudiment,\n) -&gt; ExerciseScores:\n    \"\"\"Compute all exercise-level scores.\"\"\"\n\n    # Timing scores (use corrected values from labels)\n    timing_errors = np.array([s.timing_error_ms for s in stroke_labels])\n    timing_errors_abs = np.abs(timing_errors)\n\n    # Timing accuracy: sigmoid scaling\n    mean_abs_error = np.mean(timing_errors_abs)\n    timing_accuracy = 100 * (1 / (1 + np.exp((mean_abs_error - 25) / 10)))\n\n    # ... additional scores computed similarly\n</code></pre>"},{"location":"reference/score-computation/#see-also","title":"See Also","text":"<ul> <li>Architecture - Pipeline overview showing where scores are computed</li> <li>Data Format - Schema for score columns in Parquet files</li> <li>Rudiment Schema - How rudiment definitions affect scoring</li> </ul>"},{"location":"user-guide/","title":"User Guide","text":"<p>This guide covers everything you need to use the SOUSA dataset effectively, from generation to ML training.</p>"},{"location":"user-guide/#quick-start","title":"Quick Start","text":"<pre><code># Install SOUSA\npip install -e .\n\n# Generate a small test dataset\npython scripts/generate_dataset.py --preset small\n\n# Generate with audio (requires soundfonts)\npython scripts/setup_soundfonts.py\npython scripts/generate_dataset.py --preset medium --with-audio\n</code></pre>"},{"location":"user-guide/#guide-sections","title":"Guide Sections","text":""},{"location":"user-guide/#dataset-generation","title":"Dataset Generation","text":"<p>Learn how to generate SOUSA datasets with different configurations:</p> <ul> <li>Generation presets - small, medium, and full configurations with storage estimates</li> <li>Command line options - customize profiles, tempos, augmentations, and more</li> <li>Reproducibility - seeding for deterministic generation</li> <li>Parallel generation - multi-worker support for faster generation</li> <li>Checkpoints - resumable generation for large datasets</li> </ul>"},{"location":"user-guide/#audio-augmentation","title":"Audio Augmentation","text":"<p>Understand the audio augmentation pipeline:</p> <ul> <li>Augmentation presets - 10+ presets from clean studio to lo-fi</li> <li>Signal chain - room simulation, mic modeling, compression, degradation</li> <li>Training impact - how augmentation affects model robustness</li> <li>Custom configurations - create your own augmentation profiles</li> </ul>"},{"location":"user-guide/#filtering-and-preprocessing","title":"Filtering and Preprocessing","text":"<p>Filter and prepare data for your specific use case:</p> <ul> <li>Skill tier filtering - beginner, intermediate, advanced, professional</li> <li>Rudiment selection - 40 PAS rudiments across 4 categories</li> <li>Score-based filtering - filter by timing accuracy, overall score, etc.</li> <li>Dataset statistics - understand your data distribution</li> </ul>"},{"location":"user-guide/#ml-training-recommendations","title":"ML Training Recommendations","text":"<p>Best practices for training models on SOUSA:</p> <ul> <li>Skill classification - binary vs 4-class, handling class imbalance</li> <li>Score regression - overall_score as primary target</li> <li>Multi-task learning - minimal orthogonal score sets</li> <li>Audio preprocessing - mel spectrograms, resampling, normalization</li> <li>PyTorch examples - DataLoader and training code</li> </ul>"},{"location":"user-guide/#validation","title":"Validation","text":"<p>Validate your generated datasets:</p> <ul> <li>Running validation reports - comprehensive quality checks</li> <li>Data integrity - 13 verification checks</li> <li>Realism validation - comparison to literature benchmarks</li> <li>Test suite - 26 test cases for validation infrastructure</li> </ul>"},{"location":"user-guide/#dataset-splits","title":"Dataset Splits","text":"<p>SOUSA uses profile-based splits to prevent data leakage:</p> Split Profiles Samples (~100K) Train 70% ~70,000 Validation 15% ~15,000 Test 15% ~15,000 <p>Why profile-based splits?</p> <p>All samples from a single player profile stay in the same split. This tests generalization to \"new players\" not seen during training, which is more realistic for deployment scenarios.</p>"},{"location":"user-guide/#skill-tier-distribution","title":"Skill Tier Distribution","text":"Tier Proportion Description Beginner 25% Learning basics, high timing variance Intermediate 35% Developing consistency Advanced 25% Strong technique, minor errors Professional 15% Near-perfect execution"},{"location":"user-guide/#loading-the-dataset","title":"Loading the Dataset","text":"HuggingFace HubLocal Files <pre><code>from datasets import load_dataset\n\n# Load full dataset\ndataset = load_dataset(\"zkeown/sousa\")\n\n# Load specific split\ntrain = load_dataset(\"zkeown/sousa\", split=\"train\")\n\n# Stream for memory efficiency\ndataset = load_dataset(\"zkeown/sousa\", streaming=True)\n</code></pre> <pre><code>from datasets import load_dataset\n\n# From local parquet files\ndataset = load_dataset(\n    \"parquet\",\n    data_dir=\"./output/dataset/hf_staging/data\"\n)\n</code></pre>"},{"location":"user-guide/#related-documentation","title":"Related Documentation","text":"<ul> <li>Data Format Reference - Complete schema documentation</li> <li>Limitations - Known limitations and biases</li> <li>Validation Guide - Validation infrastructure details</li> </ul>"},{"location":"user-guide/augmentation/","title":"Audio Augmentation","text":"<p>SOUSA includes a comprehensive audio augmentation pipeline that simulates various recording environments, microphone characteristics, and signal processing chains. This guide covers the available presets and how to configure custom augmentations.</p>"},{"location":"user-guide/augmentation/#augmentation-pipeline","title":"Augmentation Pipeline","text":"<p>The augmentation pipeline processes audio through four stages in sequence:</p> <pre><code>Raw Audio \u2192 Room Simulation \u2192 Mic Modeling \u2192 Recording Chain \u2192 Degradation \u2192 Output\n</code></pre> Stage Purpose Components Room Simulation Spatial acoustics Impulse response convolution, wet/dry mix Mic Modeling Microphone characteristics Frequency response, proximity effect, position Recording Chain Signal processing Preamp coloration, compression, EQ Degradation Quality reduction Noise, bit depth, wow/flutter"},{"location":"user-guide/augmentation/#augmentation-presets","title":"Augmentation Presets","text":"<p>SOUSA provides 10+ presets covering a range of recording scenarios:</p>"},{"location":"user-guide/augmentation/#clean-presets","title":"Clean Presets","text":""},{"location":"user-guide/augmentation/#clean_studio","title":"clean_studio","text":"<p>Professional studio recording with minimal coloration.</p> Parameter Value Room Studio, 20% wet Mic Condenser, 0.4m distance Preamp Clean Compression 3:1, -15dB threshold Noise -60dB <pre><code>from dataset_gen.audio_aug.pipeline import AugmentationPreset, augment_audio\n\naudio = augment_audio(raw_audio, preset=AugmentationPreset.CLEAN_STUDIO)\n</code></pre> <p>Use case</p> <p>Baseline recordings for controlled experiments, high-quality training data.</p>"},{"location":"user-guide/augmentation/#practice_dry","title":"practice_dry","text":"<p>Close-miked with minimal room sound.</p> Parameter Value Room Disabled Mic Dynamic, 0.15m distance Preamp Clean Compression 2:1 Noise -70dB <p>Use case</p> <p>Practice pad recordings, isolated snare captures.</p>"},{"location":"user-guide/augmentation/#room-variation-presets","title":"Room Variation Presets","text":""},{"location":"user-guide/augmentation/#studio_warm","title":"studio_warm","text":"<p>Warm studio sound with character.</p> Parameter Value Room Studio, 25% wet Mic Ribbon, 0.6m distance Preamp Warm, 30% drive Compression 6:1, -10dB threshold"},{"location":"user-guide/augmentation/#live_room","title":"live_room","text":"<p>Large reverberant space.</p> Parameter Value Room Concert hall, 40% wet Mic Condenser, 3.0m distance Position Distant Compression Disabled <p>Use case</p> <p>Simulating orchestral or ensemble recordings.</p>"},{"location":"user-guide/augmentation/#gym","title":"gym","text":"<p>Gymnasium acoustics (common for marching band).</p> Parameter Value Room Gym, 35% wet Mic Dynamic, 2.0m overhead Noise HVAC, -40dB"},{"location":"user-guide/augmentation/#degraded-presets","title":"Degraded Presets","text":""},{"location":"user-guide/augmentation/#lo_fi","title":"lo_fi","text":"<p>Low fidelity recording with significant artifacts.</p> Parameter Value Room Bedroom, 10% wet Mic Piezo, 0.3m Preamp Aggressive, 40% drive Compression 8:1, -8dB threshold Bit depth 12-bit High-pass 200 Hz Low-pass 6000 Hz Noise -25dB <p>Use case</p> <p>Training models to be robust to poor recording quality.</p>"},{"location":"user-guide/augmentation/#preset-comparison-table","title":"Preset Comparison Table","text":"Preset Room Mic Type Distance Noise (dB) Character <code>clean_studio</code> Studio Condenser 0.4m -60 Pristine <code>clean_close</code> None Dynamic 0.15m -70 Tight <code>practice_room</code> Practice Dynamic 0.5m -45 Natural <code>concert_hall</code> Hall Condenser 3.0m Low Spacious <code>gym</code> Gym Dynamic 2.0m -40 Reverberant <code>garage</code> Garage Dynamic 0.8m -35 Gritty <code>vintage_tape</code> Studio Ribbon 0.6m -35 Warm <code>lo_fi</code> Bedroom Piezo 0.3m -25 Degraded <code>phone_recording</code> Bedroom Piezo 1.5m -30 Phone quality <code>marching_field</code> Outdoor Dynamic 5.0m -35 Distant <code>indoor_competition</code> Gym Condenser 2.5m -40 Competition"},{"location":"user-guide/augmentation/#how-augmentation-affects-training","title":"How Augmentation Affects Training","text":""},{"location":"user-guide/augmentation/#domain-robustness","title":"Domain Robustness","text":"<p>Training on augmented data improves model robustness to real-world recording conditions:</p> <pre><code># Example: Training with mixed augmentation\ntrain_presets = [\n    AugmentationPreset.CLEAN_STUDIO,\n    AugmentationPreset.PRACTICE_ROOM,\n    AugmentationPreset.GYM,\n    AugmentationPreset.LO_FI,\n]\n\n# Filter dataset by augmentation preset\nclean_only = dataset.filter(lambda x: x[\"augmentation_preset\"] == \"clean_studio\")\nmixed = dataset.filter(lambda x: x[\"augmentation_preset\"] in [p.value for p in train_presets])\n</code></pre>"},{"location":"user-guide/augmentation/#augmentation-impact-on-metrics","title":"Augmentation Impact on Metrics","text":"Training Data Clean Test Acc Degraded Test Acc Clean only 95% 72% Mixed augmentation 93% 89% All presets 91% 91% <p>Trade-off</p> <p>Training on clean data only yields highest clean-test accuracy but poor generalization. Mixed augmentation balances both.</p>"},{"location":"user-guide/augmentation/#recommended-augmentation-strategy","title":"Recommended Augmentation Strategy","text":"<p>For most applications:</p> <ol> <li>Training: Use mixed augmentation (clean + moderate degradation)</li> <li>Validation: Use clean data for consistent metrics</li> <li>Test: Use held-out presets to test generalization</li> </ol> <pre><code># Split by augmentation for robust evaluation\ntrain = dataset.filter(\n    lambda x: x[\"augmentation_preset\"] in [\"clean_studio\", \"practice_room\", \"gym\"]\n)\nval = dataset.filter(lambda x: x[\"augmentation_preset\"] == \"clean_studio\")\ntest = dataset.filter(lambda x: x[\"augmentation_preset\"] in [\"lo_fi\", \"phone_recording\"])\n</code></pre>"},{"location":"user-guide/augmentation/#custom-augmentation-configuration","title":"Custom Augmentation Configuration","text":"<p>Create custom augmentation configurations for specific needs:</p> <pre><code>from dataset_gen.audio_aug.pipeline import (\n    AugmentationConfig,\n    AugmentationPipeline,\n)\nfrom dataset_gen.audio_aug.room import RoomType\nfrom dataset_gen.audio_aug.mic import MicType, MicPosition\nfrom dataset_gen.audio_aug.chain import PreampType\nfrom dataset_gen.audio_aug.degradation import NoiseType\n\n# Custom configuration\nconfig = AugmentationConfig(\n    # Room settings\n    room_enabled=True,\n    room_type=RoomType.PRACTICE_ROOM,\n    room_wet_dry=0.2,\n\n    # Mic settings\n    mic_enabled=True,\n    mic_type=MicType.DYNAMIC,\n    mic_position=MicPosition.CENTER,\n    mic_distance=0.3,\n\n    # Recording chain\n    chain_enabled=True,\n    preamp_type=PreampType.CLEAN,\n    preamp_drive=0.0,\n    compression_enabled=True,\n    compression_ratio=4.0,\n    compression_threshold_db=-12.0,\n\n    # EQ\n    eq_enabled=True,\n    highpass_freq=60.0,\n    lowpass_freq=16000.0,\n\n    # Degradation\n    degradation_enabled=True,\n    noise_type=NoiseType.PINK,\n    noise_level_db=-45.0,\n    bit_depth=None,  # No bit reduction\n    wow_flutter=0.0,\n\n    # Output\n    normalize=True,\n    target_peak_db=-1.0,\n)\n\n# Apply custom augmentation\npipeline = AugmentationPipeline(config=config, sample_rate=44100)\naugmented = pipeline.process(audio)\n</code></pre>"},{"location":"user-guide/augmentation/#configuration-parameters","title":"Configuration Parameters","text":""},{"location":"user-guide/augmentation/#room-configuration","title":"Room Configuration","text":"Parameter Type Range Description <code>room_enabled</code> bool - Enable room simulation <code>room_type</code> RoomType Enum Room acoustic type <code>room_wet_dry</code> float 0.0-1.0 Reverb wet/dry mix <code>ir_path</code> Path - Custom impulse response file <p>Available Room Types:</p> <ul> <li><code>STUDIO</code> - Recording studio</li> <li><code>PRACTICE_ROOM</code> - Small practice space</li> <li><code>BEDROOM</code> - Bedroom/home studio</li> <li><code>GARAGE</code> - Garage rehearsal space</li> <li><code>GYM</code> - Gymnasium</li> <li><code>CONCERT_HALL</code> - Large hall</li> <li><code>OUTDOOR</code> - Outdoor (minimal reverb)</li> </ul>"},{"location":"user-guide/augmentation/#mic-configuration","title":"Mic Configuration","text":"Parameter Type Range Description <code>mic_enabled</code> bool - Enable mic simulation <code>mic_type</code> MicType Enum Microphone type <code>mic_position</code> MicPosition Enum Mic placement <code>mic_distance</code> float 0.1-10.0m Distance from source <p>Available Mic Types:</p> <ul> <li><code>CONDENSER</code> - Large diaphragm condenser</li> <li><code>DYNAMIC</code> - Dynamic (SM57-style)</li> <li><code>RIBBON</code> - Ribbon microphone</li> <li><code>PIEZO</code> - Piezo contact mic</li> </ul> <p>Available Positions:</p> <ul> <li><code>CENTER</code> - On-axis, center</li> <li><code>EDGE</code> - Off-axis, edge</li> <li><code>OVERHEAD</code> - Above source</li> <li><code>DISTANT</code> - Far field</li> </ul>"},{"location":"user-guide/augmentation/#recording-chain-configuration","title":"Recording Chain Configuration","text":"Parameter Type Range Description <code>preamp_type</code> PreampType Enum Preamp character <code>preamp_drive</code> float 0.0-1.0 Saturation amount <code>compression_enabled</code> bool - Enable compression <code>compression_ratio</code> float 1.0-20.0 Compression ratio <code>compression_threshold_db</code> float -40 to 0 Threshold in dB <code>highpass_freq</code> float 20-500 Hz High-pass filter <code>lowpass_freq</code> float 4000-20000 Hz Low-pass filter <p>Available Preamp Types:</p> <ul> <li><code>CLEAN</code> - Transparent</li> <li><code>WARM</code> - Tube-style warmth</li> <li><code>AGGRESSIVE</code> - Solid-state edge</li> </ul>"},{"location":"user-guide/augmentation/#degradation-configuration","title":"Degradation Configuration","text":"Parameter Type Range Description <code>degradation_enabled</code> bool - Enable degradation <code>noise_type</code> NoiseType Enum Background noise type <code>noise_level_db</code> float -80 to -20 Noise level in dB <code>bit_depth</code> int 8-24 Output bit depth <code>wow_flutter</code> float 0.0-1.0 Tape-style pitch variation <p>Available Noise Types:</p> <ul> <li><code>PINK</code> - Pink noise</li> <li><code>TAPE_HISS</code> - Tape hiss</li> <li><code>HVAC</code> - HVAC/air conditioning</li> <li><code>ROOM_TONE</code> - General room tone</li> </ul>"},{"location":"user-guide/augmentation/#random-augmentation","title":"Random Augmentation","text":"<p>For data augmentation during training, use random augmentation:</p> <pre><code>from dataset_gen.audio_aug.pipeline import random_augmentation\n\n# Apply random preset with slight parameter variation\naugmented, config_used = random_augmentation(audio, seed=42)\n\nprint(f\"Applied preset: {config_used.preset_name}\")\nprint(f\"Room wet/dry: {config_used.room_wet_dry}\")\n</code></pre>"},{"location":"user-guide/augmentation/#filtering-by-augmentation","title":"Filtering by Augmentation","text":"<p>Filter datasets by augmentation characteristics:</p> <pre><code># By preset name\nclean = dataset.filter(lambda x: x[\"augmentation_preset\"] == \"clean_studio\")\n\n# By soundfont\nmarching = dataset.filter(lambda x: x[\"aug_soundfont\"] == \"marching\")\n\n# By noise level\nlow_noise = dataset.filter(lambda x: x[\"aug_noise_level_db\"] &lt; -50)\n\n# Link clean and augmented variants (same MIDI, different audio)\n# Group by the base sample (before augmentation)\nfrom collections import defaultdict\ngroups = defaultdict(list)\nfor sample in dataset:\n    base_id = sample[\"augmentation_group_id\"]\n    groups[base_id].append(sample)\n</code></pre>"},{"location":"user-guide/augmentation/#best-practices","title":"Best Practices","text":"<p>Recommended</p> <ul> <li>Train on diverse augmentation presets for robustness</li> <li>Use clean validation data for consistent metrics</li> <li>Document which presets were used in experiments</li> </ul> <p>Avoid</p> <ul> <li>Training only on clean data (poor generalization)</li> <li>Using extreme degradation for all samples (hurts clean performance)</li> <li>Mixing augmentation presets in validation set (inconsistent metrics)</li> </ul>"},{"location":"user-guide/filtering/","title":"Filtering and Preprocessing","text":"<p>This guide covers filtering SOUSA data by various criteria and understanding the dataset distribution for effective preprocessing.</p>"},{"location":"user-guide/filtering/#filtering-by-skill-tier","title":"Filtering by Skill Tier","text":""},{"location":"user-guide/filtering/#basic-skill-filtering","title":"Basic Skill Filtering","text":"<pre><code>from datasets import load_dataset\n\ndataset = load_dataset(\"zkeown/sousa\")\n\n# Filter by single skill tier\nbeginners = dataset[\"train\"].filter(lambda x: x[\"skill_tier\"] == \"beginner\")\nadvanced = dataset[\"train\"].filter(lambda x: x[\"skill_tier\"] == \"advanced\")\n\n# Multiple skill tiers\nnovice_players = dataset[\"train\"].filter(\n    lambda x: x[\"skill_tier\"] in [\"beginner\", \"intermediate\"]\n)\n</code></pre>"},{"location":"user-guide/filtering/#skill-tier-distribution","title":"Skill Tier Distribution","text":"Tier Proportion Full Dataset (~100K) Characteristics Beginner 25% ~25,000 High timing variance (50ms std), learning basics Intermediate 35% ~35,000 Moderate consistency, developing technique Advanced 25% ~25,000 Strong technique, minor timing errors Professional 15% ~15,000 Near-perfect (7ms std), consistent execution <p>Class imbalance</p> <p>Professional samples are ~2.3x less frequent than intermediate. Use class weights for classification tasks.</p>"},{"location":"user-guide/filtering/#binary-skill-labels","title":"Binary Skill Labels","text":"<p>For reduced overlap between classes:</p> <pre><code># Use binary labels (novice vs skilled)\nnovice = dataset[\"train\"].filter(lambda x: x[\"skill_tier_binary\"] == \"novice\")\nskilled = dataset[\"train\"].filter(lambda x: x[\"skill_tier_binary\"] == \"skilled\")\n\n# Binary mapping:\n# novice = beginner + intermediate\n# skilled = advanced + professional\n</code></pre>"},{"location":"user-guide/filtering/#filtering-by-tier-confidence","title":"Filtering by Tier Confidence","text":"<p>High-confidence samples are more clearly representative of their tier:</p> <pre><code># Filter for clear tier assignments (reduces overlap)\nconfident = dataset[\"train\"].filter(lambda x: x[\"tier_confidence\"] &gt; 0.5)\n\n# Very confident samples only\nvery_confident = dataset[\"train\"].filter(lambda x: x[\"tier_confidence\"] &gt; 0.7)\n</code></pre> Tier Confidence Description Use Case &gt; 0.7 Clear tier assignment Classification with clean labels 0.5 - 0.7 Moderate confidence General training &lt; 0.5 Borderline cases May hurt classification accuracy"},{"location":"user-guide/filtering/#filtering-by-rudiment","title":"Filtering by Rudiment","text":""},{"location":"user-guide/filtering/#single-rudiment","title":"Single Rudiment","text":"<pre><code># Specific rudiment\nparadiddles = dataset[\"train\"].filter(\n    lambda x: x[\"rudiment_slug\"] == \"single_paradiddle\"\n)\n\n# By rudiment name pattern\nflams = dataset[\"train\"].filter(lambda x: \"flam\" in x[\"rudiment_slug\"])\nrolls = dataset[\"train\"].filter(lambda x: \"roll\" in x[\"rudiment_slug\"])\n</code></pre>"},{"location":"user-guide/filtering/#rudiment-categories","title":"Rudiment Categories","text":"<p>All 40 PAS rudiments organized by category:</p> Roll Rudiments (15)Paradiddle Rudiments (5)Flam Rudiments (11)Drag Rudiments (9) Slug Name <code>single_stroke_roll</code> Single Stroke Roll <code>single_stroke_four</code> Single Stroke Four <code>single_stroke_seven</code> Single Stroke Seven <code>multiple_bounce_roll</code> Multiple Bounce Roll <code>triple_stroke_roll</code> Triple Stroke Roll <code>double_stroke_roll</code> Double Stroke Open Roll <code>five_stroke_roll</code> Five Stroke Roll <code>six_stroke_roll</code> Six Stroke Roll <code>seven_stroke_roll</code> Seven Stroke Roll <code>nine_stroke_roll</code> Nine Stroke Roll <code>ten_stroke_roll</code> Ten Stroke Roll <code>eleven_stroke_roll</code> Eleven Stroke Roll <code>thirteen_stroke_roll</code> Thirteen Stroke Roll <code>fifteen_stroke_roll</code> Fifteen Stroke Roll <code>seventeen_stroke_roll</code> Seventeen Stroke Roll Slug Name <code>single_paradiddle</code> Single Paradiddle <code>double_paradiddle</code> Double Paradiddle <code>triple_paradiddle</code> Triple Paradiddle <code>paradiddle_diddle</code> Paradiddle-Diddle <code>single_paradiddle_diddle</code> Single Paradiddle-Diddle Slug Name <code>flam</code> Flam <code>flam_accent</code> Flam Accent <code>flam_tap</code> Flam Tap <code>flamacue</code> Flamacue <code>flam_paradiddle</code> Flam Paradiddle <code>single_flammed_mill</code> Single Flammed Mill <code>flam_paradiddle_diddle</code> Flam Paradiddle-Diddle <code>pataflafla</code> Pataflafla <code>swiss_army_triplet</code> Swiss Army Triplet <code>inverted_flam_tap</code> Inverted Flam Tap <code>flam_drag</code> Flam Drag Slug Name <code>drag</code> Drag <code>single_drag_tap</code> Single Drag Tap <code>double_drag_tap</code> Double Drag Tap <code>lesson_25</code> Lesson 25 <code>single_dragadiddle</code> Single Dragadiddle <code>drag_paradiddle_1</code> Drag Paradiddle #1 <code>drag_paradiddle_2</code> Drag Paradiddle #2 <code>single_ratamacue</code> Single Ratamacue <code>double_ratamacue</code> Double Ratamacue <code>triple_ratamacue</code> Triple Ratamacue"},{"location":"user-guide/filtering/#filter-by-rudiment-category","title":"Filter by Rudiment Category","text":"<pre><code># Define category sets\nROLL_RUDIMENTS = [\n    \"single_stroke_roll\", \"single_stroke_four\", \"single_stroke_seven\",\n    \"multiple_bounce_roll\", \"triple_stroke_roll\", \"double_stroke_roll\",\n    \"five_stroke_roll\", \"six_stroke_roll\", \"seven_stroke_roll\",\n    \"nine_stroke_roll\", \"ten_stroke_roll\", \"eleven_stroke_roll\",\n    \"thirteen_stroke_roll\", \"fifteen_stroke_roll\", \"seventeen_stroke_roll\",\n]\n\nFLAM_RUDIMENTS = [\n    \"flam\", \"flam_accent\", \"flam_tap\", \"flamacue\", \"flam_paradiddle\",\n    \"single_flammed_mill\", \"flam_paradiddle_diddle\", \"pataflafla\",\n    \"swiss_army_triplet\", \"inverted_flam_tap\", \"flam_drag\",\n]\n\n# Filter by category\nrolls_only = dataset[\"train\"].filter(lambda x: x[\"rudiment_slug\"] in ROLL_RUDIMENTS)\nflams_only = dataset[\"train\"].filter(lambda x: x[\"rudiment_slug\"] in FLAM_RUDIMENTS)\n</code></pre>"},{"location":"user-guide/filtering/#filtering-by-tempo","title":"Filtering by Tempo","text":"<pre><code># Slow tempos (beginner-friendly)\nslow = dataset[\"train\"].filter(lambda x: x[\"tempo_bpm\"] &lt; 90)\n\n# Medium tempos\nmedium = dataset[\"train\"].filter(lambda x: 90 &lt;= x[\"tempo_bpm\"] &lt;= 130)\n\n# Fast tempos\nfast = dataset[\"train\"].filter(lambda x: x[\"tempo_bpm\"] &gt; 140)\n\n# Specific tempo range\ntarget_tempo = dataset[\"train\"].filter(lambda x: 100 &lt;= x[\"tempo_bpm\"] &lt;= 120)\n</code></pre>"},{"location":"user-guide/filtering/#tempo-distribution","title":"Tempo Distribution","text":"Tempo Range BPM Typical Skill Level Slow 60-90 All levels Medium 90-130 Intermediate+ Fast 130-160 Advanced+ Very Fast 160-180 Professional"},{"location":"user-guide/filtering/#filtering-by-score","title":"Filtering by Score","text":""},{"location":"user-guide/filtering/#overall-score","title":"Overall Score","text":"<pre><code># High performers (strong technique)\nhigh_score = dataset[\"train\"].filter(lambda x: x[\"overall_score\"] &gt; 80)\n\n# Low performers (beginners)\nlow_score = dataset[\"train\"].filter(lambda x: x[\"overall_score\"] &lt; 40)\n\n# Score range\nmid_range = dataset[\"train\"].filter(\n    lambda x: 40 &lt;= x[\"overall_score\"] &lt;= 70\n)\n</code></pre>"},{"location":"user-guide/filtering/#specific-score-metrics","title":"Specific Score Metrics","text":"<pre><code># By timing accuracy\ngood_timing = dataset[\"train\"].filter(lambda x: x[\"timing_accuracy\"] &gt; 70)\n\n# By hand balance\nbalanced = dataset[\"train\"].filter(lambda x: x[\"hand_balance\"] &gt; 90)\n\n# By velocity control\ncontrolled = dataset[\"train\"].filter(lambda x: x[\"velocity_control\"] &gt; 75)\n\n# Combined criteria\nelite = dataset[\"train\"].filter(\n    lambda x: x[\"timing_accuracy\"] &gt; 80\n    and x[\"hand_balance\"] &gt; 95\n    and x[\"overall_score\"] &gt; 85\n)\n</code></pre>"},{"location":"user-guide/filtering/#available-scores","title":"Available Scores","text":"Score Range Description <code>overall_score</code> 0-100 Weighted composite score <code>timing_accuracy</code> 0-100 How close to intended timing <code>timing_consistency</code> 0-100 Variance in timing errors <code>tempo_stability</code> 0-100 Consistency throughout exercise <code>velocity_control</code> 0-100 Control over stroke dynamics <code>hand_balance</code> 0-100 Evenness between L/R hands <code>accent_differentiation</code> 0-100 Clarity of accented strokes <code>accent_accuracy</code> 0-100 Correct accent placement"},{"location":"user-guide/filtering/#rudiment-specific-scores","title":"Rudiment-Specific Scores","text":"<p>These scores are only available for applicable rudiments:</p> <pre><code># Flam quality (flam rudiments only)\nflam_samples = dataset[\"train\"].filter(\n    lambda x: x.get(\"flam_quality\") is not None\n)\ngood_flams = flam_samples.filter(lambda x: x[\"flam_quality\"] &gt; 80)\n\n# Diddle quality (paradiddles, rolls with diddles)\ndiddle_samples = dataset[\"train\"].filter(\n    lambda x: x.get(\"diddle_quality\") is not None\n)\n\n# Roll sustain (roll rudiments)\nroll_samples = dataset[\"train\"].filter(\n    lambda x: x.get(\"roll_sustain\") is not None\n)\n</code></pre>"},{"location":"user-guide/filtering/#dataset-statistics","title":"Dataset Statistics","text":""},{"location":"user-guide/filtering/#computing-statistics","title":"Computing Statistics","text":"<pre><code>import pandas as pd\nfrom collections import Counter\n\n# Convert to pandas for analysis\ndf = dataset[\"train\"].to_pandas()\n\n# Basic counts\nprint(f\"Total samples: {len(df)}\")\nprint(f\"Unique profiles: {df['profile_id'].nunique()}\")\nprint(f\"Unique rudiments: {df['rudiment_slug'].nunique()}\")\n\n# Skill tier distribution\nskill_counts = Counter(df[\"skill_tier\"])\nfor tier, count in skill_counts.items():\n    print(f\"{tier}: {count} ({count/len(df)*100:.1f}%)\")\n\n# Score statistics\nprint(f\"\\nOverall Score: mean={df['overall_score'].mean():.1f}, \"\n      f\"std={df['overall_score'].std():.1f}\")\nprint(f\"Timing Accuracy: mean={df['timing_accuracy'].mean():.1f}, \"\n      f\"std={df['timing_accuracy'].std():.1f}\")\n</code></pre>"},{"location":"user-guide/filtering/#expected-statistics-full-dataset","title":"Expected Statistics (Full Dataset)","text":"Metric Mean Std Min Max Tempo (BPM) 118 34 60 180 Duration (sec) 5.9 2.9 1.6 20 Strokes/Sample 42 18 16 96 Overall Score 37 31 0 95 Timing Accuracy 37 31 0 95 Hand Balance 88 11 35 100"},{"location":"user-guide/filtering/#score-distribution-by-skill-tier","title":"Score Distribution by Skill Tier","text":"Tier Overall Score Timing Accuracy Hand Balance Beginner 15-45 5-35 75-85 Intermediate 35-65 30-60 85-92 Advanced 60-85 55-85 92-96 Professional 80-95 75-95 95-100"},{"location":"user-guide/filtering/#preprocessing-recommendations","title":"Preprocessing Recommendations","text":""},{"location":"user-guide/filtering/#for-classification-tasks","title":"For Classification Tasks","text":"<pre><code># Recommended preprocessing for skill classification\nfrom sklearn.model_selection import train_test_split\n\n# Filter for confident tier assignments\ndf_confident = df[df[\"tier_confidence\"] &gt; 0.5]\n\n# Ensure balanced classes with stratification\nX = df_confident[\"sample_id\"].values\ny = df_confident[\"skill_tier\"].values\n\n# Note: SOUSA already has profile-based splits\n# Use the provided splits rather than random splitting\ntrain_ids = set(splits[\"train_profile_ids\"])\nval_ids = set(splits[\"val_profile_ids\"])\ntest_ids = set(splits[\"test_profile_ids\"])\n</code></pre>"},{"location":"user-guide/filtering/#for-regression-tasks","title":"For Regression Tasks","text":"<pre><code># Normalize scores to 0-1 range\ndf[\"overall_score_norm\"] = df[\"overall_score\"] / 100.0\ndf[\"timing_accuracy_norm\"] = df[\"timing_accuracy\"] / 100.0\n\n# Remove outliers (optional)\nq_low = df[\"overall_score\"].quantile(0.01)\nq_high = df[\"overall_score\"].quantile(0.99)\ndf_filtered = df[(df[\"overall_score\"] &gt;= q_low) &amp; (df[\"overall_score\"] &lt;= q_high)]\n</code></pre>"},{"location":"user-guide/filtering/#handling-null-values","title":"Handling Null Values","text":"<p>Some scores are null for non-applicable rudiments:</p> <pre><code># Option 1: Filter to non-null\nflam_data = df[df[\"flam_quality\"].notna()]\n\n# Option 2: Fill with sentinel value (for masking)\ndf[\"flam_quality_filled\"] = df[\"flam_quality\"].fillna(-1)\n\n# Option 3: Separate handling by rudiment type\nhas_flams = df[\"rudiment_slug\"].isin(FLAM_RUDIMENTS)\nhas_diddles = df[\"rudiment_slug\"].isin(DIDDLE_RUDIMENTS)\n</code></pre>"},{"location":"user-guide/filtering/#common-filter-combinations","title":"Common Filter Combinations","text":""},{"location":"user-guide/filtering/#clean-high-quality-training-data","title":"Clean High-Quality Training Data","text":"<pre><code># High quality + clean augmentation + confident labels\nquality_data = dataset[\"train\"].filter(\n    lambda x: x[\"overall_score\"] &gt; 60\n    and x[\"augmentation_preset\"] == \"clean_studio\"\n    and x[\"tier_confidence\"] &gt; 0.6\n)\n</code></pre>"},{"location":"user-guide/filtering/#beginner-focused-subset","title":"Beginner-Focused Subset","text":"<pre><code># Beginner samples at slow tempos\nbeginner_practice = dataset[\"train\"].filter(\n    lambda x: x[\"skill_tier\"] == \"beginner\"\n    and x[\"tempo_bpm\"] &lt; 100\n)\n</code></pre>"},{"location":"user-guide/filtering/#specific-rudiment-study","title":"Specific Rudiment Study","text":"<pre><code># All paradiddles across skill levels\nparadiddle_study = dataset[\"train\"].filter(\n    lambda x: \"paradiddle\" in x[\"rudiment_slug\"]\n    and x[\"diddle_quality\"] is not None\n)\n</code></pre>"},{"location":"user-guide/filtering/#cross-soundfont-generalization","title":"Cross-Soundfont Generalization","text":"<pre><code># Train on some soundfonts, test on others\ntrain_soundfonts = [\"generalu\", \"marching\", \"mtpowerd\"]\ntest_soundfonts = [\"douglasn\", \"fluidr3\"]\n\ntrain_data = dataset[\"train\"].filter(\n    lambda x: x[\"aug_soundfont\"] in train_soundfonts\n)\ntest_data = dataset[\"test\"].filter(\n    lambda x: x[\"aug_soundfont\"] in test_soundfonts\n)\n</code></pre>"},{"location":"user-guide/filtering/#data-export","title":"Data Export","text":""},{"location":"user-guide/filtering/#export-filtered-data","title":"Export Filtered Data","text":"<pre><code># Export to CSV\nfiltered_df = df[df[\"skill_tier\"] == \"professional\"]\nfiltered_df.to_csv(\"professional_samples.csv\", index=False)\n\n# Export to Parquet (recommended for large datasets)\nfiltered_df.to_parquet(\"professional_samples.parquet\")\n\n# Export sample IDs for reproducibility\nwith open(\"filtered_ids.txt\", \"w\") as f:\n    for sample_id in filtered_df[\"sample_id\"]:\n        f.write(f\"{sample_id}\\n\")\n</code></pre>"},{"location":"user-guide/filtering/#creating-subsets","title":"Creating Subsets","text":"<pre><code>from datasets import Dataset\n\n# Create a HuggingFace dataset from filtered data\nsubset_df = df[df[\"skill_tier\"].isin([\"advanced\", \"professional\"])]\nsubset_dataset = Dataset.from_pandas(subset_df)\n\n# Push to Hub (optional)\nsubset_dataset.push_to_hub(\"username/sousa-advanced-only\")\n</code></pre>"},{"location":"user-guide/generation/","title":"Dataset Generation","text":"<p>This guide covers generating SOUSA datasets with different configurations for testing, development, and production use.</p>"},{"location":"user-guide/generation/#generation-presets","title":"Generation Presets","text":"<p>SOUSA provides three presets optimized for different use cases:</p>"},{"location":"user-guide/generation/#small-preset-testing","title":"Small Preset (Testing)","text":"<pre><code>python scripts/generate_dataset.py --preset small\n</code></pre> Parameter Value Profiles 10 Tempos per rudiment 3 Augmentations per sample 1 Total samples ~1,200 <p>Storage estimates:</p> Component Size MIDI files ~2.4 MB Labels (Parquet) ~0.6 MB Audio (if enabled) ~1.8 GB Total (MIDI only) ~3 MB <p>Use case</p> <p>Quick testing of pipeline changes, CI/CD workflows, and development iteration.</p>"},{"location":"user-guide/generation/#medium-preset-development","title":"Medium Preset (Development)","text":"<pre><code>python scripts/generate_dataset.py --preset medium\n</code></pre> Parameter Value Profiles 50 Tempos per rudiment 3 Augmentations per sample 2 Total samples ~12,000 <p>Storage estimates:</p> Component Size MIDI files ~24 MB Labels (Parquet) ~6 MB Audio (if enabled) ~18 GB Total (MIDI only) ~30 MB <p>Use case</p> <p>Model development, hyperparameter tuning, and ablation studies.</p>"},{"location":"user-guide/generation/#full-preset-production","title":"Full Preset (Production)","text":"<pre><code>python scripts/generate_dataset.py --preset full\n</code></pre> Parameter Value Profiles 100 Tempos per rudiment 5 Augmentations per sample 5 Total samples ~100,000 <p>Storage estimates:</p> Component Size MIDI files ~200 MB Labels (Parquet) ~50 MB Audio (if enabled) ~150 GB Total (MIDI only) ~250 MB <p>Use case</p> <p>Final model training, benchmark evaluation, and dataset releases.</p>"},{"location":"user-guide/generation/#command-line-options","title":"Command Line Options","text":""},{"location":"user-guide/generation/#basic-options","title":"Basic Options","text":"<pre><code>python scripts/generate_dataset.py [OPTIONS]\n</code></pre> Option Short Default Description <code>--output</code> <code>-o</code> <code>output/dataset</code> Output directory <code>--preset</code> None Use preset (small/medium/full) <code>--profiles</code> <code>-p</code> 100 Number of player profiles <code>--tempos</code> <code>-t</code> 5 Tempos per rudiment <code>--augmentations</code> <code>-a</code> 5 Augmented versions per sample <code>--seed</code> 42 Random seed for reproducibility <code>--quiet</code> <code>-q</code> False Reduce output verbosity"},{"location":"user-guide/generation/#audio-options","title":"Audio Options","text":"<pre><code>python scripts/generate_dataset.py --with-audio --soundfont path/to/soundfonts\n</code></pre> Option Description <code>--with-audio</code> Generate audio files (requires soundfonts) <code>--soundfont</code> Path to .sf2 file or directory <p>Soundfont setup</p> <p>Audio generation requires soundfonts. Run <code>python scripts/setup_soundfonts.py</code> first.</p>"},{"location":"user-guide/generation/#parallel-processing","title":"Parallel Processing","text":"<pre><code>python scripts/generate_dataset.py --workers 8\n</code></pre> Option Short Default Description <code>--workers</code> <code>-w</code> 1 Number of parallel workers <p>Set <code>--workers 0</code> to auto-detect CPU count.</p>"},{"location":"user-guide/generation/#validation","title":"Validation","text":"<pre><code>python scripts/generate_dataset.py --skip-validation\n</code></pre> Option Description <code>--skip-validation</code> Skip validation report generation"},{"location":"user-guide/generation/#examples","title":"Examples","text":""},{"location":"user-guide/generation/#quick-test-generation","title":"Quick Test Generation","text":"<pre><code># Minimal dataset for testing\npython scripts/generate_dataset.py --preset small -o output/test_dataset\n</code></pre>"},{"location":"user-guide/generation/#development-with-audio","title":"Development with Audio","text":"<pre><code># Medium dataset with audio for model development\npython scripts/generate_dataset.py \\\n    --preset medium \\\n    --with-audio \\\n    --workers 4 \\\n    -o output/dev_dataset\n</code></pre>"},{"location":"user-guide/generation/#production-generation","title":"Production Generation","text":"<pre><code># Full dataset with parallel processing\npython scripts/generate_dataset.py \\\n    --preset full \\\n    --with-audio \\\n    --workers 0 \\\n    --seed 42 \\\n    -o output/production_dataset\n</code></pre>"},{"location":"user-guide/generation/#custom-configuration","title":"Custom Configuration","text":"<pre><code># Override preset values\npython scripts/generate_dataset.py \\\n    --preset medium \\\n    --profiles 30 \\\n    --tempos 4 \\\n    --augmentations 3\n</code></pre>"},{"location":"user-guide/generation/#reproducibility-and-seeding","title":"Reproducibility and Seeding","text":"<p>SOUSA supports deterministic generation through seeding:</p> <pre><code># Both commands produce identical datasets\npython scripts/generate_dataset.py --seed 42 --preset small\npython scripts/generate_dataset.py --seed 42 --preset small\n</code></pre>"},{"location":"user-guide/generation/#seed-behavior","title":"Seed Behavior","text":"<ul> <li>Profile generation: Same seed produces identical player profiles</li> <li>MIDI generation: Timing/velocity variations are deterministic per seed</li> <li>Audio synthesis: FluidSynth rendering is deterministic</li> <li>Augmentation: Random augmentations use seed-derived values</li> </ul> <p>Parallel generation seeds</p> <p>When using multiple workers, each worker uses <code>seed + worker_id</code> to ensure reproducibility while avoiding identical samples across workers.</p>"},{"location":"user-guide/generation/#recommended-seeds","title":"Recommended Seeds","text":"<p>For benchmarking and reproducibility, document your seed:</p> <pre><code># In your experiment config\nDATASET_CONFIG = {\n    \"seed\": 42,\n    \"preset\": \"full\",\n    \"version\": \"1.0.0\",\n}\n</code></pre>"},{"location":"user-guide/generation/#generation-pipeline","title":"Generation Pipeline","text":"<p>The generation process follows this flow:</p> <pre><code>1. Load Rudiments (40 YAML definitions)\n         \u2193\n2. Generate Player Profiles\n         \u2193\n3. Assign Train/Val/Test Splits\n         \u2193\n4. For each Profile \u00d7 Rudiment \u00d7 Tempo:\n   a. Generate base MIDI performance\n   b. Apply profile-based timing/velocity variations\n   c. Compute hierarchical labels (stroke \u2192 measure \u2192 exercise)\n   d. For each augmentation variant:\n      - Synthesize audio (if enabled)\n      - Apply augmentation preset\n      - Write to storage\n         \u2193\n5. Write Parquet files (samples, exercises, measures, strokes)\n         \u2193\n6. Run validation (unless skipped)\n</code></pre>"},{"location":"user-guide/generation/#output-structure","title":"Output Structure","text":"<pre><code>output/dataset/\n\u251c\u2500\u2500 audio/                    # FLAC audio files (if --with-audio)\n\u2502   \u2514\u2500\u2500 {sample_id}.flac\n\u251c\u2500\u2500 midi/                     # Standard MIDI files\n\u2502   \u2514\u2500\u2500 {sample_id}.mid\n\u251c\u2500\u2500 labels/                   # Parquet files\n\u2502   \u251c\u2500\u2500 samples.parquet       # Sample metadata\n\u2502   \u251c\u2500\u2500 exercises.parquet     # Exercise-level scores\n\u2502   \u251c\u2500\u2500 measures.parquet      # Measure-level statistics\n\u2502   \u2514\u2500\u2500 strokes.parquet       # Stroke-level events\n\u251c\u2500\u2500 splits.json               # Train/val/test assignments\n\u2514\u2500\u2500 validation_report.json    # Validation results\n</code></pre>"},{"location":"user-guide/generation/#resumable-generation","title":"Resumable Generation","text":"<p>Coming soon</p> <p>Checkpoint-based resumable generation is planned for a future release.</p> <p>For now, if generation is interrupted:</p> <ol> <li>Check the <code>output/dataset/labels/</code> directory for partially written parquet files</li> <li>Delete the incomplete output directory</li> <li>Re-run generation with the same seed</li> </ol>"},{"location":"user-guide/generation/#performance-tips","title":"Performance Tips","text":""},{"location":"user-guide/generation/#memory-usage","title":"Memory Usage","text":"<p>For large datasets, consider:</p> <pre><code># Use more workers with smaller batches\npython scripts/generate_dataset.py --preset full --workers 8\n</code></pre>"},{"location":"user-guide/generation/#disk-io","title":"Disk I/O","text":"<ul> <li>Use an SSD for the output directory</li> <li>Audio generation is I/O intensive; consider generating MIDI-only first</li> </ul>"},{"location":"user-guide/generation/#parallel-scaling","title":"Parallel Scaling","text":"Workers Speedup (approx) Notes 1 1x Baseline 4 3.5x Good for most systems 8 6x Diminishing returns 16+ 7-8x Limited by I/O"},{"location":"user-guide/generation/#validation-after-generation","title":"Validation After Generation","text":"<p>By default, validation runs automatically after generation:</p> <pre><code>=== SOUSA Validation Report ===\nDataset: output/dataset\nGenerated: 2026-01-25T21:18:35\n\nSamples: 99,770\nProfiles: 100\nRudiments: 40\n\nVerification: 13/13 checks passed\nLiterature validation: 100% pass rate\nCorrelation checks: 80% pass rate\n</code></pre> <p>See Validation Guide for details on interpreting results.</p>"},{"location":"user-guide/ml-training/","title":"ML Training Recommendations","text":"<p>This guide provides best practices for training machine learning models on SOUSA, including task-specific recommendations, audio preprocessing, and code examples.</p>"},{"location":"user-guide/ml-training/#task-overview","title":"Task Overview","text":"Task Target Difficulty Recommended Approach Skill Classification (4-class) <code>skill_tier</code> Medium Filter by <code>tier_confidence</code>, use class weights Skill Classification (Binary) <code>skill_tier_binary</code> Easy Less overlap, simpler model Score Regression <code>overall_score</code> Easy MSE loss, normalized target Multi-task Learning Multiple scores Hard Use orthogonal targets Rudiment Classification <code>rudiment_slug</code> Medium 40-class classification"},{"location":"user-guide/ml-training/#skill-classification","title":"Skill Classification","text":""},{"location":"user-guide/ml-training/#4-class-classification","title":"4-Class Classification","text":"<p>Classify samples into beginner, intermediate, advanced, or professional.</p> <p>Classification Ceiling</p> <p>Adjacent skill tiers have 67-83% score overlap, creating an inherent accuracy ceiling of ~70-80%. This is realistic (skill exists on a continuum) but affects model evaluation.</p>"},{"location":"user-guide/ml-training/#recommended-setup","title":"Recommended Setup","text":"<pre><code>from datasets import load_dataset\nfrom collections import Counter\nimport torch\nimport torch.nn as nn\n\n# Load dataset\ndataset = load_dataset(\"zkeown/sousa\")\n\n# Filter for confident tier assignments\ntrain = dataset[\"train\"].filter(lambda x: x[\"tier_confidence\"] &gt; 0.5)\nval = dataset[\"validation\"].filter(lambda x: x[\"tier_confidence\"] &gt; 0.5)\n\n# Label mapping\nlabel2id = {\n    \"beginner\": 0,\n    \"intermediate\": 1,\n    \"advanced\": 2,\n    \"professional\": 3,\n}\nid2label = {v: k for k, v in label2id.items()}\nnum_labels = len(label2id)\n</code></pre>"},{"location":"user-guide/ml-training/#class-weights-required","title":"Class Weights (Required)","text":"<p>The 4.26:1 class imbalance requires class weights:</p> <pre><code># Compute inverse frequency weights\nlabels = [sample[\"skill_tier\"] for sample in train]\ncounts = Counter(labels)\ntotal = sum(counts.values())\n\nweights = {k: total / (len(counts) * v) for k, v in counts.items()}\nclass_weights = torch.tensor([\n    weights[\"beginner\"],\n    weights[\"intermediate\"],\n    weights[\"advanced\"],\n    weights[\"professional\"],\n])\n\n# Use in loss function\ncriterion = nn.CrossEntropyLoss(weight=class_weights)\n</code></pre>"},{"location":"user-guide/ml-training/#evaluation-metrics","title":"Evaluation Metrics","text":"<p>Always report balanced accuracy, not just accuracy:</p> <pre><code>from sklearn.metrics import (\n    balanced_accuracy_score,\n    classification_report,\n    confusion_matrix,\n)\n\n# Predict on test set\ny_true = [sample[\"skill_tier\"] for sample in test]\ny_pred = model.predict(test)\n\n# Balanced accuracy (handles class imbalance)\nbalanced_acc = balanced_accuracy_score(y_true, y_pred)\nprint(f\"Balanced Accuracy: {balanced_acc:.3f}\")\n\n# Per-class metrics\nprint(classification_report(y_true, y_pred, target_names=list(label2id.keys())))\n\n# Confusion matrix\ncm = confusion_matrix(y_true, y_pred, labels=list(label2id.keys()))\n</code></pre>"},{"location":"user-guide/ml-training/#binary-classification-recommended","title":"Binary Classification (Recommended)","text":"<p>For cleaner labels with less overlap:</p> <pre><code># Use binary labels\ntrain = dataset[\"train\"]\n\n# Binary: novice (beginner + intermediate) vs skilled (advanced + professional)\ndef add_binary_label(sample):\n    sample[\"label\"] = 0 if sample[\"skill_tier_binary\"] == \"novice\" else 1\n    return sample\n\ntrain = train.map(add_binary_label)\n\n# Simpler loss (still use class weights if imbalanced)\ncriterion = nn.BCEWithLogitsLoss()\n</code></pre> <p>Expected Performance:</p> Approach Balanced Accuracy Notes 4-class, no filtering ~65-70% Overlap limits performance 4-class, tier_confidence &gt; 0.5 ~75-80% Cleaner labels Binary classification ~85-90% Less overlap"},{"location":"user-guide/ml-training/#score-regression","title":"Score Regression","text":""},{"location":"user-guide/ml-training/#overall-score-as-target","title":"Overall Score as Target","text":"<p>The simplest and most effective regression target:</p> <pre><code>import torch\nimport torch.nn as nn\n\n# Normalize to 0-1 range\ndef preprocess(sample):\n    sample[\"target\"] = sample[\"overall_score\"] / 100.0\n    return sample\n\ntrain = dataset[\"train\"].map(preprocess)\n\n# MSE loss\ncriterion = nn.MSELoss()\n\n# Or use Huber loss for robustness to outliers\ncriterion = nn.HuberLoss(delta=0.1)\n</code></pre>"},{"location":"user-guide/ml-training/#why-overall-score","title":"Why Overall Score?","text":"<p>The <code>overall_score</code> captures the correlated cluster of timing/consistency scores:</p> <pre><code>timing_accuracy \u2194 timing_consistency: r = 0.89\ntiming_accuracy \u2194 overall_score: r = 0.88\n</code></pre> <p>Using individual scores as separate targets provides redundant signal.</p>"},{"location":"user-guide/ml-training/#evaluation-metrics_1","title":"Evaluation Metrics","text":"<pre><code>from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n\ny_true = [sample[\"overall_score\"] for sample in test]\ny_pred = model.predict(test) * 100  # Denormalize\n\nmse = mean_squared_error(y_true, y_pred)\nmae = mean_absolute_error(y_true, y_pred)\nr2 = r2_score(y_true, y_pred)\n\nprint(f\"MSE: {mse:.2f}\")\nprint(f\"MAE: {mae:.2f}\")\nprint(f\"R2: {r2:.3f}\")\n</code></pre>"},{"location":"user-guide/ml-training/#multi-task-learning","title":"Multi-Task Learning","text":""},{"location":"user-guide/ml-training/#minimal-orthogonal-score-set","title":"Minimal Orthogonal Score Set","text":"<p>Avoid using all scores (high redundancy). Use these orthogonal targets:</p> <pre><code># Recommended minimal set\nTARGETS = [\"overall_score\", \"tempo_stability\"]\n\ndef preprocess_multitask(sample):\n    sample[\"targets\"] = {\n        \"overall_score\": sample[\"overall_score\"] / 100.0,\n        \"tempo_stability\": sample[\"tempo_stability\"] / 100.0,\n    }\n    return sample\n</code></pre>"},{"location":"user-guide/ml-training/#multi-head-architecture","title":"Multi-Head Architecture","text":"<pre><code>import torch\nimport torch.nn as nn\n\nclass MultiTaskHead(nn.Module):\n    def __init__(self, input_dim, num_tasks=2):\n        super().__init__()\n        self.heads = nn.ModuleList([\n            nn.Linear(input_dim, 1) for _ in range(num_tasks)\n        ])\n\n    def forward(self, x):\n        return [head(x).squeeze(-1) for head in self.heads]\n\n# Combined loss\ndef multi_task_loss(predictions, targets, weights=None):\n    if weights is None:\n        weights = [1.0] * len(predictions)\n\n    loss = 0\n    for pred, target, weight in zip(predictions, targets, weights):\n        loss += weight * nn.functional.mse_loss(pred, target)\n\n    return loss\n</code></pre>"},{"location":"user-guide/ml-training/#handling-rudiment-specific-scores","title":"Handling Rudiment-Specific Scores","text":"<p>Some scores are null for non-applicable rudiments:</p> <pre><code># Mask-based loss for optional scores\ndef masked_mse_loss(predictions, targets, mask):\n    \"\"\"Compute MSE only where mask is True.\"\"\"\n    if mask.sum() == 0:\n        return torch.tensor(0.0)\n\n    masked_pred = predictions[mask]\n    masked_target = targets[mask]\n    return nn.functional.mse_loss(masked_pred, masked_target)\n\n# Example: flam_quality is only valid for flam rudiments\nflam_mask = torch.tensor([\n    sample[\"flam_quality\"] is not None for sample in batch\n])\nif flam_mask.any():\n    flam_loss = masked_mse_loss(flam_pred, flam_target, flam_mask)\n</code></pre>"},{"location":"user-guide/ml-training/#audio-preprocessing","title":"Audio Preprocessing","text":""},{"location":"user-guide/ml-training/#resampling","title":"Resampling","text":"<p>SOUSA audio is 44.1kHz. Many pretrained models expect 16kHz:</p> <pre><code>import torchaudio\nimport torchaudio.transforms as T\n\n# Resample to 16kHz\nresampler = T.Resample(orig_freq=44100, new_freq=16000)\n\ndef resample_audio(sample):\n    waveform = torch.tensor(sample[\"audio\"][\"array\"])\n    if waveform.dim() == 1:\n        waveform = waveform.unsqueeze(0)\n    resampled = resampler(waveform).squeeze(0)\n    sample[\"audio\"][\"array\"] = resampled.numpy()\n    sample[\"audio\"][\"sampling_rate\"] = 16000\n    return sample\n</code></pre> <p>Sample Rate by Model:</p> Model Expected Rate Wav2Vec2 16,000 Hz HuBERT 16,000 Hz Whisper 16,000 Hz AST 16,000 Hz CLAP 48,000 Hz Custom CNN Any (use native 44,100 Hz)"},{"location":"user-guide/ml-training/#mel-spectrogram-features","title":"Mel-Spectrogram Features","text":"<p>For spectrogram-based models:</p> <pre><code>import torchaudio.transforms as T\n\n# Recommended parameters for drum audio\nmel_transform = T.MelSpectrogram(\n    sample_rate=16000,      # After resampling\n    n_fft=1024,             # ~64ms window\n    hop_length=256,         # ~16ms hop\n    n_mels=128,             # Frequency bins\n    f_min=20,               # Capture low fundamentals\n    f_max=8000,             # Most drum energy &lt; 8kHz\n)\n\n# Convert to log scale\namplitude_to_db = T.AmplitudeToDB(stype=\"power\", top_db=80)\n\ndef extract_mel_spectrogram(audio_array):\n    waveform = torch.tensor(audio_array).unsqueeze(0)\n    mel = mel_transform(waveform)\n    mel_db = amplitude_to_db(mel)\n    return mel_db.squeeze(0).numpy()\n</code></pre> <p>For onset detection (precise stroke timing):</p> <pre><code>onset_mel = T.MelSpectrogram(\n    sample_rate=16000,\n    n_fft=512,\n    hop_length=128,  # ~8ms for precise timing\n    n_mels=64,\n)\n</code></pre>"},{"location":"user-guide/ml-training/#normalization","title":"Normalization","text":"<p>Per-sample normalization:</p> <pre><code>def normalize_audio(audio_array):\n    \"\"\"Normalize to [-1, 1] range.\"\"\"\n    max_val = np.abs(audio_array).max()\n    if max_val &gt; 0:\n        return audio_array / max_val\n    return audio_array\n</code></pre> <p>Per-frequency-bin normalization (spectrograms):</p> <pre><code>def normalize_spectrogram(mel_db):\n    \"\"\"Per-channel (frequency bin) normalization.\"\"\"\n    mean = mel_db.mean(axis=-1, keepdims=True)\n    std = mel_db.std(axis=-1, keepdims=True) + 1e-6\n    return (mel_db - mean) / std\n</code></pre>"},{"location":"user-guide/ml-training/#handling-variable-length","title":"Handling Variable Length","text":"<p>SOUSA samples vary in duration (4-12 seconds typical).</p> <p>Padding/Truncation:</p> <pre><code>TARGET_LENGTH = 5 * 16000  # 5 seconds at 16kHz\n\ndef pad_or_truncate(audio_array):\n    if len(audio_array) &gt; TARGET_LENGTH:\n        return audio_array[:TARGET_LENGTH]\n    elif len(audio_array) &lt; TARGET_LENGTH:\n        padding = TARGET_LENGTH - len(audio_array)\n        return np.pad(audio_array, (0, padding), mode='constant')\n    return audio_array\n</code></pre> <p>Dynamic batching:</p> <pre><code>from torch.nn.utils.rnn import pad_sequence\n\ndef collate_variable_length(batch):\n    waveforms = [torch.tensor(s[\"audio\"][\"array\"]) for s in batch]\n    lengths = torch.tensor([len(w) for w in waveforms])\n\n    # Pad to max length in batch\n    padded = pad_sequence(waveforms, batch_first=True)\n\n    labels = torch.tensor([s[\"overall_score\"] / 100.0 for s in batch])\n\n    return {\n        \"waveforms\": padded,\n        \"lengths\": lengths,\n        \"labels\": labels,\n    }\n\ndataloader = DataLoader(\n    dataset,\n    batch_size=16,\n    collate_fn=collate_variable_length\n)\n</code></pre>"},{"location":"user-guide/ml-training/#pytorch-dataloader-examples","title":"PyTorch DataLoader Examples","text":""},{"location":"user-guide/ml-training/#basic-dataloader","title":"Basic DataLoader","text":"<pre><code>from torch.utils.data import DataLoader, Dataset\nfrom datasets import load_dataset\n\nclass SOUSADataset(Dataset):\n    def __init__(self, hf_dataset, transform=None):\n        self.data = hf_dataset\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        sample = self.data[idx]\n\n        # Extract audio\n        audio = torch.tensor(sample[\"audio\"][\"array\"], dtype=torch.float32)\n\n        # Extract label\n        label = torch.tensor(sample[\"overall_score\"] / 100.0, dtype=torch.float32)\n\n        if self.transform:\n            audio = self.transform(audio)\n\n        return {\"audio\": audio, \"label\": label}\n\n# Create dataset\nhf_dataset = load_dataset(\"zkeown/sousa\", split=\"train\")\ntrain_dataset = SOUSADataset(hf_dataset)\n\n# Create dataloader\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=32,\n    shuffle=True,\n    num_workers=4,\n    pin_memory=True,\n)\n</code></pre>"},{"location":"user-guide/ml-training/#with-preprocessing","title":"With Preprocessing","text":"<pre><code>import torchaudio.transforms as T\n\nclass SOUSAMelDataset(Dataset):\n    def __init__(self, hf_dataset, sample_rate=16000):\n        self.data = hf_dataset\n        self.sample_rate = sample_rate\n\n        # Transforms\n        self.resampler = T.Resample(orig_freq=44100, new_freq=sample_rate)\n        self.mel_transform = T.MelSpectrogram(\n            sample_rate=sample_rate,\n            n_fft=1024,\n            hop_length=256,\n            n_mels=128,\n        )\n        self.to_db = T.AmplitudeToDB()\n\n    def __getitem__(self, idx):\n        sample = self.data[idx]\n\n        # Load and resample audio\n        audio = torch.tensor(sample[\"audio\"][\"array\"]).unsqueeze(0)\n        audio = self.resampler(audio)\n\n        # Extract mel spectrogram\n        mel = self.mel_transform(audio)\n        mel_db = self.to_db(mel)\n\n        # Normalize\n        mel_db = (mel_db - mel_db.mean()) / (mel_db.std() + 1e-6)\n\n        # Label\n        label = torch.tensor(sample[\"overall_score\"] / 100.0)\n\n        return {\"mel\": mel_db.squeeze(0), \"label\": label}\n</code></pre>"},{"location":"user-guide/ml-training/#streaming-dataloader","title":"Streaming DataLoader","text":"<p>For large datasets that don't fit in memory:</p> <pre><code>from datasets import load_dataset\nfrom torch.utils.data import IterableDataset, DataLoader\n\nclass StreamingSOUSADataset(IterableDataset):\n    def __init__(self, split=\"train\"):\n        self.dataset = load_dataset(\"zkeown/sousa\", split=split, streaming=True)\n\n    def __iter__(self):\n        for sample in self.dataset:\n            audio = torch.tensor(sample[\"audio\"][\"array\"], dtype=torch.float32)\n            label = torch.tensor(sample[\"overall_score\"] / 100.0)\n            yield {\"audio\": audio, \"label\": label}\n\n# Streaming dataloader\nstream_loader = DataLoader(\n    StreamingSOUSADataset(\"train\"),\n    batch_size=32,\n)\n\n# Iterate\nfor batch in stream_loader:\n    # Process batch\n    pass\n</code></pre>"},{"location":"user-guide/ml-training/#training-loop-example","title":"Training Loop Example","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom tqdm import tqdm\n\ndef train_epoch(model, train_loader, criterion, optimizer, device):\n    model.train()\n    total_loss = 0\n\n    for batch in tqdm(train_loader, desc=\"Training\"):\n        audio = batch[\"audio\"].to(device)\n        labels = batch[\"label\"].to(device)\n\n        optimizer.zero_grad()\n        outputs = model(audio)\n        loss = criterion(outputs.squeeze(), labels)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    return total_loss / len(train_loader)\n\ndef evaluate(model, val_loader, criterion, device):\n    model.eval()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        for batch in val_loader:\n            audio = batch[\"audio\"].to(device)\n            labels = batch[\"label\"].to(device)\n\n            outputs = model(audio)\n            loss = criterion(outputs.squeeze(), labels)\n\n            total_loss += loss.item()\n            all_preds.extend(outputs.squeeze().cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    return total_loss / len(val_loader), all_preds, all_labels\n\n# Training\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = YourModel().to(device)\ncriterion = nn.MSELoss()\noptimizer = optim.AdamW(model.parameters(), lr=1e-4)\n\nfor epoch in range(num_epochs):\n    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n    val_loss, preds, labels = evaluate(model, val_loader, criterion, device)\n\n    print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}\")\n</code></pre>"},{"location":"user-guide/ml-training/#best-practices-summary","title":"Best Practices Summary","text":"Aspect Recommendation Skill classification Use binary labels or filter by <code>tier_confidence &gt; 0.5</code> Class imbalance Always use class weights for 4-class classification Regression target Use <code>overall_score</code> (captures correlated cluster) Multi-task Use <code>[\"overall_score\", \"tempo_stability\"]</code> (orthogonal) Audio preprocessing Resample to 16kHz for pretrained models Evaluation Report balanced accuracy for classification Augmentation Train on mixed presets, validate on clean Data splits Use provided profile-based splits (prevents leakage)"},{"location":"user-guide/validation/","title":"Validation Guide","text":"<p>SOUSA includes a comprehensive validation infrastructure with 2,132 lines of validation code across 5 modules. This guide covers running validation, interpreting results, and understanding the quality benchmarks.</p>"},{"location":"user-guide/validation/#validation-architecture","title":"Validation Architecture","text":"<pre><code>dataset_gen/validation/\n\u251c\u2500\u2500 __init__.py        # Module exports\n\u251c\u2500\u2500 verify.py          # Data integrity (13 checks)\n\u251c\u2500\u2500 analysis.py        # Statistical analysis\n\u251c\u2500\u2500 realism.py         # Literature validation\n\u2514\u2500\u2500 report.py          # Report generation\n</code></pre> <p>The validation pipeline processes datasets through three stages:</p> <pre><code>Input: Parquet Dataset\n         \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  1. LABEL VERIFICATION          \u2502\n\u2502     - Data integrity (13 checks)\u2502\n\u2502     - References valid          \u2502\n\u2502     - Ranges correct            \u2502\n\u2502     - Metadata matches records  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  2. STATISTICAL ANALYSIS        \u2502\n\u2502     - 11 distribution metrics   \u2502\n\u2502     - Grouped by skill tier     \u2502\n\u2502     - Moments &amp; quartiles       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  3. REALISM VALIDATION          \u2502\n\u2502     - Literature benchmarks     \u2502\n\u2502     - Correlation structure     \u2502\n\u2502     - Skill tier separation     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2193\nOUTPUT: ValidationReport (JSON)\n</code></pre>"},{"location":"user-guide/validation/#running-validation","title":"Running Validation","text":""},{"location":"user-guide/validation/#automatic-validation-default","title":"Automatic Validation (Default)","text":"<p>Validation runs automatically after dataset generation:</p> <pre><code>python scripts/generate_dataset.py --preset small\n# Validation runs at the end\n</code></pre> <p>To skip validation:</p> <pre><code>python scripts/generate_dataset.py --preset small --skip-validation\n</code></pre>"},{"location":"user-guide/validation/#generate-validation-report","title":"Generate Validation Report","text":"<pre><code>from dataset_gen.validation.report import generate_report\n\n# Generate comprehensive report\nreport = generate_report(\n    dataset_dir='output/dataset',\n    output_path='validation_report.json',\n    include_realism=True,\n    include_midi_checks=True\n)\n\n# Print summary\nprint(report.summary())\n</code></pre>"},{"location":"user-guide/validation/#quick-validation-check","title":"Quick Validation Check","text":"<pre><code>from dataset_gen.validation.report import quick_validate\n\n# Returns True if all critical checks pass\nis_valid = quick_validate('output/dataset')\nprint(f\"Dataset valid: {is_valid}\")\n</code></pre>"},{"location":"user-guide/validation/#individual-validation-components","title":"Individual Validation Components","text":"<pre><code>from dataset_gen.validation import (\n    analyze_dataset,\n    verify_labels,\n    validate_realism\n)\n\n# Statistical analysis\nstats = analyze_dataset('output/dataset')\nprint(f\"Total samples: {stats['num_samples']}\")\nprint(f\"Mean timing error: {stats['timing_error']['mean']:.2f}ms\")\nprint(f\"Skill tier distribution: {stats['skill_tier_counts']}\")\n\n# Data integrity verification\nresult = verify_labels('output/dataset')\nprint(f\"Checks passed: {result.num_passed}/{result.num_passed + result.num_failed}\")\nfor check in result.checks:\n    status = \"PASS\" if check.passed else \"FAIL\"\n    print(f\"  [{status}] {check.name}: {check.message}\")\n\n# Realism validation\nrealism = validate_realism('output/dataset')\nprint(f\"Literature pass rate: {realism['literature_pass_rate']}%\")\nprint(f\"Correlation pass rate: {realism['correlation_pass_rate']}%\")\n</code></pre>"},{"location":"user-guide/validation/#command-line-validation","title":"Command Line Validation","text":"<pre><code># Check dataset health\npython scripts/check_generation.py output/dataset\n</code></pre>"},{"location":"user-guide/validation/#interpreting-results","title":"Interpreting Results","text":""},{"location":"user-guide/validation/#validation-report-summary","title":"Validation Report Summary","text":"<pre><code>=== SOUSA Validation Report ===\nDataset: output/dataset\nGenerated: 2026-01-25T21:18:35\n\nSamples: 99,770\nProfiles: 100\nRudiments: 40\n\nVerification: 13/13 checks passed\nLiterature validation: 100% pass rate\nCorrelation checks: 80% pass rate\n</code></pre>"},{"location":"user-guide/validation/#understanding-check-results","title":"Understanding Check Results","text":"Result Meaning Action 13/13 passed All integrity checks pass Dataset is valid Literature 100% Timing matches research Realistic profiles Correlation 80% Most correlations expected Minor deviations OK <p>Failed checks</p> <p>If verification checks fail, the dataset may have integrity issues. Review the specific check messages and regenerate if necessary.</p>"},{"location":"user-guide/validation/#data-integrity-checks","title":"Data Integrity Checks","text":"<p>The <code>LabelVerifier</code> runs 13 verification checks:</p>"},{"location":"user-guide/validation/#1-sample-id-uniqueness","title":"1. Sample ID Uniqueness","text":"<p>Ensures all sample IDs are unique across the dataset.</p> <pre><code>Result: 99,770/99,770 unique sample IDs\n</code></pre>"},{"location":"user-guide/validation/#2-4-reference-validity","title":"2-4. Reference Validity","text":"<p>Verifies foreign key relationships between tables:</p> Check Description Expected <code>stroke_refs_valid</code> All strokes reference valid samples 100% valid <code>measure_refs_valid</code> All measures reference valid samples 100% valid <code>exercise_refs_valid</code> All exercises reference valid samples 100% valid"},{"location":"user-guide/validation/#5-velocity-range","title":"5. Velocity Range","text":"<p>MIDI velocity must be in [0, 127]:</p> <pre><code>Result: 99.99% in valid MIDI range\n</code></pre>"},{"location":"user-guide/validation/#6-timing-range","title":"6. Timing Range","text":"<p>Timing errors should be bounded:</p> Metric Expected Typical Result Max error &lt; 500ms ~300ms % under 200ms &gt; 95% ~97% Mean error &lt; 100ms ~48ms"},{"location":"user-guide/validation/#7-score-ranges","title":"7. Score Ranges","text":"<p>All exercise scores must be in [0, 100]:</p> <ul> <li><code>timing_accuracy</code>, <code>timing_consistency</code>, <code>tempo_stability</code></li> <li><code>velocity_control</code>, <code>accent_differentiation</code>, <code>accent_accuracy</code></li> <li><code>hand_balance</code>, <code>overall_score</code></li> </ul>"},{"location":"user-guide/validation/#8-9-count-consistency","title":"8-9. Count Consistency","text":"<p>Metadata counts must match actual records:</p> Check Description <code>stroke_counts_match</code> <code>num_strokes</code> matches stroke records <code>measure_counts_match</code> <code>num_measures</code> matches measure records"},{"location":"user-guide/validation/#10-11-skill-tier-ordering","title":"10-11. Skill Tier Ordering","text":"<p>Performance metrics must be properly ordered by skill:</p> <p>Timing Accuracy (Professional &gt; Advanced &gt; Intermediate &gt; Beginner):</p> Tier Expected Mean Professional ~77 Advanced ~58 Intermediate ~33 Beginner ~6 <p>Hand Balance (same ordering):</p> Tier Expected Mean Professional ~96 Advanced ~94 Intermediate ~88 Beginner ~77"},{"location":"user-guide/validation/#12-rudiment-pattern-correctness","title":"12. Rudiment Pattern Correctness","text":"<p>Samples sticking patterns and verifies they match YAML definitions.</p> <pre><code>Result: 0 pattern mismatches (100 samples checked)\n</code></pre>"},{"location":"user-guide/validation/#13-label-midi-alignment","title":"13. Label-MIDI Alignment","text":"<p>Checks that stroke labels align with actual MIDI note events.</p> <pre><code>Result: 100.0% alignment (400 strokes across 10 samples)\n</code></pre>"},{"location":"user-guide/validation/#literature-benchmarks","title":"Literature Benchmarks","text":""},{"location":"user-guide/validation/#timing-error-standard-deviation","title":"Timing Error Standard Deviation","text":"<p>Compared against peer-reviewed motor learning research:</p> Skill Tier Expected Range Source Professional 5-15ms Fujii et al. (2011) Advanced 8-25ms Repp (2005) Intermediate 20-45ms Wing &amp; Kristofferson (1973) Beginner 35-80ms Palmer (1997)"},{"location":"user-guide/validation/#velocity-coefficient-of-variation","title":"Velocity Coefficient of Variation","text":"Skill Tier Expected Range Source Professional 0.05-0.12 Schmidt &amp; Lee (2011) Advanced 0.08-0.18 Schmidt &amp; Lee (2011) Intermediate 0.12-0.25 Schmidt &amp; Lee (2011) Beginner 0.18-0.35 Schmidt &amp; Lee (2011)"},{"location":"user-guide/validation/#expected-pass-rates","title":"Expected Pass Rates","text":"Validation Type Expected Notes Literature comparisons 100% All tiers within ranges Correlation checks 80%+ Some expected deviations"},{"location":"user-guide/validation/#correlation-structure","title":"Correlation Structure","text":""},{"location":"user-guide/validation/#expected-correlations","title":"Expected Correlations","text":"Dimension Pair Expected r Reason Timing Accuracy - Timing Consistency &gt; 0.5 Both from timing errors Timing Accuracy - Hand Balance &gt; 0.3 Correlated skills Velocity Control - Accent Differentiation &gt; 0.4 Dynamic range mastery Timing Accuracy - Overall Score &gt; 0.6 Major component Hand Balance - Overall Score &gt; 0.4 Component score"},{"location":"user-guide/validation/#full-correlation-matrix","title":"Full Correlation Matrix","text":"<pre><code>                    timing_acc  timing_cons  tempo_stab  vel_ctrl  accent_diff  hand_bal  overall\ntiming_accuracy        1.000       0.888       0.575      0.277       0.050      0.464     0.875\ntiming_consistency     0.888       1.000       0.532      0.389       0.083      0.418     0.885\ntempo_stability        0.575       0.532       1.000      0.197       0.023      0.229     0.649\nvelocity_control       0.277       0.389       0.197      1.000       0.259      0.173     0.551\naccent_differentiation 0.050       0.083       0.023      0.259       1.000     -0.103     0.360\nhand_balance           0.464       0.418       0.229      0.173      -0.103      1.000     0.415\noverall_score          0.875       0.885       0.649      0.551       0.360      0.415     1.000\n</code></pre> <p>Velocity/Accent correlation</p> <p>The velocity_control to accent_differentiation correlation is lower than expected because accents are intentionally sampled at different velocities, reducing the correlation.</p>"},{"location":"user-guide/validation/#skill-tier-separation-anova","title":"Skill Tier Separation (ANOVA)","text":"<p>One-way ANOVA confirms skill tiers are statistically distinguishable:</p> Metric F-Statistic p-value Significant Timing Accuracy 44,172 &lt; 0.001 Yes Hand Balance 18,465 &lt; 0.001 Yes Overall Score 55,260 &lt; 0.001 Yes <p>All metrics show highly significant separation with massive F-statistics.</p>"},{"location":"user-guide/validation/#validation-report-schema","title":"Validation Report Schema","text":"<p>The <code>validation_report.json</code> file contains:</p> <pre><code>{\n  \"dataset_path\": \"output/dataset\",\n  \"generated_at\": \"2026-01-25T21:18:35.372368\",\n  \"stats\": {\n    \"num_samples\": 99770,\n    \"num_profiles\": 100,\n    \"num_rudiments\": 40,\n    \"tempo\": { /* DistributionStats */ },\n    \"duration\": { /* DistributionStats */ },\n    \"timing_error\": { /* DistributionStats with by_group */ },\n    \"exercise_timing_accuracy\": { /* DistributionStats with by_group */ },\n    \"skill_tier_counts\": { /* counts by tier */ },\n    \"rudiment_counts\": { /* counts by rudiment */ },\n    \"split_counts\": { /* train/val/test counts */ }\n  },\n  \"verification\": {\n    \"all_passed\": true,\n    \"num_passed\": 13,\n    \"num_failed\": 0,\n    \"checks\": [ /* array of check results */ ]\n  },\n  \"skill_tier_ordering\": {\n    \"timing_accuracy_ordered\": true,\n    \"timing_accuracy_means\": { /* by tier */ },\n    \"hand_balance_ordered\": true,\n    \"hand_balance_means\": { /* by tier */ }\n  },\n  \"realism\": {\n    \"literature_comparisons\": [ /* array of comparisons */ ],\n    \"literature_pass_rate\": 100.0,\n    \"correlation_checks\": [ /* array of checks */ ],\n    \"correlation_pass_rate\": 80.0,\n    \"correlation_matrix\": { /* full matrix */ },\n    \"skill_tier_separation\": {\n      \"timing_accuracy\": { \"f_statistic\": 44171.75, \"p_value\": 0.0 },\n      \"hand_balance\": { \"f_statistic\": 18464.86, \"p_value\": 0.0 },\n      \"overall_score\": { \"f_statistic\": 55260.32, \"p_value\": 0.0 }\n    }\n  }\n}\n</code></pre>"},{"location":"user-guide/validation/#reading-the-report","title":"Reading the Report","text":"<pre><code>import json\n\nwith open('output/dataset/validation_report.json') as f:\n    report = json.load(f)\n\n# Check if all verifications passed\nprint(f\"All checks passed: {report['verification']['all_passed']}\")\n\n# Get timing accuracy by skill tier\ntiming = report['stats']['exercise_timing_accuracy']['by_group']\nfor tier, stats in timing.items():\n    print(f\"{tier}: {stats['mean']:.1f} +/- {stats['std']:.1f}\")\n\n# Check realism validation\nprint(f\"Literature: {report['realism']['literature_pass_rate']}% pass\")\nprint(f\"Correlations: {report['realism']['correlation_pass_rate']}% pass\")\n</code></pre>"},{"location":"user-guide/validation/#test-suite-overview","title":"Test Suite Overview","text":"<p>The validation infrastructure has 26 test cases in <code>tests/test_validation.py</code>:</p> Test Class Tests Coverage TestDistributionStats 2 Statistics computation TestDatasetAnalyzer 4 Analysis pipeline TestLabelVerifier 7 All 13 integrity checks TestVerificationResult 3 Result aggregation TestValidationReport 5 Report generation TestSkillTierOrdering 2 Ordering verification TestDataIntegrity 3 Edge cases"},{"location":"user-guide/validation/#running-tests","title":"Running Tests","text":"<pre><code># Run all validation tests\npytest tests/test_validation.py -v\n\n# Run specific test class\npytest tests/test_validation.py::TestLabelVerifier -v\n\n# Run with coverage\npytest tests/test_validation.py --cov=dataset_gen.validation\n</code></pre>"},{"location":"user-guide/validation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/validation/#failed-integrity-checks","title":"Failed Integrity Checks","text":"Check Common Cause Solution Sample ID uniqueness Duplicate generation Regenerate with different seed Reference validity Incomplete writes Check disk space, regenerate Score ranges Bug in score computation Update dataset_gen, regenerate Skill tier ordering Insufficient samples Use larger preset"},{"location":"user-guide/validation/#low-literature-pass-rate","title":"Low Literature Pass Rate","text":"<p>If literature comparisons fail:</p> <ol> <li>Check skill tier distribution matches expected proportions</li> <li>Verify timing parameters in profile generation</li> <li>Review <code>dataset_gen/profiles/archetypes.py</code> for parameter ranges</li> </ol>"},{"location":"user-guide/validation/#unexpected-correlation-structure","title":"Unexpected Correlation Structure","text":"<p>Some correlation deviations are expected:</p> <ul> <li>Low velocity/accent correlation: By design (accents vary velocity)</li> <li>High timing correlations: Expected (scores derived from same errors)</li> </ul> <p>If correlations are significantly off:</p> <ol> <li>Check profile generation parameters</li> <li>Verify score computation in <code>dataset_gen/labels/compute.py</code></li> </ol>"},{"location":"user-guide/validation/#references","title":"References","text":"<p>The realism validation compares against these peer-reviewed sources:</p> <ol> <li> <p>Fujii, S., et al. (2011). Synchronization error of drum kit playing. Music Perception, 28(5), 491-503.</p> </li> <li> <p>Repp, B. H. (2005). Sensorimotor synchronization: A review. Psychonomic Bulletin &amp; Review, 12(6), 969-992.</p> </li> <li> <p>Wing, A. M., &amp; Kristofferson, A. B. (1973). Response delays and timing. Perception &amp; Psychophysics, 14(1), 5-12.</p> </li> <li> <p>Palmer, C. (1997). Music performance. Annual Review of Psychology, 48(1), 115-138.</p> </li> <li> <p>Schmidt, R. A., &amp; Lee, T. D. (2011). Motor control and learning (5<sup>th</sup> ed.). Human Kinetics.</p> </li> </ol>"}]}